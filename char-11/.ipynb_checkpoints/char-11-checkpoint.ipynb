{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/entron/entity-embedding-rossmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入序列化数据的库，pickle可以把对象序列化，\n",
    "#然后保存到磁盘；或把磁盘文件反序列化读入内存.\n",
    "#pickle是Python独有，更一般的处理库有json\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把csv文件转换为字典\n",
    "def csv2dicts(csvfile):\n",
    "    data = []\n",
    "    keys = []\n",
    "    for row_index, row in enumerate(csvfile):\n",
    "        #把第一行标题打印出来\n",
    "        if row_index == 0:\n",
    "            keys = row\n",
    "            print(row)\n",
    "            continue\n",
    "        \n",
    "        data.append({key: value for key, value in zip(keys, row)})\n",
    "    return data\n",
    "\n",
    "#如果值为空，则用'0'填充\n",
    "def set_nan_as_string(data, replace_str='0'):\n",
    "    for i, x in enumerate(data):\n",
    "        for key, value in x.items():\n",
    "            if value == '':\n",
    "                x[key] = replace_str\n",
    "        data[i] = x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday']\n",
      "[{'Store': '1115', 'DayOfWeek': '2', 'Date': '2013-01-01', 'Sales': '0', 'Customers': '0', 'Open': '0', 'Promo': '0', 'StateHoliday': 'a', 'SchoolHoliday': '1'}, {'Store': '1114', 'DayOfWeek': '2', 'Date': '2013-01-01', 'Sales': '0', 'Customers': '0', 'Open': '0', 'Promo': '0', 'StateHoliday': 'a', 'SchoolHoliday': '1'}, {'Store': '1113', 'DayOfWeek': '2', 'Date': '2013-01-01', 'Sales': '0', 'Customers': '0', 'Open': '0', 'Promo': '0', 'StateHoliday': 'a', 'SchoolHoliday': '1'}]\n"
     ]
    }
   ],
   "source": [
    "train_data = r\".\\data\\train.csv\"\n",
    "store_data = r\".\\data\\store.csv\"\n",
    "store_states = r'.\\data\\store_states.csv'\n",
    "\n",
    "#把处理后的训练数据写入文件\n",
    "with open(train_data) as csvfile:\n",
    "    data = csv.reader(csvfile, delimiter=',')\n",
    "    with open('train_data.pickle', 'wb') as f:\n",
    "        data = csv2dicts(data)\n",
    "        #头尾倒过来\n",
    "        data = data[::-1]\n",
    "        #序列化，把数据保存到文件中\n",
    "        pickle.dump(data, f, -1)\n",
    "        print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Store': '1115',\n",
       " 'DayOfWeek': '2',\n",
       " 'Date': '2013-01-01',\n",
       " 'Sales': '0',\n",
       " 'Customers': '0',\n",
       " 'Open': '0',\n",
       " 'Promo': '0',\n",
       " 'StateHoliday': 'a',\n",
       " 'SchoolHoliday': '1'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Store', 'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
      "['Store', 'State']\n",
      "[{'Store': '1', 'StoreType': 'c', 'Assortment': 'a', 'CompetitionDistance': '1270', 'CompetitionOpenSinceMonth': '9', 'CompetitionOpenSinceYear': '2008', 'Promo2': '0', 'Promo2SinceWeek': '0', 'Promo2SinceYear': '0', 'PromoInterval': '0', 'State': 'HE'}, {'Store': '2', 'StoreType': 'a', 'Assortment': 'a', 'CompetitionDistance': '570', 'CompetitionOpenSinceMonth': '11', 'CompetitionOpenSinceYear': '2007', 'Promo2': '1', 'Promo2SinceWeek': '13', 'Promo2SinceYear': '2010', 'PromoInterval': 'Jan,Apr,Jul,Oct', 'State': 'TH'}]\n"
     ]
    }
   ],
   "source": [
    "#把处理后的store_data，store_states数据写入文件store_data.pickle\n",
    "with open(store_data) as csvfile, open(store_states) as csvfile2:\n",
    "    data = csv.reader(csvfile, delimiter=',')\n",
    "    state_data = csv.reader(csvfile2, delimiter=',')\n",
    "    with open('store_data.pickle', 'wb') as f:\n",
    "        data = csv2dicts(data)\n",
    "        state_data = csv2dicts(state_data)\n",
    "        set_nan_as_string(data)\n",
    "        #把state加到store_data数据集中，然后保存生成的数据        \n",
    "        for index, val in enumerate(data):\n",
    "            state = state_data[index]\n",
    "            val['State'] = state['State']\n",
    "            data[index] = val\n",
    "        pickle.dump(data, f, -1)\n",
    "        print(data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2、数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 加载数据\n",
    "读取pickle文件数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data.pickle', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    num_records = len(train_data)\n",
    "with open('store_data.pickle', 'rb') as f:\n",
    "    store_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017209 1115\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data),len(store_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Store': '1115',\n",
       " 'DayOfWeek': '2',\n",
       " 'Date': '2013-01-01',\n",
       " 'Sales': '0',\n",
       " 'Customers': '0',\n",
       " 'Open': '0',\n",
       " 'Promo': '0',\n",
       " 'StateHoliday': 'a',\n",
       " 'SchoolHoliday': '1'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Store': '1',\n",
       " 'StoreType': 'c',\n",
       " 'Assortment': 'a',\n",
       " 'CompetitionDistance': '1270',\n",
       " 'CompetitionOpenSinceMonth': '9',\n",
       " 'CompetitionOpenSinceYear': '2008',\n",
       " 'Promo2': '0',\n",
       " 'Promo2SinceWeek': '0',\n",
       " 'Promo2SinceYear': '0',\n",
       " 'PromoInterval': '0',\n",
       " 'State': 'HE'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 定义预处理训练数据函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对时间特征进行拆分和转换，是否促销promo等特征转换为整数\n",
    "def feature_list(record):\n",
    "    dt = datetime.strptime(record['Date'], '%Y-%m-%d')\n",
    "    store_index = int(record['Store'])\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "    day = dt.day\n",
    "    day_of_week = int(record['DayOfWeek'])\n",
    "    try:\n",
    "        store_open = int(record['Open'])\n",
    "    except:\n",
    "        store_open = 1\n",
    "\n",
    "    promo = int(record['Promo'])\n",
    "    #同时返回state对应的简称\n",
    "    return [store_open,\n",
    "            store_index,\n",
    "            day_of_week,\n",
    "            promo,\n",
    "            year,\n",
    "            month,\n",
    "            day,\n",
    "            store_data[store_index - 1]['State']\n",
    "            ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 生成训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "销售记录数:  844338\n",
      "最小销售量:46，最大销售量:41551\n"
     ]
    }
   ],
   "source": [
    "train_data_X = []\n",
    "train_data_y = []\n",
    "\n",
    "for record in train_data:\n",
    "    if record['Sales'] != '0' and record['Open'] != '':\n",
    "        fl = feature_list(record)\n",
    "        train_data_X.append(fl)\n",
    "        train_data_y.append(int(record['Sales']))\n",
    "print(\"销售记录数: \", len(train_data_y))\n",
    "\n",
    "print(\"最小销售量:{}，最大销售量:{}\".format(min(train_data_y), max(train_data_y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1097, 2, 0, 2013, 1, 1, 'RP'],\n",
       " [1, 948, 2, 0, 2013, 1, 1, 'BW'],\n",
       " [1, 769, 2, 0, 2013, 1, 1, 'NW']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看训练数据处理后的前3条记录\n",
    "train_data_X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_data_X).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5961, 4491, 5035]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看销量数据的前3条记录\n",
    "train_data_y[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 对特征数值化，并保存结果到文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 109   1   0   0   0   0   7] 5961\n"
     ]
    }
   ],
   "source": [
    "full_X = np.array(train_data_X)\n",
    "#full_X = np.array(full_X)\n",
    "train_data_X = np.array(train_data_X)\n",
    "les = []\n",
    "#对每列进行处理，把类别转换为数值\n",
    "for i in range(train_data_X.shape[1]):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(full_X[:, i])\n",
    "    les.append(le)\n",
    "    train_data_X[:, i] = le.transform(train_data_X[:, i])\n",
    "\n",
    "#处理后的数据写入pickle文件\n",
    "with open('les.pickle', 'wb') as f:\n",
    "    pickle.dump(les, f, -1)\n",
    "\n",
    "#把训练数据转换为整数\n",
    "train_data_X = train_data_X.astype(int)\n",
    "train_data_y = np.array(train_data_y)\n",
    "\n",
    "#保存数据到feature_train_data.pickle文件\n",
    "with open('feature_train_data.pickle', 'wb') as f:\n",
    "    pickle.dump((train_data_X, train_data_y), f, -1)\n",
    "    print(train_data_X[0], train_data_y[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabelEncoder可以将标签分配一个0—n_classes-1之间的编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 37,  2,  0,  0,  0, 11,  6],\n",
       "       [ 0, 36,  2,  0,  0,  0, 11, 10],\n",
       "       [ 0, 34,  2,  0,  0,  0, 11, 10],\n",
       "       [ 0, 33,  2,  0,  0,  0, 11,  6],\n",
       "       [ 0, 32,  2,  0,  0,  0, 11,  2],\n",
       "       [ 0, 31,  2,  0,  0,  0, 11,  6],\n",
       "       [ 0, 30,  2,  0,  0,  0, 11,  7],\n",
       "       [ 0, 29,  2,  0,  0,  0, 11,  8],\n",
       "       [ 0, 28,  2,  0,  0,  0, 11,  9],\n",
       "       [ 0, 27,  2,  0,  0,  0, 11,  6]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_X[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_data_X[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 导入库\n",
    "导入的库包括对数据预处理的库，SVM,XGB等算法。\n",
    "导入构建模型的tensorflow.keras库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "numpy.random.seed(123)\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#import sys\n",
    "#sys.setrecursionlimit(10000)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model as KerasModel\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Reshape,Flatten\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#屏蔽警告信息\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义一些超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "shuffle_data = False\n",
    "one_hot_as_input = False\n",
    "embeddings_as_input = False\n",
    "save_embeddings = True\n",
    "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 导入训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('feature_train_data.pickle', 'rb')\n",
    "(X, y) = pickle.load(f)\n",
    "\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 对特征进行预测处理\n",
    "预处理主要包括选择特征，把特征转换为one-hot编码等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打乱数据\n",
    "if shuffle_data:\n",
    "    print(\"Using shuffled data\")\n",
    "    sh = numpy.arange(X.shape[0])\n",
    "    numpy.random.shuffle(sh)\n",
    "    X = X[sh]\n",
    "    y = y[sh]\n",
    "\n",
    "\n",
    "#把数据集转换为oneHot编码\n",
    "def one_hot(X):\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    enc.fit(X)\n",
    "    X1= enc.transform(X)\n",
    "    return X1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把训练集转换为one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-fa94a2aadd38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mone_hot_as_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mone_hot_as_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mX1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#X2=X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-c3b1e6b9163c>\u001b[0m in \u001b[0;36mone_hot\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0menc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mX1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\python\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    488\u001b[0m             _transform_selected(X, self._legacy_fit_transform, self.dtype,\n\u001b[0;32m    489\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_categorical_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                                 copy=True)\n\u001b[0m\u001b[0;32m    491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\python\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\base.py\u001b[0m in \u001b[0;36m_transform_selected\u001b[1;34m(X, transform, dtype, selected, copy, retain_order)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mselected\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\python\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_legacy_fit_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    602\u001b[0m                 else np.unique(X[:, i]) for i in range(n_features)]\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\python\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1025\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\python\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "one_hot_as_input = True\n",
    "if one_hot_as_input:\n",
    "    X1=one_hot(X)\n",
    "    \n",
    "#X2=X\n",
    "#enc = OneHotEncoder(sparse=False)\n",
    "#X1=enc.fit_transform(X2)\n",
    "#X2 = enc.transform(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(844338, 8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 109,   1,   0,   0,   0,   0,   7])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把数据集划分为训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-bb7beb108203>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX1_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX1_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my1_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my1_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X1' is not defined"
     ]
    }
   ],
   "source": [
    "X1_train = X1[:train_size]\n",
    "X1_val = X1[train_size:]\n",
    "y1_train = y[:train_size]\n",
    "y1_val = y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 定义采样函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(X, y, n):\n",
    "    '''random samples'''\n",
    "    num_row = X.shape[0]\n",
    "    indices = numpy.random.randint(num_row, size=n)\n",
    "    return X[indices, :], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used for training: 200000\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5定义模型（不使用Embedding层）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def evaluate(self, X_val, y_val):\n",
    "        assert(min(y_val) > 0)\n",
    "        guessed_sales = self.guess(X_val)\n",
    "        relative_err = numpy.absolute((y_val - guessed_sales) / y_val)\n",
    "        result = numpy.sum(relative_err) / len(y_val)\n",
    "        return result\n",
    "\n",
    "\n",
    "class NN(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.epochs = 10\n",
    "        self.checkpointer = ModelCheckpoint(filepath=\"best_model_weights.hdf5\", verbose=1, save_best_only=True)\n",
    "        self.max_log_y = max(numpy.max(numpy.log(y_train)), numpy.max(numpy.log(y_val)))\n",
    "        self.__build_keras_model()\n",
    "        self.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    def __build_keras_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(1000, kernel_initializer=\"uniform\", input_dim=1183))\n",
    "        #self.model.add(Dense(1000, kernel_initializer=\"uniform\", input_dim=8))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(500, kernel_initializer=\"uniform\"))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.add(Activation('sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "    def _val_for_fit(self, val):\n",
    "        val = numpy.log(val) / self.max_log_y\n",
    "        return val\n",
    "\n",
    "    def _val_for_pred(self, val):\n",
    "        return numpy.exp(val * self.max_log_y)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        self.model.fit(X_train, self._val_for_fit(y_train),\n",
    "                       validation_data=(X_val, self._val_for_fit(y_val)),\n",
    "                       epochs=self.epochs, batch_size=128,\n",
    "                       # callbacks=[self.checkpointer],\n",
    "                       )\n",
    "        # self.model.load_weights('best_model_weights.hdf5')\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        result = self.model.predict(features).flatten()\n",
    "        return self._val_for_pred(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting NN...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have shape (1183,) but got array with shape (8,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-54177178f881>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitting NN...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m      \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-a0e34cbdbe11>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_log_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__build_keras_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__build_keras_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-a0e34cbdbe11>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     41\u001b[0m         self.model.fit(X_train, self._val_for_fit(y_train),\n\u001b[0;32m     42\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_val_for_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                        \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m                        \u001b[1;31m# callbacks=[self.checkpointer],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                        )\n",
      "\u001b[1;32mD:\\user\\python\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\user\\python\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\python\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\python\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\python\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2472\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\python\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    572\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    575\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_input to have shape (1183,) but got array with shape (8,)"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "print(\"Fitting NN...\")\n",
    "for i in range(1):\n",
    "     models.append(NN(X_train, y_train, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 定义特征处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#从训练结果读取各特征的embedding向量，并用这些向量作为输入值\n",
    "def embed_features(X, saved_embeddings_fname):\n",
    "    # f_embeddings = open(\"embeddings_shuffled.pickle\", \"rb\")\n",
    "    f_embeddings = open(saved_embeddings_fname, \"rb\")\n",
    "    embeddings = pickle.load(f_embeddings) \n",
    "    \n",
    "    #因store_open,promo这两列，至多只有两个值，没有进行embedding，故需排除在外\n",
    "    index_embedding_mapping = {1: 0, 2: 1, 4: 2, 5: 3, 6: 4, 7: 5}\n",
    "    X_embedded = []\n",
    "\n",
    "    (num_records, num_features) = X.shape\n",
    "    for record in X:\n",
    "        embedded_features = []\n",
    "        for i, feat in enumerate(record):\n",
    "            feat = int(feat)\n",
    "            if i not in index_embedding_mapping.keys():\n",
    "                embedded_features += [feat]\n",
    "            else:\n",
    "                embedding_index = index_embedding_mapping[i]\n",
    "                embedded_features += embeddings[embedding_index][feat].tolist()\n",
    "\n",
    "        X_embedded.append(embedded_features)\n",
    "\n",
    "    return numpy.array(X_embedded)\n",
    "\n",
    "#分别取出各特征,取出X中前8列数据，除第1列，\n",
    "def split_features(X):\n",
    "    X_list = []\n",
    "    #获取X第2列数据\n",
    "    store_index = X[..., [1]]\n",
    "    X_list.append(store_index)\n",
    "    #获取X第3列数据,以下类推\n",
    "    day_of_week = X[..., [2]]\n",
    "    X_list.append(day_of_week)\n",
    "\n",
    "    promo = X[..., [3]]\n",
    "    X_list.append(promo)\n",
    "\n",
    "    year = X[..., [4]]\n",
    "    X_list.append(year)\n",
    "\n",
    "    month = X[..., [5]]\n",
    "    X_list.append(month)\n",
    "\n",
    "    day = X[..., [6]]\n",
    "    X_list.append(day)\n",
    "\n",
    "    State = X[..., [7]]\n",
    "    X_list.append(State)\n",
    "\n",
    "    return X_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 构建传统机器学习模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def evaluate(self, X_val, y_val):\n",
    "        assert(min(y_val) > 0)\n",
    "        guessed_sales = self.guess(X_val)\n",
    "        relative_err = numpy.absolute((y_val - guessed_sales) / y_val)\n",
    "        result = numpy.sum(relative_err) / len(y_val)\n",
    "        return result\n",
    "\n",
    "\n",
    "class LinearModel(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.clf = linear_model.LinearRegression()\n",
    "        self.clf.fit(X_train, numpy.log(y_train))\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, feature):\n",
    "        return numpy.exp(self.clf.predict(feature))\n",
    "\n",
    "\n",
    "class RF(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.clf = RandomForestRegressor(n_estimators=200, verbose=True, max_depth=35, min_samples_split=2,\n",
    "                                         min_samples_leaf=1)\n",
    "        self.clf.fit(X_train, numpy.log(y_train))\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, feature):\n",
    "        return numpy.exp(self.clf.predict(feature))\n",
    "\n",
    "\n",
    "class SVM(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.__normalize_data()\n",
    "        self.clf = SVR(kernel='linear', degree=3, gamma='auto', coef0=0.0, tol=0.001,\n",
    "                       C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
    "\n",
    "        self.clf.fit(self.X_train, numpy.log(self.y_train))\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def __normalize_data(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
    "\n",
    "    def guess(self, feature):\n",
    "        return numpy.exp(self.clf.predict(feature))\n",
    "\n",
    "\n",
    "class XGBoost(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        dtrain = xgb.DMatrix(X_train, label=numpy.log(y_train))\n",
    "        evallist = [(dtrain, 'train')]\n",
    "        param = {'nthread': -1,\n",
    "                 'max_depth': 7,\n",
    "                 'eta': 0.02,\n",
    "                 'silent': 1,\n",
    "                 'objective': 'reg:linear',\n",
    "                 'colsample_bytree': 0.7,\n",
    "                 'subsample': 0.7}\n",
    "        num_round = 3000\n",
    "        self.bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, feature):\n",
    "        dtest = xgb.DMatrix(feature)\n",
    "        return numpy.exp(self.bst.predict(dtest))\n",
    "\n",
    "\n",
    "class HistricalMedian(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.history = {}\n",
    "        self.feature_index = [1, 2, 3, 4]\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            key = tuple(x[self.feature_index])\n",
    "            self.history.setdefault(key, []).append(y)\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        features = numpy.array(features)\n",
    "        features = features[:, self.feature_index]\n",
    "        guessed_sales = [numpy.median(self.history[tuple(feature)]) for feature in features]\n",
    "        return numpy.array(guessed_sales)\n",
    "\n",
    "\n",
    "class KNN(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.normalizer = Normalizer()\n",
    "        self.normalizer.fit(X_train)\n",
    "        self.clf = neighbors.KNeighborsRegressor(n_neighbors=10, weights='distance', p=1)\n",
    "        self.clf.fit(self.normalizer.transform(X_train), numpy.log(y_train))\n",
    "        print(\"Result on validation data: \", self.evaluate(self.normalizer.transform(X_val), y_val))\n",
    "\n",
    "    def guess(self, feature):\n",
    "        return numpy.exp(self.clf.predict(self.normalizer.transform(feature)))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在这里，我们的模型以a为输入，以b为输出，同样我们可以构造拥有多输入和多输出的模型\n",
    "model = Model(inputs=[a1, a2], outputs=[b1, b3, b3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 构建神经网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN_with_EntityEmbedding(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.epochs = 10\n",
    "        self.checkpointer = ModelCheckpoint(filepath=\"best_model_weights.hdf5\", verbose=1, save_best_only=True)\n",
    "        self.max_log_y = max(numpy.max(numpy.log(y_train)), numpy.max(numpy.log(y_val)))\n",
    "        self.__build_keras_model()\n",
    "        self.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    def preprocessing(self, X):\n",
    "        X_list = split_features01(X)\n",
    "        return X_list\n",
    "\n",
    "    def __build_keras_model(self):\n",
    "        input_store = Input(shape=(1,))\n",
    "        output_store = Embedding(1115, 10, name='store_embedding')(input_store)\n",
    "        output_store = Reshape(target_shape=(10,))(output_store)\n",
    "\n",
    "        input_dow = Input(shape=(1,))\n",
    "        output_dow = Embedding(7, 6, name='dow_embedding')(input_dow)\n",
    "        output_dow = Reshape(target_shape=(6,))(output_dow)\n",
    "\n",
    "        input_promo = Input(shape=(1,))\n",
    "        output_promo = Dense(1)(input_promo)\n",
    "\n",
    "        input_year = Input(shape=(1,))\n",
    "        output_year = Embedding(3, 2, name='year_embedding')(input_year)\n",
    "        output_year = Reshape(target_shape=(2,))(output_year)\n",
    "\n",
    "        input_month = Input(shape=(1,))\n",
    "        output_month = Embedding(12, 6, name='month_embedding')(input_month)\n",
    "        output_month = Reshape(target_shape=(6,))(output_month)\n",
    "\n",
    "        input_day = Input(shape=(1,))\n",
    "        output_day = Embedding(31, 10, name='day_embedding')(input_day)\n",
    "        output_day = Reshape(target_shape=(10,))(output_day)\n",
    "\n",
    "        input_germanstate = Input(shape=(1,))\n",
    "        output_germanstate = Embedding(12, 6, name='state_embedding')(input_germanstate)\n",
    "        output_germanstate = Reshape(target_shape=(6,))(output_germanstate)\n",
    "\n",
    "        input_model = [input_store, input_dow, input_promo,\n",
    "                       input_year, input_month, input_day, input_germanstate]\n",
    "\n",
    "        output_embeddings = [output_store, output_dow, output_promo,\n",
    "                             output_year, output_month, output_day, output_germanstate]\n",
    "\n",
    "        output_model = Concatenate()(output_embeddings)\n",
    "        output_model = Dense(1000, kernel_initializer=\"uniform\")(output_model)\n",
    "        output_model = Activation('relu')(output_model)\n",
    "        output_model = Dense(500, kernel_initializer=\"uniform\")(output_model)\n",
    "        output_model = Activation('relu')(output_model)\n",
    "        output_model = Dense(1)(output_model)\n",
    "        output_model = Activation('sigmoid')(output_model)\n",
    "\n",
    "        self.model = KerasModel(inputs=input_model, outputs=output_model)\n",
    "\n",
    "        self.model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "    def _val_for_fit(self, val):\n",
    "        val = numpy.log(val) / self.max_log_y\n",
    "        return val\n",
    "\n",
    "    def _val_for_pred(self, val):\n",
    "        return numpy.exp(val * self.max_log_y)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        self.model.fit(self.preprocessing(X_train), self._val_for_fit(y_train),\n",
    "                       validation_data=(self.preprocessing(X_val), self._val_for_fit(y_val)),\n",
    "                       epochs=self.epochs, batch_size=128,\n",
    "                       # callbacks=[self.checkpointer],\n",
    "                       )\n",
    "        # self.model.load_weights('best_model_weights.hdf5')\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        features = self.preprocessing(features)\n",
    "        result = self.model.predict(features).flatten()\n",
    "        return self._val_for_pred(result)\n",
    "\n",
    "\n",
    "class NN(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.epochs = 10\n",
    "        self.checkpointer = ModelCheckpoint(filepath=\"best_model_weights.hdf5\", verbose=1, save_best_only=True)\n",
    "        self.max_log_y = max(numpy.max(numpy.log(y_train)), numpy.max(numpy.log(y_val)))\n",
    "        self.__build_keras_model()\n",
    "        self.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    def __build_keras_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(1000, kernel_initializer=\"uniform\", input_dim=1183))\n",
    "        #self.model.add(Dense(1000, kernel_initializer=\"uniform\", input_dim=8))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(500, kernel_initializer=\"uniform\"))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.add(Activation('sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "    def _val_for_fit(self, val):\n",
    "        val = numpy.log(val) / self.max_log_y\n",
    "        return val\n",
    "\n",
    "    def _val_for_pred(self, val):\n",
    "        return numpy.exp(val * self.max_log_y)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        self.model.fit(X_train, self._val_for_fit(y_train),\n",
    "                       validation_data=(X_val, self._val_for_fit(y_val)),\n",
    "                       epochs=self.epochs, batch_size=128,\n",
    "                       # callbacks=[self.checkpointer],\n",
    "                       )\n",
    "        # self.model.load_weights('best_model_weights.hdf5')\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        result = self.model.predict(features).flatten()\n",
    "        return self._val_for_pred(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(123)\n",
    "from models import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "train_ratio = 0.9\n",
    "shuffle_data = False\n",
    "one_hot_as_input = False\n",
    "embeddings_as_input = False\n",
    "save_embeddings = True\n",
    "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 导入训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('feature_train_data.pickle', 'rb')\n",
    "(X, y) = pickle.load(f)\n",
    "\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844338"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 划分训练与验证数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### 把X转换为one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#enc = OneHotEncoder(sparse=False)\n",
    "#enc.fit(X)\n",
    "#X= enc.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(844338, 8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 打乱数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using shuffled data\n"
     ]
    }
   ],
   "source": [
    "#打乱数据\n",
    "shuffle_data=True\n",
    "if shuffle_data:\n",
    "    print(\"Using shuffled data\")\n",
    "    sh = numpy.arange(X.shape[0])\n",
    "    numpy.random.shuffle(sh)\n",
    "    X = X[sh]\n",
    "    y = y[sh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 468,   4,   0,   1,   6,  17,   5])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 154,   2,   0,   0,   0,  11,  11])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 定义特征预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if shuffle_data:\n",
    "    print(\"Using shuffled data\")\n",
    "    sh = numpy.arange(X.shape[0])\n",
    "    numpy.random.shuffle(sh)\n",
    "    X = X[sh]\n",
    "    y = y[sh]\n",
    "\n",
    "if embeddings_as_input:\n",
    "    print(\"Using learned embeddings as input\")\n",
    "    X = embed_features(X, saved_embeddings_fname)\n",
    "\n",
    "#对数据集X的各列进行oneHot编码\n",
    "if one_hot_as_input:\n",
    "    print(\"Using one-hot encoding as input\")\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 定义取样函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(X, y, n):\n",
    "    '''random samples'''\n",
    "    num_row = X.shape[0]\n",
    "    indices = numpy.random.randint(num_row, size=n)\n",
    "    return X[indices, :], y[indices]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.random.randint使用示例：\n",
    "np.random.randint(5, size=(2, 4))\n",
    "array([[4, 0, 2, 1],\n",
    "       [3, 2, 2, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used for training: 200000\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 训练xgb模型（不进行one-hot转换，也没有进行Embedding，数据没有shuffle）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting XGBoost...\n",
      "[0]\ttrain-rmse:8.10386\n",
      "[1]\ttrain-rmse:7.94217\n",
      "[2]\ttrain-rmse:7.78374\n",
      "[3]\ttrain-rmse:7.62846\n",
      "[4]\ttrain-rmse:7.47632\n",
      "[5]\ttrain-rmse:7.32718\n",
      "[6]\ttrain-rmse:7.18105\n",
      "[7]\ttrain-rmse:7.03787\n",
      "[8]\ttrain-rmse:6.89751\n",
      "[9]\ttrain-rmse:6.76001\n",
      "[10]\ttrain-rmse:6.62523\n",
      "[11]\ttrain-rmse:6.49317\n",
      "[12]\ttrain-rmse:6.3638\n",
      "[13]\ttrain-rmse:6.23696\n",
      "[14]\ttrain-rmse:6.11276\n",
      "[15]\ttrain-rmse:5.99098\n",
      "[16]\ttrain-rmse:5.87168\n",
      "[17]\ttrain-rmse:5.75472\n",
      "[18]\ttrain-rmse:5.64012\n",
      "[19]\ttrain-rmse:5.52783\n",
      "[20]\ttrain-rmse:5.41777\n",
      "[21]\ttrain-rmse:5.30995\n",
      "[22]\ttrain-rmse:5.20427\n",
      "[23]\ttrain-rmse:5.10075\n",
      "[24]\ttrain-rmse:4.9993\n",
      "[25]\ttrain-rmse:4.89989\n",
      "[26]\ttrain-rmse:4.80247\n",
      "[27]\ttrain-rmse:4.70702\n",
      "[28]\ttrain-rmse:4.6135\n",
      "[29]\ttrain-rmse:4.52187\n",
      "[30]\ttrain-rmse:4.43206\n",
      "[31]\ttrain-rmse:4.34404\n",
      "[32]\ttrain-rmse:4.25782\n",
      "[33]\ttrain-rmse:4.17331\n",
      "[34]\ttrain-rmse:4.09051\n",
      "[35]\ttrain-rmse:4.00938\n",
      "[36]\ttrain-rmse:3.92986\n",
      "[37]\ttrain-rmse:3.85198\n",
      "[38]\ttrain-rmse:3.77565\n",
      "[39]\ttrain-rmse:3.70087\n",
      "[40]\ttrain-rmse:3.62761\n",
      "[41]\ttrain-rmse:3.55585\n",
      "[42]\ttrain-rmse:3.48551\n",
      "[43]\ttrain-rmse:3.41662\n",
      "[44]\ttrain-rmse:3.3491\n",
      "[45]\ttrain-rmse:3.28292\n",
      "[46]\ttrain-rmse:3.2181\n",
      "[47]\ttrain-rmse:3.15459\n",
      "[48]\ttrain-rmse:3.09234\n",
      "[49]\ttrain-rmse:3.03137\n",
      "[50]\ttrain-rmse:2.97162\n",
      "[51]\ttrain-rmse:2.91307\n",
      "[52]\ttrain-rmse:2.85575\n",
      "[53]\ttrain-rmse:2.79958\n",
      "[54]\ttrain-rmse:2.74456\n",
      "[55]\ttrain-rmse:2.69063\n",
      "[56]\ttrain-rmse:2.63778\n",
      "[57]\ttrain-rmse:2.58602\n",
      "[58]\ttrain-rmse:2.53533\n",
      "[59]\ttrain-rmse:2.48567\n",
      "[60]\ttrain-rmse:2.43703\n",
      "[61]\ttrain-rmse:2.38939\n",
      "[62]\ttrain-rmse:2.34272\n",
      "[63]\ttrain-rmse:2.297\n",
      "[64]\ttrain-rmse:2.25219\n",
      "[65]\ttrain-rmse:2.20832\n",
      "[66]\ttrain-rmse:2.16533\n",
      "[67]\ttrain-rmse:2.12323\n",
      "[68]\ttrain-rmse:2.08199\n",
      "[69]\ttrain-rmse:2.04155\n",
      "[70]\ttrain-rmse:2.00199\n",
      "[71]\ttrain-rmse:1.96328\n",
      "[72]\ttrain-rmse:1.92532\n",
      "[73]\ttrain-rmse:1.88819\n",
      "[74]\ttrain-rmse:1.85178\n",
      "[75]\ttrain-rmse:1.81613\n",
      "[76]\ttrain-rmse:1.78125\n",
      "[77]\ttrain-rmse:1.74706\n",
      "[78]\ttrain-rmse:1.71361\n",
      "[79]\ttrain-rmse:1.68084\n",
      "[80]\ttrain-rmse:1.6487\n",
      "[81]\ttrain-rmse:1.61728\n",
      "[82]\ttrain-rmse:1.5865\n",
      "[83]\ttrain-rmse:1.55636\n",
      "[84]\ttrain-rmse:1.52685\n",
      "[85]\ttrain-rmse:1.49799\n",
      "[86]\ttrain-rmse:1.46973\n",
      "[87]\ttrain-rmse:1.44208\n",
      "[88]\ttrain-rmse:1.41498\n",
      "[89]\ttrain-rmse:1.38844\n",
      "[90]\ttrain-rmse:1.36242\n",
      "[91]\ttrain-rmse:1.33701\n",
      "[92]\ttrain-rmse:1.31218\n",
      "[93]\ttrain-rmse:1.28784\n",
      "[94]\ttrain-rmse:1.26403\n",
      "[95]\ttrain-rmse:1.24063\n",
      "[96]\ttrain-rmse:1.21776\n",
      "[97]\ttrain-rmse:1.19543\n",
      "[98]\ttrain-rmse:1.17362\n",
      "[99]\ttrain-rmse:1.15216\n",
      "[100]\ttrain-rmse:1.13126\n",
      "[101]\ttrain-rmse:1.11083\n",
      "[102]\ttrain-rmse:1.09073\n",
      "[103]\ttrain-rmse:1.07106\n",
      "[104]\ttrain-rmse:1.05194\n",
      "[105]\ttrain-rmse:1.03323\n",
      "[106]\ttrain-rmse:1.0149\n",
      "[107]\ttrain-rmse:0.996904\n",
      "[108]\ttrain-rmse:0.979433\n",
      "[109]\ttrain-rmse:0.962261\n",
      "[110]\ttrain-rmse:0.945526\n",
      "[111]\ttrain-rmse:0.929174\n",
      "[112]\ttrain-rmse:0.913231\n",
      "[113]\ttrain-rmse:0.897624\n",
      "[114]\ttrain-rmse:0.882254\n",
      "[115]\ttrain-rmse:0.867375\n",
      "[116]\ttrain-rmse:0.852853\n",
      "[117]\ttrain-rmse:0.838607\n",
      "[118]\ttrain-rmse:0.82465\n",
      "[119]\ttrain-rmse:0.811061\n",
      "[120]\ttrain-rmse:0.797826\n",
      "[121]\ttrain-rmse:0.78487\n",
      "[122]\ttrain-rmse:0.772125\n",
      "[123]\ttrain-rmse:0.759763\n",
      "[124]\ttrain-rmse:0.747683\n",
      "[125]\ttrain-rmse:0.735901\n",
      "[126]\ttrain-rmse:0.724446\n",
      "[127]\ttrain-rmse:0.713226\n",
      "[128]\ttrain-rmse:0.702298\n",
      "[129]\ttrain-rmse:0.691657\n",
      "[130]\ttrain-rmse:0.681112\n",
      "[131]\ttrain-rmse:0.670964\n",
      "[132]\ttrain-rmse:0.661002\n",
      "[133]\ttrain-rmse:0.651191\n",
      "[134]\ttrain-rmse:0.641767\n",
      "[135]\ttrain-rmse:0.632477\n",
      "[136]\ttrain-rmse:0.62356\n",
      "[137]\ttrain-rmse:0.61473\n",
      "[138]\ttrain-rmse:0.606236\n",
      "[139]\ttrain-rmse:0.597984\n",
      "[140]\ttrain-rmse:0.589966\n",
      "[141]\ttrain-rmse:0.58196\n",
      "[142]\ttrain-rmse:0.574315\n",
      "[143]\ttrain-rmse:0.566906\n",
      "[144]\ttrain-rmse:0.559464\n",
      "[145]\ttrain-rmse:0.552302\n",
      "[146]\ttrain-rmse:0.545399\n",
      "[147]\ttrain-rmse:0.538711\n",
      "[148]\ttrain-rmse:0.532207\n",
      "[149]\ttrain-rmse:0.525541\n",
      "[150]\ttrain-rmse:0.51939\n",
      "[151]\ttrain-rmse:0.513412\n",
      "[152]\ttrain-rmse:0.507434\n",
      "[153]\ttrain-rmse:0.501597\n",
      "[154]\ttrain-rmse:0.496159\n",
      "[155]\ttrain-rmse:0.490669\n",
      "[156]\ttrain-rmse:0.485416\n",
      "[157]\ttrain-rmse:0.480367\n",
      "[158]\ttrain-rmse:0.475282\n",
      "[159]\ttrain-rmse:0.470522\n",
      "[160]\ttrain-rmse:0.465723\n",
      "[161]\ttrain-rmse:0.46122\n",
      "[162]\ttrain-rmse:0.45688\n",
      "[163]\ttrain-rmse:0.452358\n",
      "[164]\ttrain-rmse:0.4483\n",
      "[165]\ttrain-rmse:0.444297\n",
      "[166]\ttrain-rmse:0.440287\n",
      "[167]\ttrain-rmse:0.436364\n",
      "[168]\ttrain-rmse:0.432379\n",
      "[169]\ttrain-rmse:0.428556\n",
      "[170]\ttrain-rmse:0.424916\n",
      "[171]\ttrain-rmse:0.421644\n",
      "[172]\ttrain-rmse:0.418495\n",
      "[173]\ttrain-rmse:0.415103\n",
      "[174]\ttrain-rmse:0.412049\n",
      "[175]\ttrain-rmse:0.40873\n",
      "[176]\ttrain-rmse:0.405884\n",
      "[177]\ttrain-rmse:0.403171\n",
      "[178]\ttrain-rmse:0.400557\n",
      "[179]\ttrain-rmse:0.397614\n",
      "[180]\ttrain-rmse:0.395131\n",
      "[181]\ttrain-rmse:0.392691\n",
      "[182]\ttrain-rmse:0.390283\n",
      "[183]\ttrain-rmse:0.387993\n",
      "[184]\ttrain-rmse:0.385801\n",
      "[185]\ttrain-rmse:0.383344\n",
      "[186]\ttrain-rmse:0.381333\n",
      "[187]\ttrain-rmse:0.379409\n",
      "[188]\ttrain-rmse:0.377501\n",
      "[189]\ttrain-rmse:0.375672\n",
      "[190]\ttrain-rmse:0.373882\n",
      "[191]\ttrain-rmse:0.372197\n",
      "[192]\ttrain-rmse:0.370329\n",
      "[193]\ttrain-rmse:0.368413\n",
      "[194]\ttrain-rmse:0.366564\n",
      "[195]\ttrain-rmse:0.364982\n",
      "[196]\ttrain-rmse:0.363534\n",
      "[197]\ttrain-rmse:0.362169\n",
      "[198]\ttrain-rmse:0.360864\n",
      "[199]\ttrain-rmse:0.359531\n",
      "[200]\ttrain-rmse:0.358138\n",
      "[201]\ttrain-rmse:0.356934\n",
      "[202]\ttrain-rmse:0.35571\n",
      "[203]\ttrain-rmse:0.354554\n",
      "[204]\ttrain-rmse:0.35321\n",
      "[205]\ttrain-rmse:0.352138\n",
      "[206]\ttrain-rmse:0.350981\n",
      "[207]\ttrain-rmse:0.349893\n",
      "[208]\ttrain-rmse:0.348923\n",
      "[209]\ttrain-rmse:0.347955\n",
      "[210]\ttrain-rmse:0.346965\n",
      "[211]\ttrain-rmse:0.346111\n",
      "[212]\ttrain-rmse:0.345182\n",
      "[213]\ttrain-rmse:0.344399\n",
      "[214]\ttrain-rmse:0.343631\n",
      "[215]\ttrain-rmse:0.342822\n",
      "[216]\ttrain-rmse:0.341766\n",
      "[217]\ttrain-rmse:0.341066\n",
      "[218]\ttrain-rmse:0.340411\n",
      "[219]\ttrain-rmse:0.339522\n",
      "[220]\ttrain-rmse:0.338896\n",
      "[221]\ttrain-rmse:0.338204\n",
      "[222]\ttrain-rmse:0.337582\n",
      "[223]\ttrain-rmse:0.336716\n",
      "[224]\ttrain-rmse:0.336105\n",
      "[225]\ttrain-rmse:0.335595\n",
      "[226]\ttrain-rmse:0.334806\n",
      "[227]\ttrain-rmse:0.334302\n",
      "[228]\ttrain-rmse:0.333786\n",
      "[229]\ttrain-rmse:0.333318\n",
      "[230]\ttrain-rmse:0.332882\n",
      "[231]\ttrain-rmse:0.33248\n",
      "[232]\ttrain-rmse:0.332085\n",
      "[233]\ttrain-rmse:0.331675\n",
      "[234]\ttrain-rmse:0.331258\n",
      "[235]\ttrain-rmse:0.330828\n",
      "[236]\ttrain-rmse:0.330278\n",
      "[237]\ttrain-rmse:0.329875\n",
      "[238]\ttrain-rmse:0.329469\n",
      "[239]\ttrain-rmse:0.329117\n",
      "[240]\ttrain-rmse:0.328542\n",
      "[241]\ttrain-rmse:0.328185\n",
      "[242]\ttrain-rmse:0.327662\n",
      "[243]\ttrain-rmse:0.32732\n",
      "[244]\ttrain-rmse:0.327033\n",
      "[245]\ttrain-rmse:0.326534\n",
      "[246]\ttrain-rmse:0.326244\n",
      "[247]\ttrain-rmse:0.325942\n",
      "[248]\ttrain-rmse:0.325688\n",
      "[249]\ttrain-rmse:0.325409\n",
      "[250]\ttrain-rmse:0.325165\n",
      "[251]\ttrain-rmse:0.324918\n",
      "[252]\ttrain-rmse:0.324663\n",
      "[253]\ttrain-rmse:0.324419\n",
      "[254]\ttrain-rmse:0.324197\n",
      "[255]\ttrain-rmse:0.323767\n",
      "[256]\ttrain-rmse:0.323357\n",
      "[257]\ttrain-rmse:0.323174\n",
      "[258]\ttrain-rmse:0.32296\n",
      "[259]\ttrain-rmse:0.322708\n",
      "[260]\ttrain-rmse:0.322501\n",
      "[261]\ttrain-rmse:0.322304\n",
      "[262]\ttrain-rmse:0.322115\n",
      "[263]\ttrain-rmse:0.32188\n",
      "[264]\ttrain-rmse:0.321518\n",
      "[265]\ttrain-rmse:0.321146\n",
      "[266]\ttrain-rmse:0.320928\n",
      "[267]\ttrain-rmse:0.320742\n",
      "[268]\ttrain-rmse:0.320593\n",
      "[269]\ttrain-rmse:0.320441\n",
      "[270]\ttrain-rmse:0.320317\n",
      "[271]\ttrain-rmse:0.319984\n",
      "[272]\ttrain-rmse:0.319837\n",
      "[273]\ttrain-rmse:0.31943\n",
      "[274]\ttrain-rmse:0.319332\n",
      "[275]\ttrain-rmse:0.319189\n",
      "[276]\ttrain-rmse:0.318779\n",
      "[277]\ttrain-rmse:0.318674\n",
      "[278]\ttrain-rmse:0.318525\n",
      "[279]\ttrain-rmse:0.318433\n",
      "[280]\ttrain-rmse:0.31804\n",
      "[281]\ttrain-rmse:0.317934\n",
      "[282]\ttrain-rmse:0.31784\n",
      "[283]\ttrain-rmse:0.317674\n",
      "[284]\ttrain-rmse:0.317573\n",
      "[285]\ttrain-rmse:0.317428\n",
      "[286]\ttrain-rmse:0.317307\n",
      "[287]\ttrain-rmse:0.317155\n",
      "[288]\ttrain-rmse:0.317032\n",
      "[289]\ttrain-rmse:0.316632\n",
      "[290]\ttrain-rmse:0.315925\n",
      "[291]\ttrain-rmse:0.315831\n",
      "[292]\ttrain-rmse:0.315754\n",
      "[293]\ttrain-rmse:0.315649\n",
      "[294]\ttrain-rmse:0.315561\n",
      "[295]\ttrain-rmse:0.314991\n",
      "[296]\ttrain-rmse:0.314888\n",
      "[297]\ttrain-rmse:0.314626\n",
      "[298]\ttrain-rmse:0.31456\n",
      "[299]\ttrain-rmse:0.31417\n",
      "[300]\ttrain-rmse:0.314031\n",
      "[301]\ttrain-rmse:0.313366\n",
      "[302]\ttrain-rmse:0.313279\n",
      "[303]\ttrain-rmse:0.313214\n",
      "[304]\ttrain-rmse:0.312708\n",
      "[305]\ttrain-rmse:0.31266\n",
      "[306]\ttrain-rmse:0.312557\n",
      "[307]\ttrain-rmse:0.312433\n",
      "[308]\ttrain-rmse:0.312383\n",
      "[309]\ttrain-rmse:0.312289\n",
      "[310]\ttrain-rmse:0.312034\n",
      "[311]\ttrain-rmse:0.311843\n",
      "[312]\ttrain-rmse:0.311779\n",
      "[313]\ttrain-rmse:0.3117\n",
      "[314]\ttrain-rmse:0.311645\n",
      "[315]\ttrain-rmse:0.311159\n",
      "[316]\ttrain-rmse:0.310779\n",
      "[317]\ttrain-rmse:0.31071\n",
      "[318]\ttrain-rmse:0.310611\n",
      "[319]\ttrain-rmse:0.310105\n",
      "[320]\ttrain-rmse:0.310039\n",
      "[321]\ttrain-rmse:0.309972\n",
      "[322]\ttrain-rmse:0.30989\n",
      "[323]\ttrain-rmse:0.309746\n",
      "[324]\ttrain-rmse:0.309242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[325]\ttrain-rmse:0.308843\n",
      "[326]\ttrain-rmse:0.308431\n",
      "[327]\ttrain-rmse:0.308193\n",
      "[328]\ttrain-rmse:0.308084\n",
      "[329]\ttrain-rmse:0.308029\n",
      "[330]\ttrain-rmse:0.307976\n",
      "[331]\ttrain-rmse:0.307908\n",
      "[332]\ttrain-rmse:0.307828\n",
      "[333]\ttrain-rmse:0.30772\n",
      "[334]\ttrain-rmse:0.307474\n",
      "[335]\ttrain-rmse:0.307428\n",
      "[336]\ttrain-rmse:0.306994\n",
      "[337]\ttrain-rmse:0.306771\n",
      "[338]\ttrain-rmse:0.306664\n",
      "[339]\ttrain-rmse:0.306381\n",
      "[340]\ttrain-rmse:0.306153\n",
      "[341]\ttrain-rmse:0.306043\n",
      "[342]\ttrain-rmse:0.305954\n",
      "[343]\ttrain-rmse:0.305915\n",
      "[344]\ttrain-rmse:0.305844\n",
      "[345]\ttrain-rmse:0.305794\n",
      "[346]\ttrain-rmse:0.305716\n",
      "[347]\ttrain-rmse:0.305642\n",
      "[348]\ttrain-rmse:0.30529\n",
      "[349]\ttrain-rmse:0.304906\n",
      "[350]\ttrain-rmse:0.304841\n",
      "[351]\ttrain-rmse:0.304748\n",
      "[352]\ttrain-rmse:0.304243\n",
      "[353]\ttrain-rmse:0.303882\n",
      "[354]\ttrain-rmse:0.303598\n",
      "[355]\ttrain-rmse:0.30357\n",
      "[356]\ttrain-rmse:0.303358\n",
      "[357]\ttrain-rmse:0.303137\n",
      "[358]\ttrain-rmse:0.303099\n",
      "[359]\ttrain-rmse:0.303067\n",
      "[360]\ttrain-rmse:0.303002\n",
      "[361]\ttrain-rmse:0.302675\n",
      "[362]\ttrain-rmse:0.302623\n",
      "[363]\ttrain-rmse:0.302312\n",
      "[364]\ttrain-rmse:0.302035\n",
      "[365]\ttrain-rmse:0.301993\n",
      "[366]\ttrain-rmse:0.301946\n",
      "[367]\ttrain-rmse:0.301375\n",
      "[368]\ttrain-rmse:0.301112\n",
      "[369]\ttrain-rmse:0.300824\n",
      "[370]\ttrain-rmse:0.300634\n",
      "[371]\ttrain-rmse:0.300568\n",
      "[372]\ttrain-rmse:0.300531\n",
      "[373]\ttrain-rmse:0.300144\n",
      "[374]\ttrain-rmse:0.300119\n",
      "[375]\ttrain-rmse:0.299749\n",
      "[376]\ttrain-rmse:0.299685\n",
      "[377]\ttrain-rmse:0.299642\n",
      "[378]\ttrain-rmse:0.299445\n",
      "[379]\ttrain-rmse:0.299363\n",
      "[380]\ttrain-rmse:0.299049\n",
      "[381]\ttrain-rmse:0.298865\n",
      "[382]\ttrain-rmse:0.298548\n",
      "[383]\ttrain-rmse:0.298365\n",
      "[384]\ttrain-rmse:0.298187\n",
      "[385]\ttrain-rmse:0.29814\n",
      "[386]\ttrain-rmse:0.298077\n",
      "[387]\ttrain-rmse:0.298039\n",
      "[388]\ttrain-rmse:0.297983\n",
      "[389]\ttrain-rmse:0.297955\n",
      "[390]\ttrain-rmse:0.297922\n",
      "[391]\ttrain-rmse:0.297902\n",
      "[392]\ttrain-rmse:0.297392\n",
      "[393]\ttrain-rmse:0.297358\n",
      "[394]\ttrain-rmse:0.297105\n",
      "[395]\ttrain-rmse:0.297073\n",
      "[396]\ttrain-rmse:0.296687\n",
      "[397]\ttrain-rmse:0.296663\n",
      "[398]\ttrain-rmse:0.29658\n",
      "[399]\ttrain-rmse:0.296395\n",
      "[400]\ttrain-rmse:0.296134\n",
      "[401]\ttrain-rmse:0.296118\n",
      "[402]\ttrain-rmse:0.296095\n",
      "[403]\ttrain-rmse:0.295843\n",
      "[404]\ttrain-rmse:0.295632\n",
      "[405]\ttrain-rmse:0.295548\n",
      "[406]\ttrain-rmse:0.295512\n",
      "[407]\ttrain-rmse:0.295263\n",
      "[408]\ttrain-rmse:0.295231\n",
      "[409]\ttrain-rmse:0.2952\n",
      "[410]\ttrain-rmse:0.294798\n",
      "[411]\ttrain-rmse:0.294473\n",
      "[412]\ttrain-rmse:0.294171\n",
      "[413]\ttrain-rmse:0.294133\n",
      "[414]\ttrain-rmse:0.294075\n",
      "[415]\ttrain-rmse:0.294054\n",
      "[416]\ttrain-rmse:0.294019\n",
      "[417]\ttrain-rmse:0.293963\n",
      "[418]\ttrain-rmse:0.29391\n",
      "[419]\ttrain-rmse:0.293857\n",
      "[420]\ttrain-rmse:0.293664\n",
      "[421]\ttrain-rmse:0.293345\n",
      "[422]\ttrain-rmse:0.293263\n",
      "[423]\ttrain-rmse:0.292899\n",
      "[424]\ttrain-rmse:0.292493\n",
      "[425]\ttrain-rmse:0.292301\n",
      "[426]\ttrain-rmse:0.292096\n",
      "[427]\ttrain-rmse:0.291709\n",
      "[428]\ttrain-rmse:0.291276\n",
      "[429]\ttrain-rmse:0.291241\n",
      "[430]\ttrain-rmse:0.291163\n",
      "[431]\ttrain-rmse:0.290865\n",
      "[432]\ttrain-rmse:0.290508\n",
      "[433]\ttrain-rmse:0.290452\n",
      "[434]\ttrain-rmse:0.290397\n",
      "[435]\ttrain-rmse:0.29019\n",
      "[436]\ttrain-rmse:0.290029\n",
      "[437]\ttrain-rmse:0.290001\n",
      "[438]\ttrain-rmse:0.289844\n",
      "[439]\ttrain-rmse:0.289496\n",
      "[440]\ttrain-rmse:0.28948\n",
      "[441]\ttrain-rmse:0.289243\n",
      "[442]\ttrain-rmse:0.289186\n",
      "[443]\ttrain-rmse:0.289128\n",
      "[444]\ttrain-rmse:0.289108\n",
      "[445]\ttrain-rmse:0.289071\n",
      "[446]\ttrain-rmse:0.288832\n",
      "[447]\ttrain-rmse:0.288518\n",
      "[448]\ttrain-rmse:0.28848\n",
      "[449]\ttrain-rmse:0.28846\n",
      "[450]\ttrain-rmse:0.288284\n",
      "[451]\ttrain-rmse:0.287998\n",
      "[452]\ttrain-rmse:0.287723\n",
      "[453]\ttrain-rmse:0.287693\n",
      "[454]\ttrain-rmse:0.287667\n",
      "[455]\ttrain-rmse:0.287624\n",
      "[456]\ttrain-rmse:0.287579\n",
      "[457]\ttrain-rmse:0.28743\n",
      "[458]\ttrain-rmse:0.287378\n",
      "[459]\ttrain-rmse:0.287302\n",
      "[460]\ttrain-rmse:0.287075\n",
      "[461]\ttrain-rmse:0.287023\n",
      "[462]\ttrain-rmse:0.286805\n",
      "[463]\ttrain-rmse:0.28677\n",
      "[464]\ttrain-rmse:0.286496\n",
      "[465]\ttrain-rmse:0.286193\n",
      "[466]\ttrain-rmse:0.286148\n",
      "[467]\ttrain-rmse:0.286125\n",
      "[468]\ttrain-rmse:0.28569\n",
      "[469]\ttrain-rmse:0.285593\n",
      "[470]\ttrain-rmse:0.285542\n",
      "[471]\ttrain-rmse:0.285352\n",
      "[472]\ttrain-rmse:0.285277\n",
      "[473]\ttrain-rmse:0.285252\n",
      "[474]\ttrain-rmse:0.285223\n",
      "[475]\ttrain-rmse:0.285193\n",
      "[476]\ttrain-rmse:0.285156\n",
      "[477]\ttrain-rmse:0.284817\n",
      "[478]\ttrain-rmse:0.284796\n",
      "[479]\ttrain-rmse:0.284552\n",
      "[480]\ttrain-rmse:0.284263\n",
      "[481]\ttrain-rmse:0.284222\n",
      "[482]\ttrain-rmse:0.284156\n",
      "[483]\ttrain-rmse:0.284098\n",
      "[484]\ttrain-rmse:0.283803\n",
      "[485]\ttrain-rmse:0.283787\n",
      "[486]\ttrain-rmse:0.283756\n",
      "[487]\ttrain-rmse:0.283699\n",
      "[488]\ttrain-rmse:0.283356\n",
      "[489]\ttrain-rmse:0.283065\n",
      "[490]\ttrain-rmse:0.282902\n",
      "[491]\ttrain-rmse:0.282637\n",
      "[492]\ttrain-rmse:0.282399\n",
      "[493]\ttrain-rmse:0.282219\n",
      "[494]\ttrain-rmse:0.281797\n",
      "[495]\ttrain-rmse:0.281478\n",
      "[496]\ttrain-rmse:0.281281\n",
      "[497]\ttrain-rmse:0.281212\n",
      "[498]\ttrain-rmse:0.281153\n",
      "[499]\ttrain-rmse:0.281128\n",
      "[500]\ttrain-rmse:0.281081\n",
      "[501]\ttrain-rmse:0.280896\n",
      "[502]\ttrain-rmse:0.280722\n",
      "[503]\ttrain-rmse:0.280552\n",
      "[504]\ttrain-rmse:0.280522\n",
      "[505]\ttrain-rmse:0.280493\n",
      "[506]\ttrain-rmse:0.280461\n",
      "[507]\ttrain-rmse:0.280446\n",
      "[508]\ttrain-rmse:0.28032\n",
      "[509]\ttrain-rmse:0.279969\n",
      "[510]\ttrain-rmse:0.27994\n",
      "[511]\ttrain-rmse:0.279912\n",
      "[512]\ttrain-rmse:0.279896\n",
      "[513]\ttrain-rmse:0.279843\n",
      "[514]\ttrain-rmse:0.279578\n",
      "[515]\ttrain-rmse:0.279314\n",
      "[516]\ttrain-rmse:0.279002\n",
      "[517]\ttrain-rmse:0.27898\n",
      "[518]\ttrain-rmse:0.278933\n",
      "[519]\ttrain-rmse:0.278901\n",
      "[520]\ttrain-rmse:0.278589\n",
      "[521]\ttrain-rmse:0.278444\n",
      "[522]\ttrain-rmse:0.278052\n",
      "[523]\ttrain-rmse:0.27801\n",
      "[524]\ttrain-rmse:0.27781\n",
      "[525]\ttrain-rmse:0.277476\n",
      "[526]\ttrain-rmse:0.277445\n",
      "[527]\ttrain-rmse:0.277418\n",
      "[528]\ttrain-rmse:0.277148\n",
      "[529]\ttrain-rmse:0.277123\n",
      "[530]\ttrain-rmse:0.276759\n",
      "[531]\ttrain-rmse:0.27652\n",
      "[532]\ttrain-rmse:0.276278\n",
      "[533]\ttrain-rmse:0.276037\n",
      "[534]\ttrain-rmse:0.275805\n",
      "[535]\ttrain-rmse:0.275565\n",
      "[536]\ttrain-rmse:0.27554\n",
      "[537]\ttrain-rmse:0.275515\n",
      "[538]\ttrain-rmse:0.275161\n",
      "[539]\ttrain-rmse:0.275151\n",
      "[540]\ttrain-rmse:0.27513\n",
      "[541]\ttrain-rmse:0.275115\n",
      "[542]\ttrain-rmse:0.275102\n",
      "[543]\ttrain-rmse:0.274966\n",
      "[544]\ttrain-rmse:0.27473\n",
      "[545]\ttrain-rmse:0.27442\n",
      "[546]\ttrain-rmse:0.2744\n",
      "[547]\ttrain-rmse:0.274378\n",
      "[548]\ttrain-rmse:0.274359\n",
      "[549]\ttrain-rmse:0.274117\n",
      "[550]\ttrain-rmse:0.274088\n",
      "[551]\ttrain-rmse:0.273854\n",
      "[552]\ttrain-rmse:0.273657\n",
      "[553]\ttrain-rmse:0.273622\n",
      "[554]\ttrain-rmse:0.273296\n",
      "[555]\ttrain-rmse:0.27301\n",
      "[556]\ttrain-rmse:0.272738\n",
      "[557]\ttrain-rmse:0.272482\n",
      "[558]\ttrain-rmse:0.272272\n",
      "[559]\ttrain-rmse:0.272083\n",
      "[560]\ttrain-rmse:0.272043\n",
      "[561]\ttrain-rmse:0.272021\n",
      "[562]\ttrain-rmse:0.271916\n",
      "[563]\ttrain-rmse:0.271786\n",
      "[564]\ttrain-rmse:0.271539\n",
      "[565]\ttrain-rmse:0.271514\n",
      "[566]\ttrain-rmse:0.271488\n",
      "[567]\ttrain-rmse:0.271137\n",
      "[568]\ttrain-rmse:0.270887\n",
      "[569]\ttrain-rmse:0.270663\n",
      "[570]\ttrain-rmse:0.270492\n",
      "[571]\ttrain-rmse:0.270431\n",
      "[572]\ttrain-rmse:0.270419\n",
      "[573]\ttrain-rmse:0.270396\n",
      "[574]\ttrain-rmse:0.270355\n",
      "[575]\ttrain-rmse:0.270232\n",
      "[576]\ttrain-rmse:0.270056\n",
      "[577]\ttrain-rmse:0.269794\n",
      "[578]\ttrain-rmse:0.26977\n",
      "[579]\ttrain-rmse:0.269494\n",
      "[580]\ttrain-rmse:0.269466\n",
      "[581]\ttrain-rmse:0.269216\n",
      "[582]\ttrain-rmse:0.268894\n",
      "[583]\ttrain-rmse:0.268872\n",
      "[584]\ttrain-rmse:0.268752\n",
      "[585]\ttrain-rmse:0.268733\n",
      "[586]\ttrain-rmse:0.268434\n",
      "[587]\ttrain-rmse:0.268392\n",
      "[588]\ttrain-rmse:0.268218\n",
      "[589]\ttrain-rmse:0.268184\n",
      "[590]\ttrain-rmse:0.268168\n",
      "[591]\ttrain-rmse:0.267929\n",
      "[592]\ttrain-rmse:0.267905\n",
      "[593]\ttrain-rmse:0.267743\n",
      "[594]\ttrain-rmse:0.26758\n",
      "[595]\ttrain-rmse:0.267563\n",
      "[596]\ttrain-rmse:0.267541\n",
      "[597]\ttrain-rmse:0.267375\n",
      "[598]\ttrain-rmse:0.267359\n",
      "[599]\ttrain-rmse:0.26731\n",
      "[600]\ttrain-rmse:0.267285\n",
      "[601]\ttrain-rmse:0.267154\n",
      "[602]\ttrain-rmse:0.267141\n",
      "[603]\ttrain-rmse:0.267107\n",
      "[604]\ttrain-rmse:0.267073\n",
      "[605]\ttrain-rmse:0.267055\n",
      "[606]\ttrain-rmse:0.267029\n",
      "[607]\ttrain-rmse:0.267008\n",
      "[608]\ttrain-rmse:0.266852\n",
      "[609]\ttrain-rmse:0.26684\n",
      "[610]\ttrain-rmse:0.266793\n",
      "[611]\ttrain-rmse:0.2666\n",
      "[612]\ttrain-rmse:0.266401\n",
      "[613]\ttrain-rmse:0.266254\n",
      "[614]\ttrain-rmse:0.266234\n",
      "[615]\ttrain-rmse:0.266225\n",
      "[616]\ttrain-rmse:0.266178\n",
      "[617]\ttrain-rmse:0.266126\n",
      "[618]\ttrain-rmse:0.265859\n",
      "[619]\ttrain-rmse:0.265842\n",
      "[620]\ttrain-rmse:0.265829\n",
      "[621]\ttrain-rmse:0.265817\n",
      "[622]\ttrain-rmse:0.265583\n",
      "[623]\ttrain-rmse:0.265364\n",
      "[624]\ttrain-rmse:0.265351\n",
      "[625]\ttrain-rmse:0.265118\n",
      "[626]\ttrain-rmse:0.264776\n",
      "[627]\ttrain-rmse:0.264523\n",
      "[628]\ttrain-rmse:0.26449\n",
      "[629]\ttrain-rmse:0.264301\n",
      "[630]\ttrain-rmse:0.264292\n",
      "[631]\ttrain-rmse:0.264162\n",
      "[632]\ttrain-rmse:0.264105\n",
      "[633]\ttrain-rmse:0.263951\n",
      "[634]\ttrain-rmse:0.263656\n",
      "[635]\ttrain-rmse:0.263507\n",
      "[636]\ttrain-rmse:0.263487\n",
      "[637]\ttrain-rmse:0.263212\n",
      "[638]\ttrain-rmse:0.263009\n",
      "[639]\ttrain-rmse:0.262981\n",
      "[640]\ttrain-rmse:0.262783\n",
      "[641]\ttrain-rmse:0.262766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[642]\ttrain-rmse:0.262747\n",
      "[643]\ttrain-rmse:0.262599\n",
      "[644]\ttrain-rmse:0.262569\n",
      "[645]\ttrain-rmse:0.262559\n",
      "[646]\ttrain-rmse:0.262411\n",
      "[647]\ttrain-rmse:0.262358\n",
      "[648]\ttrain-rmse:0.262346\n",
      "[649]\ttrain-rmse:0.26219\n",
      "[650]\ttrain-rmse:0.26205\n",
      "[651]\ttrain-rmse:0.262031\n",
      "[652]\ttrain-rmse:0.262023\n",
      "[653]\ttrain-rmse:0.261977\n",
      "[654]\ttrain-rmse:0.261938\n",
      "[655]\ttrain-rmse:0.261886\n",
      "[656]\ttrain-rmse:0.261734\n",
      "[657]\ttrain-rmse:0.261514\n",
      "[658]\ttrain-rmse:0.261297\n",
      "[659]\ttrain-rmse:0.261052\n",
      "[660]\ttrain-rmse:0.260844\n",
      "[661]\ttrain-rmse:0.260691\n",
      "[662]\ttrain-rmse:0.260668\n",
      "[663]\ttrain-rmse:0.260518\n",
      "[664]\ttrain-rmse:0.26036\n",
      "[665]\ttrain-rmse:0.260222\n",
      "[666]\ttrain-rmse:0.260208\n",
      "[667]\ttrain-rmse:0.260188\n",
      "[668]\ttrain-rmse:0.260076\n",
      "[669]\ttrain-rmse:0.260064\n",
      "[670]\ttrain-rmse:0.259926\n",
      "[671]\ttrain-rmse:0.259883\n",
      "[672]\ttrain-rmse:0.259849\n",
      "[673]\ttrain-rmse:0.259831\n",
      "[674]\ttrain-rmse:0.259816\n",
      "[675]\ttrain-rmse:0.259674\n",
      "[676]\ttrain-rmse:0.25965\n",
      "[677]\ttrain-rmse:0.259639\n",
      "[678]\ttrain-rmse:0.259624\n",
      "[679]\ttrain-rmse:0.259607\n",
      "[680]\ttrain-rmse:0.259489\n",
      "[681]\ttrain-rmse:0.259472\n",
      "[682]\ttrain-rmse:0.259069\n",
      "[683]\ttrain-rmse:0.258807\n",
      "[684]\ttrain-rmse:0.258529\n",
      "[685]\ttrain-rmse:0.258506\n",
      "[686]\ttrain-rmse:0.258493\n",
      "[687]\ttrain-rmse:0.258465\n",
      "[688]\ttrain-rmse:0.258448\n",
      "[689]\ttrain-rmse:0.258174\n",
      "[690]\ttrain-rmse:0.258158\n",
      "[691]\ttrain-rmse:0.258145\n",
      "[692]\ttrain-rmse:0.257979\n",
      "[693]\ttrain-rmse:0.257959\n",
      "[694]\ttrain-rmse:0.257752\n",
      "[695]\ttrain-rmse:0.257612\n",
      "[696]\ttrain-rmse:0.25759\n",
      "[697]\ttrain-rmse:0.257382\n",
      "[698]\ttrain-rmse:0.257146\n",
      "[699]\ttrain-rmse:0.25713\n",
      "[700]\ttrain-rmse:0.256998\n",
      "[701]\ttrain-rmse:0.256875\n",
      "[702]\ttrain-rmse:0.256755\n",
      "[703]\ttrain-rmse:0.256739\n",
      "[704]\ttrain-rmse:0.256728\n",
      "[705]\ttrain-rmse:0.25661\n",
      "[706]\ttrain-rmse:0.256416\n",
      "[707]\ttrain-rmse:0.256194\n",
      "[708]\ttrain-rmse:0.255909\n",
      "[709]\ttrain-rmse:0.255897\n",
      "[710]\ttrain-rmse:0.255567\n",
      "[711]\ttrain-rmse:0.25533\n",
      "[712]\ttrain-rmse:0.255197\n",
      "[713]\ttrain-rmse:0.255181\n",
      "[714]\ttrain-rmse:0.254982\n",
      "[715]\ttrain-rmse:0.254916\n",
      "[716]\ttrain-rmse:0.254908\n",
      "[717]\ttrain-rmse:0.25489\n",
      "[718]\ttrain-rmse:0.254657\n",
      "[719]\ttrain-rmse:0.254454\n",
      "[720]\ttrain-rmse:0.254278\n",
      "[721]\ttrain-rmse:0.254076\n",
      "[722]\ttrain-rmse:0.25388\n",
      "[723]\ttrain-rmse:0.253868\n",
      "[724]\ttrain-rmse:0.253685\n",
      "[725]\ttrain-rmse:0.253505\n",
      "[726]\ttrain-rmse:0.253472\n",
      "[727]\ttrain-rmse:0.253189\n",
      "[728]\ttrain-rmse:0.253148\n",
      "[729]\ttrain-rmse:0.25313\n",
      "[730]\ttrain-rmse:0.253108\n",
      "[731]\ttrain-rmse:0.252877\n",
      "[732]\ttrain-rmse:0.252844\n",
      "[733]\ttrain-rmse:0.252825\n",
      "[734]\ttrain-rmse:0.252715\n",
      "[735]\ttrain-rmse:0.252588\n",
      "[736]\ttrain-rmse:0.252439\n",
      "[737]\ttrain-rmse:0.252398\n",
      "[738]\ttrain-rmse:0.252382\n",
      "[739]\ttrain-rmse:0.252264\n",
      "[740]\ttrain-rmse:0.252227\n",
      "[741]\ttrain-rmse:0.251967\n",
      "[742]\ttrain-rmse:0.251821\n",
      "[743]\ttrain-rmse:0.251541\n",
      "[744]\ttrain-rmse:0.251414\n",
      "[745]\ttrain-rmse:0.251266\n",
      "[746]\ttrain-rmse:0.251064\n",
      "[747]\ttrain-rmse:0.250765\n",
      "[748]\ttrain-rmse:0.250749\n",
      "[749]\ttrain-rmse:0.250739\n",
      "[750]\ttrain-rmse:0.250711\n",
      "[751]\ttrain-rmse:0.250681\n",
      "[752]\ttrain-rmse:0.250648\n",
      "[753]\ttrain-rmse:0.25063\n",
      "[754]\ttrain-rmse:0.250449\n",
      "[755]\ttrain-rmse:0.250165\n",
      "[756]\ttrain-rmse:0.249967\n",
      "[757]\ttrain-rmse:0.249934\n",
      "[758]\ttrain-rmse:0.249899\n",
      "[759]\ttrain-rmse:0.249578\n",
      "[760]\ttrain-rmse:0.249376\n",
      "[761]\ttrain-rmse:0.249281\n",
      "[762]\ttrain-rmse:0.24914\n",
      "[763]\ttrain-rmse:0.249\n",
      "[764]\ttrain-rmse:0.248869\n",
      "[765]\ttrain-rmse:0.24871\n",
      "[766]\ttrain-rmse:0.248594\n",
      "[767]\ttrain-rmse:0.248563\n",
      "[768]\ttrain-rmse:0.248549\n",
      "[769]\ttrain-rmse:0.248435\n",
      "[770]\ttrain-rmse:0.248249\n",
      "[771]\ttrain-rmse:0.248141\n",
      "[772]\ttrain-rmse:0.247928\n",
      "[773]\ttrain-rmse:0.247917\n",
      "[774]\ttrain-rmse:0.247896\n",
      "[775]\ttrain-rmse:0.247892\n",
      "[776]\ttrain-rmse:0.24772\n",
      "[777]\ttrain-rmse:0.247597\n",
      "[778]\ttrain-rmse:0.247278\n",
      "[779]\ttrain-rmse:0.247267\n",
      "[780]\ttrain-rmse:0.247231\n",
      "[781]\ttrain-rmse:0.24719\n",
      "[782]\ttrain-rmse:0.24717\n",
      "[783]\ttrain-rmse:0.24692\n",
      "[784]\ttrain-rmse:0.246904\n",
      "[785]\ttrain-rmse:0.246807\n",
      "[786]\ttrain-rmse:0.246674\n",
      "[787]\ttrain-rmse:0.246629\n",
      "[788]\ttrain-rmse:0.246525\n",
      "[789]\ttrain-rmse:0.246399\n",
      "[790]\ttrain-rmse:0.246391\n",
      "[791]\ttrain-rmse:0.246259\n",
      "[792]\ttrain-rmse:0.246141\n",
      "[793]\ttrain-rmse:0.246033\n",
      "[794]\ttrain-rmse:0.245862\n",
      "[795]\ttrain-rmse:0.245849\n",
      "[796]\ttrain-rmse:0.245573\n",
      "[797]\ttrain-rmse:0.245559\n",
      "[798]\ttrain-rmse:0.245383\n",
      "[799]\ttrain-rmse:0.24537\n",
      "[800]\ttrain-rmse:0.245177\n",
      "[801]\ttrain-rmse:0.24516\n",
      "[802]\ttrain-rmse:0.245146\n",
      "[803]\ttrain-rmse:0.245133\n",
      "[804]\ttrain-rmse:0.24504\n",
      "[805]\ttrain-rmse:0.24502\n",
      "[806]\ttrain-rmse:0.244969\n",
      "[807]\ttrain-rmse:0.244802\n",
      "[808]\ttrain-rmse:0.24462\n",
      "[809]\ttrain-rmse:0.24461\n",
      "[810]\ttrain-rmse:0.2446\n",
      "[811]\ttrain-rmse:0.244573\n",
      "[812]\ttrain-rmse:0.24456\n",
      "[813]\ttrain-rmse:0.244547\n",
      "[814]\ttrain-rmse:0.24443\n",
      "[815]\ttrain-rmse:0.244134\n",
      "[816]\ttrain-rmse:0.244024\n",
      "[817]\ttrain-rmse:0.244012\n",
      "[818]\ttrain-rmse:0.243914\n",
      "[819]\ttrain-rmse:0.243898\n",
      "[820]\ttrain-rmse:0.243662\n",
      "[821]\ttrain-rmse:0.243563\n",
      "[822]\ttrain-rmse:0.243529\n",
      "[823]\ttrain-rmse:0.243517\n",
      "[824]\ttrain-rmse:0.243498\n",
      "[825]\ttrain-rmse:0.243466\n",
      "[826]\ttrain-rmse:0.243453\n",
      "[827]\ttrain-rmse:0.243435\n",
      "[828]\ttrain-rmse:0.243423\n",
      "[829]\ttrain-rmse:0.243274\n",
      "[830]\ttrain-rmse:0.243146\n",
      "[831]\ttrain-rmse:0.243129\n",
      "[832]\ttrain-rmse:0.243118\n",
      "[833]\ttrain-rmse:0.242995\n",
      "[834]\ttrain-rmse:0.242798\n",
      "[835]\ttrain-rmse:0.24276\n",
      "[836]\ttrain-rmse:0.242664\n",
      "[837]\ttrain-rmse:0.242507\n",
      "[838]\ttrain-rmse:0.242482\n",
      "[839]\ttrain-rmse:0.242472\n",
      "[840]\ttrain-rmse:0.242431\n",
      "[841]\ttrain-rmse:0.242418\n",
      "[842]\ttrain-rmse:0.242249\n",
      "[843]\ttrain-rmse:0.24209\n",
      "[844]\ttrain-rmse:0.241894\n",
      "[845]\ttrain-rmse:0.241778\n",
      "[846]\ttrain-rmse:0.241512\n",
      "[847]\ttrain-rmse:0.241502\n",
      "[848]\ttrain-rmse:0.241487\n",
      "[849]\ttrain-rmse:0.241312\n",
      "[850]\ttrain-rmse:0.24114\n",
      "[851]\ttrain-rmse:0.240914\n",
      "[852]\ttrain-rmse:0.240834\n",
      "[853]\ttrain-rmse:0.240799\n",
      "[854]\ttrain-rmse:0.240781\n",
      "[855]\ttrain-rmse:0.240685\n",
      "[856]\ttrain-rmse:0.240576\n",
      "[857]\ttrain-rmse:0.240481\n",
      "[858]\ttrain-rmse:0.240418\n",
      "[859]\ttrain-rmse:0.240409\n",
      "[860]\ttrain-rmse:0.240248\n",
      "[861]\ttrain-rmse:0.240233\n",
      "[862]\ttrain-rmse:0.240074\n",
      "[863]\ttrain-rmse:0.24006\n",
      "[864]\ttrain-rmse:0.240046\n",
      "[865]\ttrain-rmse:0.240037\n",
      "[866]\ttrain-rmse:0.239899\n",
      "[867]\ttrain-rmse:0.239891\n",
      "[868]\ttrain-rmse:0.239744\n",
      "[869]\ttrain-rmse:0.23958\n",
      "[870]\ttrain-rmse:0.239548\n",
      "[871]\ttrain-rmse:0.239523\n",
      "[872]\ttrain-rmse:0.239355\n",
      "[873]\ttrain-rmse:0.239346\n",
      "[874]\ttrain-rmse:0.239213\n",
      "[875]\ttrain-rmse:0.239099\n",
      "[876]\ttrain-rmse:0.239094\n",
      "[877]\ttrain-rmse:0.23908\n",
      "[878]\ttrain-rmse:0.239049\n",
      "[879]\ttrain-rmse:0.239036\n",
      "[880]\ttrain-rmse:0.23884\n",
      "[881]\ttrain-rmse:0.238661\n",
      "[882]\ttrain-rmse:0.23865\n",
      "[883]\ttrain-rmse:0.238474\n",
      "[884]\ttrain-rmse:0.238374\n",
      "[885]\ttrain-rmse:0.238274\n",
      "[886]\ttrain-rmse:0.238126\n",
      "[887]\ttrain-rmse:0.237979\n",
      "[888]\ttrain-rmse:0.23797\n",
      "[889]\ttrain-rmse:0.237956\n",
      "[890]\ttrain-rmse:0.23795\n",
      "[891]\ttrain-rmse:0.237791\n",
      "[892]\ttrain-rmse:0.237556\n",
      "[893]\ttrain-rmse:0.237432\n",
      "[894]\ttrain-rmse:0.237355\n",
      "[895]\ttrain-rmse:0.237347\n",
      "[896]\ttrain-rmse:0.237192\n",
      "[897]\ttrain-rmse:0.237114\n",
      "[898]\ttrain-rmse:0.237029\n",
      "[899]\ttrain-rmse:0.236913\n",
      "[900]\ttrain-rmse:0.23676\n",
      "[901]\ttrain-rmse:0.236621\n",
      "[902]\ttrain-rmse:0.236372\n",
      "[903]\ttrain-rmse:0.236367\n",
      "[904]\ttrain-rmse:0.236237\n",
      "[905]\ttrain-rmse:0.236222\n",
      "[906]\ttrain-rmse:0.236208\n",
      "[907]\ttrain-rmse:0.236062\n",
      "[908]\ttrain-rmse:0.235972\n",
      "[909]\ttrain-rmse:0.235857\n",
      "[910]\ttrain-rmse:0.235845\n",
      "[911]\ttrain-rmse:0.235686\n",
      "[912]\ttrain-rmse:0.235494\n",
      "[913]\ttrain-rmse:0.235322\n",
      "[914]\ttrain-rmse:0.235308\n",
      "[915]\ttrain-rmse:0.235176\n",
      "[916]\ttrain-rmse:0.235094\n",
      "[917]\ttrain-rmse:0.234979\n",
      "[918]\ttrain-rmse:0.234968\n",
      "[919]\ttrain-rmse:0.234938\n",
      "[920]\ttrain-rmse:0.234925\n",
      "[921]\ttrain-rmse:0.234786\n",
      "[922]\ttrain-rmse:0.234641\n",
      "[923]\ttrain-rmse:0.234525\n",
      "[924]\ttrain-rmse:0.234348\n",
      "[925]\ttrain-rmse:0.234246\n",
      "[926]\ttrain-rmse:0.234237\n",
      "[927]\ttrain-rmse:0.234119\n",
      "[928]\ttrain-rmse:0.234\n",
      "[929]\ttrain-rmse:0.233845\n",
      "[930]\ttrain-rmse:0.233835\n",
      "[931]\ttrain-rmse:0.233827\n",
      "[932]\ttrain-rmse:0.233822\n",
      "[933]\ttrain-rmse:0.233604\n",
      "[934]\ttrain-rmse:0.233486\n",
      "[935]\ttrain-rmse:0.233472\n",
      "[936]\ttrain-rmse:0.233351\n",
      "[937]\ttrain-rmse:0.233245\n",
      "[938]\ttrain-rmse:0.233162\n",
      "[939]\ttrain-rmse:0.233151\n",
      "[940]\ttrain-rmse:0.233132\n",
      "[941]\ttrain-rmse:0.233112\n",
      "[942]\ttrain-rmse:0.232982\n",
      "[943]\ttrain-rmse:0.232867\n",
      "[944]\ttrain-rmse:0.23274\n",
      "[945]\ttrain-rmse:0.23273\n",
      "[946]\ttrain-rmse:0.232581\n",
      "[947]\ttrain-rmse:0.23247\n",
      "[948]\ttrain-rmse:0.232457\n",
      "[949]\ttrain-rmse:0.232449\n",
      "[950]\ttrain-rmse:0.232444\n",
      "[951]\ttrain-rmse:0.232298\n",
      "[952]\ttrain-rmse:0.232293\n",
      "[953]\ttrain-rmse:0.232285\n",
      "[954]\ttrain-rmse:0.232092\n",
      "[955]\ttrain-rmse:0.232058\n",
      "[956]\ttrain-rmse:0.231954\n",
      "[957]\ttrain-rmse:0.23181\n",
      "[958]\ttrain-rmse:0.23178\n",
      "[959]\ttrain-rmse:0.231668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[960]\ttrain-rmse:0.231562\n",
      "[961]\ttrain-rmse:0.231469\n",
      "[962]\ttrain-rmse:0.231323\n",
      "[963]\ttrain-rmse:0.231309\n",
      "[964]\ttrain-rmse:0.231297\n",
      "[965]\ttrain-rmse:0.23129\n",
      "[966]\ttrain-rmse:0.231278\n",
      "[967]\ttrain-rmse:0.231186\n",
      "[968]\ttrain-rmse:0.231111\n",
      "[969]\ttrain-rmse:0.231066\n",
      "[970]\ttrain-rmse:0.230987\n",
      "[971]\ttrain-rmse:0.230972\n",
      "[972]\ttrain-rmse:0.230857\n",
      "[973]\ttrain-rmse:0.230699\n",
      "[974]\ttrain-rmse:0.230601\n",
      "[975]\ttrain-rmse:0.230409\n",
      "[976]\ttrain-rmse:0.230394\n",
      "[977]\ttrain-rmse:0.23038\n",
      "[978]\ttrain-rmse:0.230366\n",
      "[979]\ttrain-rmse:0.230357\n",
      "[980]\ttrain-rmse:0.230232\n",
      "[981]\ttrain-rmse:0.230124\n",
      "[982]\ttrain-rmse:0.230095\n",
      "[983]\ttrain-rmse:0.229861\n",
      "[984]\ttrain-rmse:0.229837\n",
      "[985]\ttrain-rmse:0.229684\n",
      "[986]\ttrain-rmse:0.22966\n",
      "[987]\ttrain-rmse:0.229652\n",
      "[988]\ttrain-rmse:0.229511\n",
      "[989]\ttrain-rmse:0.229492\n",
      "[990]\ttrain-rmse:0.229479\n",
      "[991]\ttrain-rmse:0.229266\n",
      "[992]\ttrain-rmse:0.229245\n",
      "[993]\ttrain-rmse:0.229226\n",
      "[994]\ttrain-rmse:0.229193\n",
      "[995]\ttrain-rmse:0.2291\n",
      "[996]\ttrain-rmse:0.22898\n",
      "[997]\ttrain-rmse:0.228857\n",
      "[998]\ttrain-rmse:0.228721\n",
      "[999]\ttrain-rmse:0.228478\n",
      "[1000]\ttrain-rmse:0.22831\n",
      "[1001]\ttrain-rmse:0.228296\n",
      "[1002]\ttrain-rmse:0.228212\n",
      "[1003]\ttrain-rmse:0.228073\n",
      "[1004]\ttrain-rmse:0.22794\n",
      "[1005]\ttrain-rmse:0.227771\n",
      "[1006]\ttrain-rmse:0.227764\n",
      "[1007]\ttrain-rmse:0.227662\n",
      "[1008]\ttrain-rmse:0.227543\n",
      "[1009]\ttrain-rmse:0.227531\n",
      "[1010]\ttrain-rmse:0.227444\n",
      "[1011]\ttrain-rmse:0.227413\n",
      "[1012]\ttrain-rmse:0.22734\n",
      "[1013]\ttrain-rmse:0.227311\n",
      "[1014]\ttrain-rmse:0.227225\n",
      "[1015]\ttrain-rmse:0.227202\n",
      "[1016]\ttrain-rmse:0.227191\n",
      "[1017]\ttrain-rmse:0.227178\n",
      "[1018]\ttrain-rmse:0.22706\n",
      "[1019]\ttrain-rmse:0.226972\n",
      "[1020]\ttrain-rmse:0.226844\n",
      "[1021]\ttrain-rmse:0.226724\n",
      "[1022]\ttrain-rmse:0.226712\n",
      "[1023]\ttrain-rmse:0.226548\n",
      "[1024]\ttrain-rmse:0.226429\n",
      "[1025]\ttrain-rmse:0.226288\n",
      "[1026]\ttrain-rmse:0.226176\n",
      "[1027]\ttrain-rmse:0.22608\n",
      "[1028]\ttrain-rmse:0.226071\n",
      "[1029]\ttrain-rmse:0.226063\n",
      "[1030]\ttrain-rmse:0.225934\n",
      "[1031]\ttrain-rmse:0.22573\n",
      "[1032]\ttrain-rmse:0.225618\n",
      "[1033]\ttrain-rmse:0.225602\n",
      "[1034]\ttrain-rmse:0.225596\n",
      "[1035]\ttrain-rmse:0.225586\n",
      "[1036]\ttrain-rmse:0.225486\n",
      "[1037]\ttrain-rmse:0.225475\n",
      "[1038]\ttrain-rmse:0.225462\n",
      "[1039]\ttrain-rmse:0.225368\n",
      "[1040]\ttrain-rmse:0.225237\n",
      "[1041]\ttrain-rmse:0.225227\n",
      "[1042]\ttrain-rmse:0.225128\n",
      "[1043]\ttrain-rmse:0.224983\n",
      "[1044]\ttrain-rmse:0.224866\n",
      "[1045]\ttrain-rmse:0.224838\n",
      "[1046]\ttrain-rmse:0.224636\n",
      "[1047]\ttrain-rmse:0.224621\n",
      "[1048]\ttrain-rmse:0.224614\n",
      "[1049]\ttrain-rmse:0.2246\n",
      "[1050]\ttrain-rmse:0.224578\n",
      "[1051]\ttrain-rmse:0.224403\n",
      "[1052]\ttrain-rmse:0.22433\n",
      "[1053]\ttrain-rmse:0.224307\n",
      "[1054]\ttrain-rmse:0.224284\n",
      "[1055]\ttrain-rmse:0.224276\n",
      "[1056]\ttrain-rmse:0.22419\n",
      "[1057]\ttrain-rmse:0.224096\n",
      "[1058]\ttrain-rmse:0.224089\n",
      "[1059]\ttrain-rmse:0.224044\n",
      "[1060]\ttrain-rmse:0.22393\n",
      "[1061]\ttrain-rmse:0.22392\n",
      "[1062]\ttrain-rmse:0.223911\n",
      "[1063]\ttrain-rmse:0.223841\n",
      "[1064]\ttrain-rmse:0.223711\n",
      "[1065]\ttrain-rmse:0.223695\n",
      "[1066]\ttrain-rmse:0.223604\n",
      "[1067]\ttrain-rmse:0.223509\n",
      "[1068]\ttrain-rmse:0.223502\n",
      "[1069]\ttrain-rmse:0.223499\n",
      "[1070]\ttrain-rmse:0.223301\n",
      "[1071]\ttrain-rmse:0.223091\n",
      "[1072]\ttrain-rmse:0.222964\n",
      "[1073]\ttrain-rmse:0.222875\n",
      "[1074]\ttrain-rmse:0.222855\n",
      "[1075]\ttrain-rmse:0.222829\n",
      "[1076]\ttrain-rmse:0.222769\n",
      "[1077]\ttrain-rmse:0.222761\n",
      "[1078]\ttrain-rmse:0.222751\n",
      "[1079]\ttrain-rmse:0.222666\n",
      "[1080]\ttrain-rmse:0.222655\n",
      "[1081]\ttrain-rmse:0.222522\n",
      "[1082]\ttrain-rmse:0.222515\n",
      "[1083]\ttrain-rmse:0.222409\n",
      "[1084]\ttrain-rmse:0.222377\n",
      "[1085]\ttrain-rmse:0.222371\n",
      "[1086]\ttrain-rmse:0.222259\n",
      "[1087]\ttrain-rmse:0.222122\n",
      "[1088]\ttrain-rmse:0.222114\n",
      "[1089]\ttrain-rmse:0.221908\n",
      "[1090]\ttrain-rmse:0.221781\n",
      "[1091]\ttrain-rmse:0.22175\n",
      "[1092]\ttrain-rmse:0.221652\n",
      "[1093]\ttrain-rmse:0.221593\n",
      "[1094]\ttrain-rmse:0.221491\n",
      "[1095]\ttrain-rmse:0.221406\n",
      "[1096]\ttrain-rmse:0.221398\n",
      "[1097]\ttrain-rmse:0.221326\n",
      "[1098]\ttrain-rmse:0.221308\n",
      "[1099]\ttrain-rmse:0.221289\n",
      "[1100]\ttrain-rmse:0.221285\n",
      "[1101]\ttrain-rmse:0.221191\n",
      "[1102]\ttrain-rmse:0.221102\n",
      "[1103]\ttrain-rmse:0.221082\n",
      "[1104]\ttrain-rmse:0.221075\n",
      "[1105]\ttrain-rmse:0.220995\n",
      "[1106]\ttrain-rmse:0.220966\n",
      "[1107]\ttrain-rmse:0.220949\n",
      "[1108]\ttrain-rmse:0.220839\n",
      "[1109]\ttrain-rmse:0.220776\n",
      "[1110]\ttrain-rmse:0.220674\n",
      "[1111]\ttrain-rmse:0.220667\n",
      "[1112]\ttrain-rmse:0.220492\n",
      "[1113]\ttrain-rmse:0.220404\n",
      "[1114]\ttrain-rmse:0.220312\n",
      "[1115]\ttrain-rmse:0.220212\n",
      "[1116]\ttrain-rmse:0.220195\n",
      "[1117]\ttrain-rmse:0.220169\n",
      "[1118]\ttrain-rmse:0.220159\n",
      "[1119]\ttrain-rmse:0.220149\n",
      "[1120]\ttrain-rmse:0.220063\n",
      "[1121]\ttrain-rmse:0.220049\n",
      "[1122]\ttrain-rmse:0.21989\n",
      "[1123]\ttrain-rmse:0.219798\n",
      "[1124]\ttrain-rmse:0.219784\n",
      "[1125]\ttrain-rmse:0.219615\n",
      "[1126]\ttrain-rmse:0.219603\n",
      "[1127]\ttrain-rmse:0.219593\n",
      "[1128]\ttrain-rmse:0.219567\n",
      "[1129]\ttrain-rmse:0.219409\n",
      "[1130]\ttrain-rmse:0.219341\n",
      "[1131]\ttrain-rmse:0.219286\n",
      "[1132]\ttrain-rmse:0.219261\n",
      "[1133]\ttrain-rmse:0.21916\n",
      "[1134]\ttrain-rmse:0.219145\n",
      "[1135]\ttrain-rmse:0.219113\n",
      "[1136]\ttrain-rmse:0.219054\n",
      "[1137]\ttrain-rmse:0.218947\n",
      "[1138]\ttrain-rmse:0.21877\n",
      "[1139]\ttrain-rmse:0.218688\n",
      "[1140]\ttrain-rmse:0.218682\n",
      "[1141]\ttrain-rmse:0.218673\n",
      "[1142]\ttrain-rmse:0.218669\n",
      "[1143]\ttrain-rmse:0.218495\n",
      "[1144]\ttrain-rmse:0.218389\n",
      "[1145]\ttrain-rmse:0.218373\n",
      "[1146]\ttrain-rmse:0.218359\n",
      "[1147]\ttrain-rmse:0.21834\n",
      "[1148]\ttrain-rmse:0.218326\n",
      "[1149]\ttrain-rmse:0.21832\n",
      "[1150]\ttrain-rmse:0.218112\n",
      "[1151]\ttrain-rmse:0.217971\n",
      "[1152]\ttrain-rmse:0.217962\n",
      "[1153]\ttrain-rmse:0.217872\n",
      "[1154]\ttrain-rmse:0.217861\n",
      "[1155]\ttrain-rmse:0.217781\n",
      "[1156]\ttrain-rmse:0.21777\n",
      "[1157]\ttrain-rmse:0.217661\n",
      "[1158]\ttrain-rmse:0.217514\n",
      "[1159]\ttrain-rmse:0.217408\n",
      "[1160]\ttrain-rmse:0.217238\n",
      "[1161]\ttrain-rmse:0.217141\n",
      "[1162]\ttrain-rmse:0.217064\n",
      "[1163]\ttrain-rmse:0.21706\n",
      "[1164]\ttrain-rmse:0.217043\n",
      "[1165]\ttrain-rmse:0.217025\n",
      "[1166]\ttrain-rmse:0.216886\n",
      "[1167]\ttrain-rmse:0.216878\n",
      "[1168]\ttrain-rmse:0.216713\n",
      "[1169]\ttrain-rmse:0.216616\n",
      "[1170]\ttrain-rmse:0.216608\n",
      "[1171]\ttrain-rmse:0.2166\n",
      "[1172]\ttrain-rmse:0.216594\n",
      "[1173]\ttrain-rmse:0.216565\n",
      "[1174]\ttrain-rmse:0.216472\n",
      "[1175]\ttrain-rmse:0.216468\n",
      "[1176]\ttrain-rmse:0.216372\n",
      "[1177]\ttrain-rmse:0.216367\n",
      "[1178]\ttrain-rmse:0.216304\n",
      "[1179]\ttrain-rmse:0.216289\n",
      "[1180]\ttrain-rmse:0.216263\n",
      "[1181]\ttrain-rmse:0.216172\n",
      "[1182]\ttrain-rmse:0.216165\n",
      "[1183]\ttrain-rmse:0.216149\n",
      "[1184]\ttrain-rmse:0.216135\n",
      "[1185]\ttrain-rmse:0.216037\n",
      "[1186]\ttrain-rmse:0.216014\n",
      "[1187]\ttrain-rmse:0.216003\n",
      "[1188]\ttrain-rmse:0.2159\n",
      "[1189]\ttrain-rmse:0.215892\n",
      "[1190]\ttrain-rmse:0.215881\n",
      "[1191]\ttrain-rmse:0.215877\n",
      "[1192]\ttrain-rmse:0.215765\n",
      "[1193]\ttrain-rmse:0.215669\n",
      "[1194]\ttrain-rmse:0.215656\n",
      "[1195]\ttrain-rmse:0.215648\n",
      "[1196]\ttrain-rmse:0.215642\n",
      "[1197]\ttrain-rmse:0.215425\n",
      "[1198]\ttrain-rmse:0.215367\n",
      "[1199]\ttrain-rmse:0.215362\n",
      "[1200]\ttrain-rmse:0.215267\n",
      "[1201]\ttrain-rmse:0.215141\n",
      "[1202]\ttrain-rmse:0.215002\n",
      "[1203]\ttrain-rmse:0.214986\n",
      "[1204]\ttrain-rmse:0.21498\n",
      "[1205]\ttrain-rmse:0.214953\n",
      "[1206]\ttrain-rmse:0.214933\n",
      "[1207]\ttrain-rmse:0.21492\n",
      "[1208]\ttrain-rmse:0.214916\n",
      "[1209]\ttrain-rmse:0.214831\n",
      "[1210]\ttrain-rmse:0.214713\n",
      "[1211]\ttrain-rmse:0.21471\n",
      "[1212]\ttrain-rmse:0.214597\n",
      "[1213]\ttrain-rmse:0.214506\n",
      "[1214]\ttrain-rmse:0.214384\n",
      "[1215]\ttrain-rmse:0.214375\n",
      "[1216]\ttrain-rmse:0.214234\n",
      "[1217]\ttrain-rmse:0.214137\n",
      "[1218]\ttrain-rmse:0.214121\n",
      "[1219]\ttrain-rmse:0.214015\n",
      "[1220]\ttrain-rmse:0.213926\n",
      "[1221]\ttrain-rmse:0.213808\n",
      "[1222]\ttrain-rmse:0.213639\n",
      "[1223]\ttrain-rmse:0.21363\n",
      "[1224]\ttrain-rmse:0.213408\n",
      "[1225]\ttrain-rmse:0.213401\n",
      "[1226]\ttrain-rmse:0.213212\n",
      "[1227]\ttrain-rmse:0.213206\n",
      "[1228]\ttrain-rmse:0.213117\n",
      "[1229]\ttrain-rmse:0.213097\n",
      "[1230]\ttrain-rmse:0.213092\n",
      "[1231]\ttrain-rmse:0.213085\n",
      "[1232]\ttrain-rmse:0.213058\n",
      "[1233]\ttrain-rmse:0.212947\n",
      "[1234]\ttrain-rmse:0.212943\n",
      "[1235]\ttrain-rmse:0.212869\n",
      "[1236]\ttrain-rmse:0.212721\n",
      "[1237]\ttrain-rmse:0.212647\n",
      "[1238]\ttrain-rmse:0.21264\n",
      "[1239]\ttrain-rmse:0.212511\n",
      "[1240]\ttrain-rmse:0.212493\n",
      "[1241]\ttrain-rmse:0.212389\n",
      "[1242]\ttrain-rmse:0.212314\n",
      "[1243]\ttrain-rmse:0.212231\n",
      "[1244]\ttrain-rmse:0.212112\n",
      "[1245]\ttrain-rmse:0.212103\n",
      "[1246]\ttrain-rmse:0.212091\n",
      "[1247]\ttrain-rmse:0.212082\n",
      "[1248]\ttrain-rmse:0.212012\n",
      "[1249]\ttrain-rmse:0.211924\n",
      "[1250]\ttrain-rmse:0.211906\n",
      "[1251]\ttrain-rmse:0.2119\n",
      "[1252]\ttrain-rmse:0.21189\n",
      "[1253]\ttrain-rmse:0.211829\n",
      "[1254]\ttrain-rmse:0.211823\n",
      "[1255]\ttrain-rmse:0.211738\n",
      "[1256]\ttrain-rmse:0.211609\n",
      "[1257]\ttrain-rmse:0.211429\n",
      "[1258]\ttrain-rmse:0.21136\n",
      "[1259]\ttrain-rmse:0.211249\n",
      "[1260]\ttrain-rmse:0.211075\n",
      "[1261]\ttrain-rmse:0.210994\n",
      "[1262]\ttrain-rmse:0.210987\n",
      "[1263]\ttrain-rmse:0.210934\n",
      "[1264]\ttrain-rmse:0.21093\n",
      "[1265]\ttrain-rmse:0.210917\n",
      "[1266]\ttrain-rmse:0.210817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1267]\ttrain-rmse:0.210812\n",
      "[1268]\ttrain-rmse:0.210799\n",
      "[1269]\ttrain-rmse:0.210795\n",
      "[1270]\ttrain-rmse:0.210724\n",
      "[1271]\ttrain-rmse:0.210713\n",
      "[1272]\ttrain-rmse:0.210707\n",
      "[1273]\ttrain-rmse:0.210597\n",
      "[1274]\ttrain-rmse:0.210525\n",
      "[1275]\ttrain-rmse:0.21052\n",
      "[1276]\ttrain-rmse:0.210457\n",
      "[1277]\ttrain-rmse:0.21034\n",
      "[1278]\ttrain-rmse:0.21022\n",
      "[1279]\ttrain-rmse:0.210166\n",
      "[1280]\ttrain-rmse:0.210118\n",
      "[1281]\ttrain-rmse:0.209967\n",
      "[1282]\ttrain-rmse:0.209947\n",
      "[1283]\ttrain-rmse:0.209927\n",
      "[1284]\ttrain-rmse:0.20987\n",
      "[1285]\ttrain-rmse:0.209853\n",
      "[1286]\ttrain-rmse:0.209847\n",
      "[1287]\ttrain-rmse:0.209713\n",
      "[1288]\ttrain-rmse:0.209699\n",
      "[1289]\ttrain-rmse:0.209528\n",
      "[1290]\ttrain-rmse:0.209435\n",
      "[1291]\ttrain-rmse:0.209427\n",
      "[1292]\ttrain-rmse:0.209356\n",
      "[1293]\ttrain-rmse:0.209349\n",
      "[1294]\ttrain-rmse:0.209237\n",
      "[1295]\ttrain-rmse:0.209229\n",
      "[1296]\ttrain-rmse:0.209224\n",
      "[1297]\ttrain-rmse:0.209188\n",
      "[1298]\ttrain-rmse:0.209093\n",
      "[1299]\ttrain-rmse:0.209043\n",
      "[1300]\ttrain-rmse:0.209036\n",
      "[1301]\ttrain-rmse:0.209018\n",
      "[1302]\ttrain-rmse:0.209007\n",
      "[1303]\ttrain-rmse:0.20893\n",
      "[1304]\ttrain-rmse:0.208866\n",
      "[1305]\ttrain-rmse:0.208834\n",
      "[1306]\ttrain-rmse:0.208825\n",
      "[1307]\ttrain-rmse:0.208754\n",
      "[1308]\ttrain-rmse:0.208748\n",
      "[1309]\ttrain-rmse:0.20856\n",
      "[1310]\ttrain-rmse:0.208445\n",
      "[1311]\ttrain-rmse:0.208356\n",
      "[1312]\ttrain-rmse:0.208268\n",
      "[1313]\ttrain-rmse:0.208252\n",
      "[1314]\ttrain-rmse:0.208122\n",
      "[1315]\ttrain-rmse:0.20802\n",
      "[1316]\ttrain-rmse:0.207952\n",
      "[1317]\ttrain-rmse:0.207808\n",
      "[1318]\ttrain-rmse:0.207686\n",
      "[1319]\ttrain-rmse:0.207604\n",
      "[1320]\ttrain-rmse:0.207524\n",
      "[1321]\ttrain-rmse:0.207431\n",
      "[1322]\ttrain-rmse:0.207425\n",
      "[1323]\ttrain-rmse:0.207305\n",
      "[1324]\ttrain-rmse:0.207205\n",
      "[1325]\ttrain-rmse:0.2072\n",
      "[1326]\ttrain-rmse:0.207123\n",
      "[1327]\ttrain-rmse:0.207111\n",
      "[1328]\ttrain-rmse:0.206943\n",
      "[1329]\ttrain-rmse:0.206796\n",
      "[1330]\ttrain-rmse:0.206774\n",
      "[1331]\ttrain-rmse:0.206622\n",
      "[1332]\ttrain-rmse:0.206616\n",
      "[1333]\ttrain-rmse:0.206482\n",
      "[1334]\ttrain-rmse:0.206402\n",
      "[1335]\ttrain-rmse:0.206326\n",
      "[1336]\ttrain-rmse:0.206321\n",
      "[1337]\ttrain-rmse:0.206313\n",
      "[1338]\ttrain-rmse:0.206293\n",
      "[1339]\ttrain-rmse:0.206203\n",
      "[1340]\ttrain-rmse:0.206075\n",
      "[1341]\ttrain-rmse:0.20595\n",
      "[1342]\ttrain-rmse:0.205898\n",
      "[1343]\ttrain-rmse:0.205764\n",
      "[1344]\ttrain-rmse:0.205701\n",
      "[1345]\ttrain-rmse:0.205635\n",
      "[1346]\ttrain-rmse:0.205633\n",
      "[1347]\ttrain-rmse:0.205555\n",
      "[1348]\ttrain-rmse:0.205546\n",
      "[1349]\ttrain-rmse:0.205468\n",
      "[1350]\ttrain-rmse:0.205358\n",
      "[1351]\ttrain-rmse:0.205352\n",
      "[1352]\ttrain-rmse:0.205271\n",
      "[1353]\ttrain-rmse:0.2052\n",
      "[1354]\ttrain-rmse:0.205194\n",
      "[1355]\ttrain-rmse:0.205188\n",
      "[1356]\ttrain-rmse:0.205181\n",
      "[1357]\ttrain-rmse:0.205176\n",
      "[1358]\ttrain-rmse:0.205095\n",
      "[1359]\ttrain-rmse:0.205091\n",
      "[1360]\ttrain-rmse:0.205074\n",
      "[1361]\ttrain-rmse:0.205065\n",
      "[1362]\ttrain-rmse:0.205061\n",
      "[1363]\ttrain-rmse:0.205052\n",
      "[1364]\ttrain-rmse:0.205046\n",
      "[1365]\ttrain-rmse:0.205039\n",
      "[1366]\ttrain-rmse:0.205017\n",
      "[1367]\ttrain-rmse:0.204942\n",
      "[1368]\ttrain-rmse:0.204938\n",
      "[1369]\ttrain-rmse:0.204858\n",
      "[1370]\ttrain-rmse:0.204851\n",
      "[1371]\ttrain-rmse:0.20471\n",
      "[1372]\ttrain-rmse:0.204641\n",
      "[1373]\ttrain-rmse:0.204614\n",
      "[1374]\ttrain-rmse:0.20461\n",
      "[1375]\ttrain-rmse:0.204522\n",
      "[1376]\ttrain-rmse:0.204517\n",
      "[1377]\ttrain-rmse:0.204445\n",
      "[1378]\ttrain-rmse:0.204359\n",
      "[1379]\ttrain-rmse:0.204349\n",
      "[1380]\ttrain-rmse:0.204325\n",
      "[1381]\ttrain-rmse:0.204262\n",
      "[1382]\ttrain-rmse:0.204153\n",
      "[1383]\ttrain-rmse:0.204076\n",
      "[1384]\ttrain-rmse:0.204005\n",
      "[1385]\ttrain-rmse:0.204\n",
      "[1386]\ttrain-rmse:0.203946\n",
      "[1387]\ttrain-rmse:0.20386\n",
      "[1388]\ttrain-rmse:0.20381\n",
      "[1389]\ttrain-rmse:0.203712\n",
      "[1390]\ttrain-rmse:0.203705\n",
      "[1391]\ttrain-rmse:0.203634\n",
      "[1392]\ttrain-rmse:0.203626\n",
      "[1393]\ttrain-rmse:0.203619\n",
      "[1394]\ttrain-rmse:0.203595\n",
      "[1395]\ttrain-rmse:0.203525\n",
      "[1396]\ttrain-rmse:0.203442\n",
      "[1397]\ttrain-rmse:0.203315\n",
      "[1398]\ttrain-rmse:0.203233\n",
      "[1399]\ttrain-rmse:0.203079\n",
      "[1400]\ttrain-rmse:0.203066\n",
      "[1401]\ttrain-rmse:0.203056\n",
      "[1402]\ttrain-rmse:0.202919\n",
      "[1403]\ttrain-rmse:0.202831\n",
      "[1404]\ttrain-rmse:0.202826\n",
      "[1405]\ttrain-rmse:0.20266\n",
      "[1406]\ttrain-rmse:0.202584\n",
      "[1407]\ttrain-rmse:0.202569\n",
      "[1408]\ttrain-rmse:0.202411\n",
      "[1409]\ttrain-rmse:0.202403\n",
      "[1410]\ttrain-rmse:0.202298\n",
      "[1411]\ttrain-rmse:0.202159\n",
      "[1412]\ttrain-rmse:0.202154\n",
      "[1413]\ttrain-rmse:0.202\n",
      "[1414]\ttrain-rmse:0.201986\n",
      "[1415]\ttrain-rmse:0.201976\n",
      "[1416]\ttrain-rmse:0.201877\n",
      "[1417]\ttrain-rmse:0.20173\n",
      "[1418]\ttrain-rmse:0.201718\n",
      "[1419]\ttrain-rmse:0.201611\n",
      "[1420]\ttrain-rmse:0.201605\n",
      "[1421]\ttrain-rmse:0.201541\n",
      "[1422]\ttrain-rmse:0.20143\n",
      "[1423]\ttrain-rmse:0.201344\n",
      "[1424]\ttrain-rmse:0.201281\n",
      "[1425]\ttrain-rmse:0.201254\n",
      "[1426]\ttrain-rmse:0.201117\n",
      "[1427]\ttrain-rmse:0.2011\n",
      "[1428]\ttrain-rmse:0.20109\n",
      "[1429]\ttrain-rmse:0.201084\n",
      "[1430]\ttrain-rmse:0.201064\n",
      "[1431]\ttrain-rmse:0.201045\n",
      "[1432]\ttrain-rmse:0.200958\n",
      "[1433]\ttrain-rmse:0.200879\n",
      "[1434]\ttrain-rmse:0.200789\n",
      "[1435]\ttrain-rmse:0.200771\n",
      "[1436]\ttrain-rmse:0.200767\n",
      "[1437]\ttrain-rmse:0.200754\n",
      "[1438]\ttrain-rmse:0.20075\n",
      "[1439]\ttrain-rmse:0.200674\n",
      "[1440]\ttrain-rmse:0.200557\n",
      "[1441]\ttrain-rmse:0.20055\n",
      "[1442]\ttrain-rmse:0.200482\n",
      "[1443]\ttrain-rmse:0.200421\n",
      "[1444]\ttrain-rmse:0.200267\n",
      "[1445]\ttrain-rmse:0.200201\n",
      "[1446]\ttrain-rmse:0.200191\n",
      "[1447]\ttrain-rmse:0.200186\n",
      "[1448]\ttrain-rmse:0.200123\n",
      "[1449]\ttrain-rmse:0.200104\n",
      "[1450]\ttrain-rmse:0.200084\n",
      "[1451]\ttrain-rmse:0.200079\n",
      "[1452]\ttrain-rmse:0.199967\n",
      "[1453]\ttrain-rmse:0.199857\n",
      "[1454]\ttrain-rmse:0.199777\n",
      "[1455]\ttrain-rmse:0.199674\n",
      "[1456]\ttrain-rmse:0.199668\n",
      "[1457]\ttrain-rmse:0.199659\n",
      "[1458]\ttrain-rmse:0.199651\n",
      "[1459]\ttrain-rmse:0.199644\n",
      "[1460]\ttrain-rmse:0.19962\n",
      "[1461]\ttrain-rmse:0.199613\n",
      "[1462]\ttrain-rmse:0.199456\n",
      "[1463]\ttrain-rmse:0.199451\n",
      "[1464]\ttrain-rmse:0.199443\n",
      "[1465]\ttrain-rmse:0.199362\n",
      "[1466]\ttrain-rmse:0.199286\n",
      "[1467]\ttrain-rmse:0.199258\n",
      "[1468]\ttrain-rmse:0.199151\n",
      "[1469]\ttrain-rmse:0.199043\n",
      "[1470]\ttrain-rmse:0.198975\n",
      "[1471]\ttrain-rmse:0.198965\n",
      "[1472]\ttrain-rmse:0.198962\n",
      "[1473]\ttrain-rmse:0.198914\n",
      "[1474]\ttrain-rmse:0.198862\n",
      "[1475]\ttrain-rmse:0.198798\n",
      "[1476]\ttrain-rmse:0.198694\n",
      "[1477]\ttrain-rmse:0.198592\n",
      "[1478]\ttrain-rmse:0.198581\n",
      "[1479]\ttrain-rmse:0.198577\n",
      "[1480]\ttrain-rmse:0.198572\n",
      "[1481]\ttrain-rmse:0.198514\n",
      "[1482]\ttrain-rmse:0.198468\n",
      "[1483]\ttrain-rmse:0.198369\n",
      "[1484]\ttrain-rmse:0.198366\n",
      "[1485]\ttrain-rmse:0.19818\n",
      "[1486]\ttrain-rmse:0.19817\n",
      "[1487]\ttrain-rmse:0.198085\n",
      "[1488]\ttrain-rmse:0.198021\n",
      "[1489]\ttrain-rmse:0.197961\n",
      "[1490]\ttrain-rmse:0.197907\n",
      "[1491]\ttrain-rmse:0.197854\n",
      "[1492]\ttrain-rmse:0.197808\n",
      "[1493]\ttrain-rmse:0.197672\n",
      "[1494]\ttrain-rmse:0.197655\n",
      "[1495]\ttrain-rmse:0.19765\n",
      "[1496]\ttrain-rmse:0.197647\n",
      "[1497]\ttrain-rmse:0.197555\n",
      "[1498]\ttrain-rmse:0.197493\n",
      "[1499]\ttrain-rmse:0.197474\n",
      "[1500]\ttrain-rmse:0.197421\n",
      "[1501]\ttrain-rmse:0.197363\n",
      "[1502]\ttrain-rmse:0.197349\n",
      "[1503]\ttrain-rmse:0.19729\n",
      "[1504]\ttrain-rmse:0.197281\n",
      "[1505]\ttrain-rmse:0.197208\n",
      "[1506]\ttrain-rmse:0.197198\n",
      "[1507]\ttrain-rmse:0.197189\n",
      "[1508]\ttrain-rmse:0.197091\n",
      "[1509]\ttrain-rmse:0.197081\n",
      "[1510]\ttrain-rmse:0.196992\n",
      "[1511]\ttrain-rmse:0.196895\n",
      "[1512]\ttrain-rmse:0.196873\n",
      "[1513]\ttrain-rmse:0.19672\n",
      "[1514]\ttrain-rmse:0.196715\n",
      "[1515]\ttrain-rmse:0.196711\n",
      "[1516]\ttrain-rmse:0.196606\n",
      "[1517]\ttrain-rmse:0.196524\n",
      "[1518]\ttrain-rmse:0.196516\n",
      "[1519]\ttrain-rmse:0.196305\n",
      "[1520]\ttrain-rmse:0.1962\n",
      "[1521]\ttrain-rmse:0.196195\n",
      "[1522]\ttrain-rmse:0.196189\n",
      "[1523]\ttrain-rmse:0.196135\n",
      "[1524]\ttrain-rmse:0.196047\n",
      "[1525]\ttrain-rmse:0.195911\n",
      "[1526]\ttrain-rmse:0.195858\n",
      "[1527]\ttrain-rmse:0.195799\n",
      "[1528]\ttrain-rmse:0.195696\n",
      "[1529]\ttrain-rmse:0.195604\n",
      "[1530]\ttrain-rmse:0.195585\n",
      "[1531]\ttrain-rmse:0.195523\n",
      "[1532]\ttrain-rmse:0.195515\n",
      "[1533]\ttrain-rmse:0.195471\n",
      "[1534]\ttrain-rmse:0.195463\n",
      "[1535]\ttrain-rmse:0.195385\n",
      "[1536]\ttrain-rmse:0.195282\n",
      "[1537]\ttrain-rmse:0.195222\n",
      "[1538]\ttrain-rmse:0.195165\n",
      "[1539]\ttrain-rmse:0.19516\n",
      "[1540]\ttrain-rmse:0.195152\n",
      "[1541]\ttrain-rmse:0.195144\n",
      "[1542]\ttrain-rmse:0.195127\n",
      "[1543]\ttrain-rmse:0.194994\n",
      "[1544]\ttrain-rmse:0.194984\n",
      "[1545]\ttrain-rmse:0.19497\n",
      "[1546]\ttrain-rmse:0.194881\n",
      "[1547]\ttrain-rmse:0.194875\n",
      "[1548]\ttrain-rmse:0.19486\n",
      "[1549]\ttrain-rmse:0.194858\n",
      "[1550]\ttrain-rmse:0.194784\n",
      "[1551]\ttrain-rmse:0.19461\n",
      "[1552]\ttrain-rmse:0.194518\n",
      "[1553]\ttrain-rmse:0.194472\n",
      "[1554]\ttrain-rmse:0.194413\n",
      "[1555]\ttrain-rmse:0.194408\n",
      "[1556]\ttrain-rmse:0.194398\n",
      "[1557]\ttrain-rmse:0.194331\n",
      "[1558]\ttrain-rmse:0.19432\n",
      "[1559]\ttrain-rmse:0.194315\n",
      "[1560]\ttrain-rmse:0.19424\n",
      "[1561]\ttrain-rmse:0.19423\n",
      "[1562]\ttrain-rmse:0.194216\n",
      "[1563]\ttrain-rmse:0.194089\n",
      "[1564]\ttrain-rmse:0.194067\n",
      "[1565]\ttrain-rmse:0.193983\n",
      "[1566]\ttrain-rmse:0.19388\n",
      "[1567]\ttrain-rmse:0.193785\n",
      "[1568]\ttrain-rmse:0.193781\n",
      "[1569]\ttrain-rmse:0.193734\n",
      "[1570]\ttrain-rmse:0.193648\n",
      "[1571]\ttrain-rmse:0.193553\n",
      "[1572]\ttrain-rmse:0.19355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1573]\ttrain-rmse:0.193545\n",
      "[1574]\ttrain-rmse:0.193537\n",
      "[1575]\ttrain-rmse:0.193501\n",
      "[1576]\ttrain-rmse:0.19349\n",
      "[1577]\ttrain-rmse:0.193479\n",
      "[1578]\ttrain-rmse:0.193419\n",
      "[1579]\ttrain-rmse:0.193365\n",
      "[1580]\ttrain-rmse:0.193308\n",
      "[1581]\ttrain-rmse:0.193296\n",
      "[1582]\ttrain-rmse:0.1932\n",
      "[1583]\ttrain-rmse:0.193196\n",
      "[1584]\ttrain-rmse:0.193188\n",
      "[1585]\ttrain-rmse:0.19318\n",
      "[1586]\ttrain-rmse:0.193175\n",
      "[1587]\ttrain-rmse:0.193056\n",
      "[1588]\ttrain-rmse:0.193047\n",
      "[1589]\ttrain-rmse:0.192964\n",
      "[1590]\ttrain-rmse:0.192871\n",
      "[1591]\ttrain-rmse:0.192858\n",
      "[1592]\ttrain-rmse:0.192853\n",
      "[1593]\ttrain-rmse:0.19277\n",
      "[1594]\ttrain-rmse:0.192733\n",
      "[1595]\ttrain-rmse:0.192687\n",
      "[1596]\ttrain-rmse:0.19259\n",
      "[1597]\ttrain-rmse:0.192501\n",
      "[1598]\ttrain-rmse:0.192357\n",
      "[1599]\ttrain-rmse:0.192268\n",
      "[1600]\ttrain-rmse:0.192242\n",
      "[1601]\ttrain-rmse:0.19223\n",
      "[1602]\ttrain-rmse:0.19209\n",
      "[1603]\ttrain-rmse:0.191992\n",
      "[1604]\ttrain-rmse:0.191987\n",
      "[1605]\ttrain-rmse:0.191981\n",
      "[1606]\ttrain-rmse:0.191965\n",
      "[1607]\ttrain-rmse:0.191884\n",
      "[1608]\ttrain-rmse:0.191875\n",
      "[1609]\ttrain-rmse:0.191871\n",
      "[1610]\ttrain-rmse:0.191866\n",
      "[1611]\ttrain-rmse:0.191863\n",
      "[1612]\ttrain-rmse:0.191858\n",
      "[1613]\ttrain-rmse:0.191856\n",
      "[1614]\ttrain-rmse:0.191795\n",
      "[1615]\ttrain-rmse:0.191791\n",
      "[1616]\ttrain-rmse:0.191635\n",
      "[1617]\ttrain-rmse:0.191626\n",
      "[1618]\ttrain-rmse:0.191622\n",
      "[1619]\ttrain-rmse:0.191619\n",
      "[1620]\ttrain-rmse:0.191583\n",
      "[1621]\ttrain-rmse:0.191579\n",
      "[1622]\ttrain-rmse:0.191531\n",
      "[1623]\ttrain-rmse:0.19147\n",
      "[1624]\ttrain-rmse:0.191392\n",
      "[1625]\ttrain-rmse:0.191383\n",
      "[1626]\ttrain-rmse:0.191335\n",
      "[1627]\ttrain-rmse:0.19133\n",
      "[1628]\ttrain-rmse:0.191324\n",
      "[1629]\ttrain-rmse:0.191244\n",
      "[1630]\ttrain-rmse:0.191212\n",
      "[1631]\ttrain-rmse:0.191089\n",
      "[1632]\ttrain-rmse:0.191083\n",
      "[1633]\ttrain-rmse:0.191002\n",
      "[1634]\ttrain-rmse:0.190944\n",
      "[1635]\ttrain-rmse:0.190841\n",
      "[1636]\ttrain-rmse:0.190837\n",
      "[1637]\ttrain-rmse:0.190834\n",
      "[1638]\ttrain-rmse:0.190703\n",
      "[1639]\ttrain-rmse:0.190644\n",
      "[1640]\ttrain-rmse:0.190639\n",
      "[1641]\ttrain-rmse:0.190634\n",
      "[1642]\ttrain-rmse:0.190548\n",
      "[1643]\ttrain-rmse:0.190545\n",
      "[1644]\ttrain-rmse:0.190493\n",
      "[1645]\ttrain-rmse:0.190432\n",
      "[1646]\ttrain-rmse:0.190427\n",
      "[1647]\ttrain-rmse:0.190373\n",
      "[1648]\ttrain-rmse:0.190289\n",
      "[1649]\ttrain-rmse:0.190156\n",
      "[1650]\ttrain-rmse:0.190121\n",
      "[1651]\ttrain-rmse:0.190118\n",
      "[1652]\ttrain-rmse:0.190112\n",
      "[1653]\ttrain-rmse:0.190031\n",
      "[1654]\ttrain-rmse:0.190024\n",
      "[1655]\ttrain-rmse:0.190022\n",
      "[1656]\ttrain-rmse:0.189944\n",
      "[1657]\ttrain-rmse:0.189845\n",
      "[1658]\ttrain-rmse:0.18978\n",
      "[1659]\ttrain-rmse:0.189771\n",
      "[1660]\ttrain-rmse:0.189767\n",
      "[1661]\ttrain-rmse:0.189765\n",
      "[1662]\ttrain-rmse:0.189686\n",
      "[1663]\ttrain-rmse:0.189601\n",
      "[1664]\ttrain-rmse:0.189598\n",
      "[1665]\ttrain-rmse:0.189508\n",
      "[1666]\ttrain-rmse:0.189435\n",
      "[1667]\ttrain-rmse:0.189365\n",
      "[1668]\ttrain-rmse:0.189328\n",
      "[1669]\ttrain-rmse:0.189273\n",
      "[1670]\ttrain-rmse:0.189222\n",
      "[1671]\ttrain-rmse:0.189216\n",
      "[1672]\ttrain-rmse:0.18921\n",
      "[1673]\ttrain-rmse:0.189164\n",
      "[1674]\ttrain-rmse:0.189159\n",
      "[1675]\ttrain-rmse:0.189101\n",
      "[1676]\ttrain-rmse:0.189052\n",
      "[1677]\ttrain-rmse:0.188997\n",
      "[1678]\ttrain-rmse:0.188912\n",
      "[1679]\ttrain-rmse:0.188908\n",
      "[1680]\ttrain-rmse:0.188905\n",
      "[1681]\ttrain-rmse:0.188833\n",
      "[1682]\ttrain-rmse:0.188829\n",
      "[1683]\ttrain-rmse:0.188786\n",
      "[1684]\ttrain-rmse:0.18874\n",
      "[1685]\ttrain-rmse:0.188735\n",
      "[1686]\ttrain-rmse:0.188697\n",
      "[1687]\ttrain-rmse:0.188635\n",
      "[1688]\ttrain-rmse:0.188539\n",
      "[1689]\ttrain-rmse:0.188355\n",
      "[1690]\ttrain-rmse:0.188261\n",
      "[1691]\ttrain-rmse:0.188197\n",
      "[1692]\ttrain-rmse:0.188186\n",
      "[1693]\ttrain-rmse:0.188176\n",
      "[1694]\ttrain-rmse:0.188168\n",
      "[1695]\ttrain-rmse:0.18807\n",
      "[1696]\ttrain-rmse:0.188063\n",
      "[1697]\ttrain-rmse:0.188057\n",
      "[1698]\ttrain-rmse:0.187972\n",
      "[1699]\ttrain-rmse:0.187969\n",
      "[1700]\ttrain-rmse:0.187965\n",
      "[1701]\ttrain-rmse:0.18787\n",
      "[1702]\ttrain-rmse:0.187859\n",
      "[1703]\ttrain-rmse:0.18781\n",
      "[1704]\ttrain-rmse:0.187696\n",
      "[1705]\ttrain-rmse:0.187691\n",
      "[1706]\ttrain-rmse:0.187648\n",
      "[1707]\ttrain-rmse:0.187607\n",
      "[1708]\ttrain-rmse:0.187538\n",
      "[1709]\ttrain-rmse:0.187466\n",
      "[1710]\ttrain-rmse:0.187461\n",
      "[1711]\ttrain-rmse:0.187362\n",
      "[1712]\ttrain-rmse:0.187227\n",
      "[1713]\ttrain-rmse:0.18711\n",
      "[1714]\ttrain-rmse:0.187105\n",
      "[1715]\ttrain-rmse:0.187101\n",
      "[1716]\ttrain-rmse:0.187091\n",
      "[1717]\ttrain-rmse:0.187088\n",
      "[1718]\ttrain-rmse:0.187078\n",
      "[1719]\ttrain-rmse:0.187038\n",
      "[1720]\ttrain-rmse:0.187035\n",
      "[1721]\ttrain-rmse:0.186947\n",
      "[1722]\ttrain-rmse:0.186876\n",
      "[1723]\ttrain-rmse:0.186856\n",
      "[1724]\ttrain-rmse:0.186846\n",
      "[1725]\ttrain-rmse:0.186768\n",
      "[1726]\ttrain-rmse:0.186763\n",
      "[1727]\ttrain-rmse:0.186758\n",
      "[1728]\ttrain-rmse:0.186711\n",
      "[1729]\ttrain-rmse:0.186707\n",
      "[1730]\ttrain-rmse:0.186634\n",
      "[1731]\ttrain-rmse:0.186594\n",
      "[1732]\ttrain-rmse:0.186504\n",
      "[1733]\ttrain-rmse:0.186439\n",
      "[1734]\ttrain-rmse:0.18634\n",
      "[1735]\ttrain-rmse:0.186333\n",
      "[1736]\ttrain-rmse:0.186325\n",
      "[1737]\ttrain-rmse:0.186321\n",
      "[1738]\ttrain-rmse:0.186262\n",
      "[1739]\ttrain-rmse:0.18626\n",
      "[1740]\ttrain-rmse:0.186194\n",
      "[1741]\ttrain-rmse:0.186191\n",
      "[1742]\ttrain-rmse:0.186103\n",
      "[1743]\ttrain-rmse:0.186094\n",
      "[1744]\ttrain-rmse:0.18609\n",
      "[1745]\ttrain-rmse:0.185959\n",
      "[1746]\ttrain-rmse:0.185845\n",
      "[1747]\ttrain-rmse:0.185834\n",
      "[1748]\ttrain-rmse:0.185826\n",
      "[1749]\ttrain-rmse:0.185804\n",
      "[1750]\ttrain-rmse:0.185733\n",
      "[1751]\ttrain-rmse:0.185727\n",
      "[1752]\ttrain-rmse:0.185723\n",
      "[1753]\ttrain-rmse:0.185721\n",
      "[1754]\ttrain-rmse:0.185719\n",
      "[1755]\ttrain-rmse:0.18566\n",
      "[1756]\ttrain-rmse:0.18558\n",
      "[1757]\ttrain-rmse:0.185574\n",
      "[1758]\ttrain-rmse:0.18557\n",
      "[1759]\ttrain-rmse:0.185497\n",
      "[1760]\ttrain-rmse:0.185454\n",
      "[1761]\ttrain-rmse:0.185442\n",
      "[1762]\ttrain-rmse:0.185434\n",
      "[1763]\ttrain-rmse:0.185427\n",
      "[1764]\ttrain-rmse:0.18535\n",
      "[1765]\ttrain-rmse:0.185347\n",
      "[1766]\ttrain-rmse:0.185326\n",
      "[1767]\ttrain-rmse:0.185322\n",
      "[1768]\ttrain-rmse:0.185222\n",
      "[1769]\ttrain-rmse:0.185116\n",
      "[1770]\ttrain-rmse:0.185111\n",
      "[1771]\ttrain-rmse:0.185087\n",
      "[1772]\ttrain-rmse:0.18508\n",
      "[1773]\ttrain-rmse:0.185025\n",
      "[1774]\ttrain-rmse:0.185011\n",
      "[1775]\ttrain-rmse:0.184967\n",
      "[1776]\ttrain-rmse:0.184914\n",
      "[1777]\ttrain-rmse:0.184853\n",
      "[1778]\ttrain-rmse:0.184849\n",
      "[1779]\ttrain-rmse:0.184846\n",
      "[1780]\ttrain-rmse:0.184833\n",
      "[1781]\ttrain-rmse:0.184803\n",
      "[1782]\ttrain-rmse:0.184796\n",
      "[1783]\ttrain-rmse:0.184726\n",
      "[1784]\ttrain-rmse:0.184654\n",
      "[1785]\ttrain-rmse:0.184588\n",
      "[1786]\ttrain-rmse:0.184502\n",
      "[1787]\ttrain-rmse:0.184445\n",
      "[1788]\ttrain-rmse:0.184365\n",
      "[1789]\ttrain-rmse:0.184255\n",
      "[1790]\ttrain-rmse:0.184251\n",
      "[1791]\ttrain-rmse:0.184205\n",
      "[1792]\ttrain-rmse:0.184136\n",
      "[1793]\ttrain-rmse:0.184091\n",
      "[1794]\ttrain-rmse:0.184033\n",
      "[1795]\ttrain-rmse:0.184031\n",
      "[1796]\ttrain-rmse:0.183994\n",
      "[1797]\ttrain-rmse:0.18394\n",
      "[1798]\ttrain-rmse:0.183873\n",
      "[1799]\ttrain-rmse:0.18387\n",
      "[1800]\ttrain-rmse:0.183776\n",
      "[1801]\ttrain-rmse:0.183773\n",
      "[1802]\ttrain-rmse:0.183656\n",
      "[1803]\ttrain-rmse:0.183647\n",
      "[1804]\ttrain-rmse:0.183567\n",
      "[1805]\ttrain-rmse:0.183532\n",
      "[1806]\ttrain-rmse:0.18349\n",
      "[1807]\ttrain-rmse:0.183435\n",
      "[1808]\ttrain-rmse:0.183426\n",
      "[1809]\ttrain-rmse:0.183377\n",
      "[1810]\ttrain-rmse:0.183366\n",
      "[1811]\ttrain-rmse:0.183299\n",
      "[1812]\ttrain-rmse:0.183228\n",
      "[1813]\ttrain-rmse:0.183126\n",
      "[1814]\ttrain-rmse:0.183123\n",
      "[1815]\ttrain-rmse:0.183118\n",
      "[1816]\ttrain-rmse:0.183045\n",
      "[1817]\ttrain-rmse:0.182992\n",
      "[1818]\ttrain-rmse:0.182913\n",
      "[1819]\ttrain-rmse:0.182849\n",
      "[1820]\ttrain-rmse:0.182844\n",
      "[1821]\ttrain-rmse:0.182798\n",
      "[1822]\ttrain-rmse:0.182734\n",
      "[1823]\ttrain-rmse:0.182732\n",
      "[1824]\ttrain-rmse:0.182723\n",
      "[1825]\ttrain-rmse:0.182656\n",
      "[1826]\ttrain-rmse:0.182644\n",
      "[1827]\ttrain-rmse:0.18264\n",
      "[1828]\ttrain-rmse:0.182602\n",
      "[1829]\ttrain-rmse:0.182503\n",
      "[1830]\ttrain-rmse:0.182497\n",
      "[1831]\ttrain-rmse:0.182382\n",
      "[1832]\ttrain-rmse:0.18238\n",
      "[1833]\ttrain-rmse:0.182304\n",
      "[1834]\ttrain-rmse:0.182213\n",
      "[1835]\ttrain-rmse:0.182131\n",
      "[1836]\ttrain-rmse:0.181992\n",
      "[1837]\ttrain-rmse:0.181942\n",
      "[1838]\ttrain-rmse:0.18194\n",
      "[1839]\ttrain-rmse:0.181908\n",
      "[1840]\ttrain-rmse:0.181861\n",
      "[1841]\ttrain-rmse:0.181779\n",
      "[1842]\ttrain-rmse:0.181733\n",
      "[1843]\ttrain-rmse:0.181634\n",
      "[1844]\ttrain-rmse:0.181597\n",
      "[1845]\ttrain-rmse:0.181592\n",
      "[1846]\ttrain-rmse:0.181506\n",
      "[1847]\ttrain-rmse:0.181498\n",
      "[1848]\ttrain-rmse:0.181455\n",
      "[1849]\ttrain-rmse:0.181444\n",
      "[1850]\ttrain-rmse:0.181414\n",
      "[1851]\ttrain-rmse:0.1814\n",
      "[1852]\ttrain-rmse:0.181329\n",
      "[1853]\ttrain-rmse:0.18127\n",
      "[1854]\ttrain-rmse:0.181218\n",
      "[1855]\ttrain-rmse:0.181163\n",
      "[1856]\ttrain-rmse:0.181155\n",
      "[1857]\ttrain-rmse:0.181151\n",
      "[1858]\ttrain-rmse:0.18108\n",
      "[1859]\ttrain-rmse:0.181035\n",
      "[1860]\ttrain-rmse:0.181033\n",
      "[1861]\ttrain-rmse:0.18099\n",
      "[1862]\ttrain-rmse:0.180959\n",
      "[1863]\ttrain-rmse:0.180955\n",
      "[1864]\ttrain-rmse:0.180901\n",
      "[1865]\ttrain-rmse:0.180792\n",
      "[1866]\ttrain-rmse:0.180786\n",
      "[1867]\ttrain-rmse:0.180784\n",
      "[1868]\ttrain-rmse:0.180779\n",
      "[1869]\ttrain-rmse:0.180773\n",
      "[1870]\ttrain-rmse:0.180767\n",
      "[1871]\ttrain-rmse:0.180763\n",
      "[1872]\ttrain-rmse:0.180645\n",
      "[1873]\ttrain-rmse:0.180641\n",
      "[1874]\ttrain-rmse:0.180639\n",
      "[1875]\ttrain-rmse:0.180621\n",
      "[1876]\ttrain-rmse:0.180615\n",
      "[1877]\ttrain-rmse:0.180612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1878]\ttrain-rmse:0.180607\n",
      "[1879]\ttrain-rmse:0.180603\n",
      "[1880]\ttrain-rmse:0.180565\n",
      "[1881]\ttrain-rmse:0.180514\n",
      "[1882]\ttrain-rmse:0.180507\n",
      "[1883]\ttrain-rmse:0.180503\n",
      "[1884]\ttrain-rmse:0.180462\n",
      "[1885]\ttrain-rmse:0.180398\n",
      "[1886]\ttrain-rmse:0.180269\n",
      "[1887]\ttrain-rmse:0.180215\n",
      "[1888]\ttrain-rmse:0.1802\n",
      "[1889]\ttrain-rmse:0.180185\n",
      "[1890]\ttrain-rmse:0.180118\n",
      "[1891]\ttrain-rmse:0.180048\n",
      "[1892]\ttrain-rmse:0.180044\n",
      "[1893]\ttrain-rmse:0.179981\n",
      "[1894]\ttrain-rmse:0.17994\n",
      "[1895]\ttrain-rmse:0.179934\n",
      "[1896]\ttrain-rmse:0.179928\n",
      "[1897]\ttrain-rmse:0.179924\n",
      "[1898]\ttrain-rmse:0.179921\n",
      "[1899]\ttrain-rmse:0.179915\n",
      "[1900]\ttrain-rmse:0.179873\n",
      "[1901]\ttrain-rmse:0.179867\n",
      "[1902]\ttrain-rmse:0.179812\n",
      "[1903]\ttrain-rmse:0.179762\n",
      "[1904]\ttrain-rmse:0.179752\n",
      "[1905]\ttrain-rmse:0.179731\n",
      "[1906]\ttrain-rmse:0.179726\n",
      "[1907]\ttrain-rmse:0.179687\n",
      "[1908]\ttrain-rmse:0.179682\n",
      "[1909]\ttrain-rmse:0.17965\n",
      "[1910]\ttrain-rmse:0.179588\n",
      "[1911]\ttrain-rmse:0.179545\n",
      "[1912]\ttrain-rmse:0.17948\n",
      "[1913]\ttrain-rmse:0.179403\n",
      "[1914]\ttrain-rmse:0.179357\n",
      "[1915]\ttrain-rmse:0.179315\n",
      "[1916]\ttrain-rmse:0.179226\n",
      "[1917]\ttrain-rmse:0.179183\n",
      "[1918]\ttrain-rmse:0.17914\n",
      "[1919]\ttrain-rmse:0.179096\n",
      "[1920]\ttrain-rmse:0.179092\n",
      "[1921]\ttrain-rmse:0.179084\n",
      "[1922]\ttrain-rmse:0.179023\n",
      "[1923]\ttrain-rmse:0.178991\n",
      "[1924]\ttrain-rmse:0.178959\n",
      "[1925]\ttrain-rmse:0.178892\n",
      "[1926]\ttrain-rmse:0.178888\n",
      "[1927]\ttrain-rmse:0.178841\n",
      "[1928]\ttrain-rmse:0.178776\n",
      "[1929]\ttrain-rmse:0.178773\n",
      "[1930]\ttrain-rmse:0.178768\n",
      "[1931]\ttrain-rmse:0.178764\n",
      "[1932]\ttrain-rmse:0.178731\n",
      "[1933]\ttrain-rmse:0.178702\n",
      "[1934]\ttrain-rmse:0.178698\n",
      "[1935]\ttrain-rmse:0.178691\n",
      "[1936]\ttrain-rmse:0.178689\n",
      "[1937]\ttrain-rmse:0.178687\n",
      "[1938]\ttrain-rmse:0.178625\n",
      "[1939]\ttrain-rmse:0.178541\n",
      "[1940]\ttrain-rmse:0.178535\n",
      "[1941]\ttrain-rmse:0.17846\n",
      "[1942]\ttrain-rmse:0.178385\n",
      "[1943]\ttrain-rmse:0.17831\n",
      "[1944]\ttrain-rmse:0.178203\n",
      "[1945]\ttrain-rmse:0.178167\n",
      "[1946]\ttrain-rmse:0.17816\n",
      "[1947]\ttrain-rmse:0.178086\n",
      "[1948]\ttrain-rmse:0.178051\n",
      "[1949]\ttrain-rmse:0.178033\n",
      "[1950]\ttrain-rmse:0.177954\n",
      "[1951]\ttrain-rmse:0.177877\n",
      "[1952]\ttrain-rmse:0.17783\n",
      "[1953]\ttrain-rmse:0.177824\n",
      "[1954]\ttrain-rmse:0.177793\n",
      "[1955]\ttrain-rmse:0.177754\n",
      "[1956]\ttrain-rmse:0.177677\n",
      "[1957]\ttrain-rmse:0.177673\n",
      "[1958]\ttrain-rmse:0.177671\n",
      "[1959]\ttrain-rmse:0.17766\n",
      "[1960]\ttrain-rmse:0.177591\n",
      "[1961]\ttrain-rmse:0.177488\n",
      "[1962]\ttrain-rmse:0.177426\n",
      "[1963]\ttrain-rmse:0.177359\n",
      "[1964]\ttrain-rmse:0.177299\n",
      "[1965]\ttrain-rmse:0.177294\n",
      "[1966]\ttrain-rmse:0.17726\n",
      "[1967]\ttrain-rmse:0.177255\n",
      "[1968]\ttrain-rmse:0.177198\n",
      "[1969]\ttrain-rmse:0.177193\n",
      "[1970]\ttrain-rmse:0.177129\n",
      "[1971]\ttrain-rmse:0.177053\n",
      "[1972]\ttrain-rmse:0.177036\n",
      "[1973]\ttrain-rmse:0.177023\n",
      "[1974]\ttrain-rmse:0.176972\n",
      "[1975]\ttrain-rmse:0.176941\n",
      "[1976]\ttrain-rmse:0.176856\n",
      "[1977]\ttrain-rmse:0.176812\n",
      "[1978]\ttrain-rmse:0.176768\n",
      "[1979]\ttrain-rmse:0.176763\n",
      "[1980]\ttrain-rmse:0.176728\n",
      "[1981]\ttrain-rmse:0.176645\n",
      "[1982]\ttrain-rmse:0.176559\n",
      "[1983]\ttrain-rmse:0.176469\n",
      "[1984]\ttrain-rmse:0.176464\n",
      "[1985]\ttrain-rmse:0.17639\n",
      "[1986]\ttrain-rmse:0.176319\n",
      "[1987]\ttrain-rmse:0.176272\n",
      "[1988]\ttrain-rmse:0.17627\n",
      "[1989]\ttrain-rmse:0.176264\n",
      "[1990]\ttrain-rmse:0.176204\n",
      "[1991]\ttrain-rmse:0.176073\n",
      "[1992]\ttrain-rmse:0.17607\n",
      "[1993]\ttrain-rmse:0.175986\n",
      "[1994]\ttrain-rmse:0.175944\n",
      "[1995]\ttrain-rmse:0.175898\n",
      "[1996]\ttrain-rmse:0.175891\n",
      "[1997]\ttrain-rmse:0.175842\n",
      "[1998]\ttrain-rmse:0.175822\n",
      "[1999]\ttrain-rmse:0.175817\n",
      "[2000]\ttrain-rmse:0.175791\n",
      "[2001]\ttrain-rmse:0.175746\n",
      "[2002]\ttrain-rmse:0.175742\n",
      "[2003]\ttrain-rmse:0.175739\n",
      "[2004]\ttrain-rmse:0.175733\n",
      "[2005]\ttrain-rmse:0.175724\n",
      "[2006]\ttrain-rmse:0.175723\n",
      "[2007]\ttrain-rmse:0.175649\n",
      "[2008]\ttrain-rmse:0.175643\n",
      "[2009]\ttrain-rmse:0.175634\n",
      "[2010]\ttrain-rmse:0.17563\n",
      "[2011]\ttrain-rmse:0.175571\n",
      "[2012]\ttrain-rmse:0.175534\n",
      "[2013]\ttrain-rmse:0.175483\n",
      "[2014]\ttrain-rmse:0.175445\n",
      "[2015]\ttrain-rmse:0.175319\n",
      "[2016]\ttrain-rmse:0.175314\n",
      "[2017]\ttrain-rmse:0.175206\n",
      "[2018]\ttrain-rmse:0.175147\n",
      "[2019]\ttrain-rmse:0.175095\n",
      "[2020]\ttrain-rmse:0.175092\n",
      "[2021]\ttrain-rmse:0.175085\n",
      "[2022]\ttrain-rmse:0.175079\n",
      "[2023]\ttrain-rmse:0.175049\n",
      "[2024]\ttrain-rmse:0.175007\n",
      "[2025]\ttrain-rmse:0.174888\n",
      "[2026]\ttrain-rmse:0.174882\n",
      "[2027]\ttrain-rmse:0.174849\n",
      "[2028]\ttrain-rmse:0.174797\n",
      "[2029]\ttrain-rmse:0.174792\n",
      "[2030]\ttrain-rmse:0.174786\n",
      "[2031]\ttrain-rmse:0.174784\n",
      "[2032]\ttrain-rmse:0.174721\n",
      "[2033]\ttrain-rmse:0.174714\n",
      "[2034]\ttrain-rmse:0.174706\n",
      "[2035]\ttrain-rmse:0.174702\n",
      "[2036]\ttrain-rmse:0.174626\n",
      "[2037]\ttrain-rmse:0.174622\n",
      "[2038]\ttrain-rmse:0.174534\n",
      "[2039]\ttrain-rmse:0.174473\n",
      "[2040]\ttrain-rmse:0.17447\n",
      "[2041]\ttrain-rmse:0.174428\n",
      "[2042]\ttrain-rmse:0.174339\n",
      "[2043]\ttrain-rmse:0.174291\n",
      "[2044]\ttrain-rmse:0.174243\n",
      "[2045]\ttrain-rmse:0.174227\n",
      "[2046]\ttrain-rmse:0.174222\n",
      "[2047]\ttrain-rmse:0.174219\n",
      "[2048]\ttrain-rmse:0.174109\n",
      "[2049]\ttrain-rmse:0.174041\n",
      "[2050]\ttrain-rmse:0.173984\n",
      "[2051]\ttrain-rmse:0.173955\n",
      "[2052]\ttrain-rmse:0.173951\n",
      "[2053]\ttrain-rmse:0.173944\n",
      "[2054]\ttrain-rmse:0.17389\n",
      "[2055]\ttrain-rmse:0.173881\n",
      "[2056]\ttrain-rmse:0.173808\n",
      "[2057]\ttrain-rmse:0.173749\n",
      "[2058]\ttrain-rmse:0.173745\n",
      "[2059]\ttrain-rmse:0.173689\n",
      "[2060]\ttrain-rmse:0.173682\n",
      "[2061]\ttrain-rmse:0.173646\n",
      "[2062]\ttrain-rmse:0.173638\n",
      "[2063]\ttrain-rmse:0.173604\n",
      "[2064]\ttrain-rmse:0.173594\n",
      "[2065]\ttrain-rmse:0.173513\n",
      "[2066]\ttrain-rmse:0.173511\n",
      "[2067]\ttrain-rmse:0.173457\n",
      "[2068]\ttrain-rmse:0.17336\n",
      "[2069]\ttrain-rmse:0.173358\n",
      "[2070]\ttrain-rmse:0.173254\n",
      "[2071]\ttrain-rmse:0.173248\n",
      "[2072]\ttrain-rmse:0.173243\n",
      "[2073]\ttrain-rmse:0.173163\n",
      "[2074]\ttrain-rmse:0.173098\n",
      "[2075]\ttrain-rmse:0.173058\n",
      "[2076]\ttrain-rmse:0.17305\n",
      "[2077]\ttrain-rmse:0.173047\n",
      "[2078]\ttrain-rmse:0.173018\n",
      "[2079]\ttrain-rmse:0.172964\n",
      "[2080]\ttrain-rmse:0.17296\n",
      "[2081]\ttrain-rmse:0.1729\n",
      "[2082]\ttrain-rmse:0.172813\n",
      "[2083]\ttrain-rmse:0.172763\n",
      "[2084]\ttrain-rmse:0.17276\n",
      "[2085]\ttrain-rmse:0.172705\n",
      "[2086]\ttrain-rmse:0.172699\n",
      "[2087]\ttrain-rmse:0.172696\n",
      "[2088]\ttrain-rmse:0.172647\n",
      "[2089]\ttrain-rmse:0.172644\n",
      "[2090]\ttrain-rmse:0.172569\n",
      "[2091]\ttrain-rmse:0.172526\n",
      "[2092]\ttrain-rmse:0.172485\n",
      "[2093]\ttrain-rmse:0.172435\n",
      "[2094]\ttrain-rmse:0.172332\n",
      "[2095]\ttrain-rmse:0.172242\n",
      "[2096]\ttrain-rmse:0.17224\n",
      "[2097]\ttrain-rmse:0.172196\n",
      "[2098]\ttrain-rmse:0.172083\n",
      "[2099]\ttrain-rmse:0.172003\n",
      "[2100]\ttrain-rmse:0.171968\n",
      "[2101]\ttrain-rmse:0.171913\n",
      "[2102]\ttrain-rmse:0.171849\n",
      "[2103]\ttrain-rmse:0.171846\n",
      "[2104]\ttrain-rmse:0.171801\n",
      "[2105]\ttrain-rmse:0.171735\n",
      "[2106]\ttrain-rmse:0.171703\n",
      "[2107]\ttrain-rmse:0.171699\n",
      "[2108]\ttrain-rmse:0.171697\n",
      "[2109]\ttrain-rmse:0.171665\n",
      "[2110]\ttrain-rmse:0.171611\n",
      "[2111]\ttrain-rmse:0.171609\n",
      "[2112]\ttrain-rmse:0.171568\n",
      "[2113]\ttrain-rmse:0.171563\n",
      "[2114]\ttrain-rmse:0.171502\n",
      "[2115]\ttrain-rmse:0.171447\n",
      "[2116]\ttrain-rmse:0.171445\n",
      "[2117]\ttrain-rmse:0.17135\n",
      "[2118]\ttrain-rmse:0.171318\n",
      "[2119]\ttrain-rmse:0.171246\n",
      "[2120]\ttrain-rmse:0.171241\n",
      "[2121]\ttrain-rmse:0.171162\n",
      "[2122]\ttrain-rmse:0.171092\n",
      "[2123]\ttrain-rmse:0.171045\n",
      "[2124]\ttrain-rmse:0.171023\n",
      "[2125]\ttrain-rmse:0.171019\n",
      "[2126]\ttrain-rmse:0.170975\n",
      "[2127]\ttrain-rmse:0.170957\n",
      "[2128]\ttrain-rmse:0.170911\n",
      "[2129]\ttrain-rmse:0.170857\n",
      "[2130]\ttrain-rmse:0.170781\n",
      "[2131]\ttrain-rmse:0.170731\n",
      "[2132]\ttrain-rmse:0.170687\n",
      "[2133]\ttrain-rmse:0.170626\n",
      "[2134]\ttrain-rmse:0.170541\n",
      "[2135]\ttrain-rmse:0.170439\n",
      "[2136]\ttrain-rmse:0.170387\n",
      "[2137]\ttrain-rmse:0.170384\n",
      "[2138]\ttrain-rmse:0.170308\n",
      "[2139]\ttrain-rmse:0.170249\n",
      "[2140]\ttrain-rmse:0.170243\n",
      "[2141]\ttrain-rmse:0.170236\n",
      "[2142]\ttrain-rmse:0.1702\n",
      "[2143]\ttrain-rmse:0.170069\n",
      "[2144]\ttrain-rmse:0.170023\n",
      "[2145]\ttrain-rmse:0.170014\n",
      "[2146]\ttrain-rmse:0.169926\n",
      "[2147]\ttrain-rmse:0.169918\n",
      "[2148]\ttrain-rmse:0.169847\n",
      "[2149]\ttrain-rmse:0.169755\n",
      "[2150]\ttrain-rmse:0.169744\n",
      "[2151]\ttrain-rmse:0.169741\n",
      "[2152]\ttrain-rmse:0.169668\n",
      "[2153]\ttrain-rmse:0.169664\n",
      "[2154]\ttrain-rmse:0.169659\n",
      "[2155]\ttrain-rmse:0.16954\n",
      "[2156]\ttrain-rmse:0.169537\n",
      "[2157]\ttrain-rmse:0.169514\n",
      "[2158]\ttrain-rmse:0.169502\n",
      "[2159]\ttrain-rmse:0.169454\n",
      "[2160]\ttrain-rmse:0.169362\n",
      "[2161]\ttrain-rmse:0.169358\n",
      "[2162]\ttrain-rmse:0.169338\n",
      "[2163]\ttrain-rmse:0.169334\n",
      "[2164]\ttrain-rmse:0.169332\n",
      "[2165]\ttrain-rmse:0.169273\n",
      "[2166]\ttrain-rmse:0.169244\n",
      "[2167]\ttrain-rmse:0.169196\n",
      "[2168]\ttrain-rmse:0.16912\n",
      "[2169]\ttrain-rmse:0.169044\n",
      "[2170]\ttrain-rmse:0.168985\n",
      "[2171]\ttrain-rmse:0.168979\n",
      "[2172]\ttrain-rmse:0.168974\n",
      "[2173]\ttrain-rmse:0.16897\n",
      "[2174]\ttrain-rmse:0.168943\n",
      "[2175]\ttrain-rmse:0.168938\n",
      "[2176]\ttrain-rmse:0.168932\n",
      "[2177]\ttrain-rmse:0.168854\n",
      "[2178]\ttrain-rmse:0.168749\n",
      "[2179]\ttrain-rmse:0.168725\n",
      "[2180]\ttrain-rmse:0.168698\n",
      "[2181]\ttrain-rmse:0.168677\n",
      "[2182]\ttrain-rmse:0.168676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2183]\ttrain-rmse:0.168672\n",
      "[2184]\ttrain-rmse:0.168609\n",
      "[2185]\ttrain-rmse:0.168606\n",
      "[2186]\ttrain-rmse:0.168601\n",
      "[2187]\ttrain-rmse:0.168569\n",
      "[2188]\ttrain-rmse:0.168567\n",
      "[2189]\ttrain-rmse:0.168556\n",
      "[2190]\ttrain-rmse:0.168553\n",
      "[2191]\ttrain-rmse:0.168548\n",
      "[2192]\ttrain-rmse:0.16847\n",
      "[2193]\ttrain-rmse:0.168463\n",
      "[2194]\ttrain-rmse:0.168385\n",
      "[2195]\ttrain-rmse:0.168358\n",
      "[2196]\ttrain-rmse:0.168298\n",
      "[2197]\ttrain-rmse:0.168293\n",
      "[2198]\ttrain-rmse:0.168177\n",
      "[2199]\ttrain-rmse:0.16805\n",
      "[2200]\ttrain-rmse:0.167999\n",
      "[2201]\ttrain-rmse:0.167936\n",
      "[2202]\ttrain-rmse:0.167929\n",
      "[2203]\ttrain-rmse:0.167896\n",
      "[2204]\ttrain-rmse:0.167858\n",
      "[2205]\ttrain-rmse:0.167802\n",
      "[2206]\ttrain-rmse:0.167683\n",
      "[2207]\ttrain-rmse:0.167678\n",
      "[2208]\ttrain-rmse:0.167625\n",
      "[2209]\ttrain-rmse:0.167593\n",
      "[2210]\ttrain-rmse:0.167539\n",
      "[2211]\ttrain-rmse:0.167538\n",
      "[2212]\ttrain-rmse:0.167509\n",
      "[2213]\ttrain-rmse:0.167493\n",
      "[2214]\ttrain-rmse:0.167443\n",
      "[2215]\ttrain-rmse:0.167368\n",
      "[2216]\ttrain-rmse:0.167363\n",
      "[2217]\ttrain-rmse:0.167284\n",
      "[2218]\ttrain-rmse:0.167218\n",
      "[2219]\ttrain-rmse:0.167183\n",
      "[2220]\ttrain-rmse:0.167177\n",
      "[2221]\ttrain-rmse:0.167141\n",
      "[2222]\ttrain-rmse:0.167139\n",
      "[2223]\ttrain-rmse:0.167112\n",
      "[2224]\ttrain-rmse:0.167103\n",
      "[2225]\ttrain-rmse:0.167093\n",
      "[2226]\ttrain-rmse:0.167052\n",
      "[2227]\ttrain-rmse:0.166999\n",
      "[2228]\ttrain-rmse:0.166951\n",
      "[2229]\ttrain-rmse:0.166909\n",
      "[2230]\ttrain-rmse:0.166849\n",
      "[2231]\ttrain-rmse:0.166843\n",
      "[2232]\ttrain-rmse:0.166817\n",
      "[2233]\ttrain-rmse:0.166768\n",
      "[2234]\ttrain-rmse:0.166765\n",
      "[2235]\ttrain-rmse:0.166688\n",
      "[2236]\ttrain-rmse:0.166685\n",
      "[2237]\ttrain-rmse:0.166681\n",
      "[2238]\ttrain-rmse:0.166614\n",
      "[2239]\ttrain-rmse:0.166611\n",
      "[2240]\ttrain-rmse:0.166607\n",
      "[2241]\ttrain-rmse:0.166534\n",
      "[2242]\ttrain-rmse:0.1665\n",
      "[2243]\ttrain-rmse:0.166496\n",
      "[2244]\ttrain-rmse:0.166468\n",
      "[2245]\ttrain-rmse:0.166386\n",
      "[2246]\ttrain-rmse:0.16634\n",
      "[2247]\ttrain-rmse:0.16633\n",
      "[2248]\ttrain-rmse:0.166303\n",
      "[2249]\ttrain-rmse:0.166292\n",
      "[2250]\ttrain-rmse:0.166291\n",
      "[2251]\ttrain-rmse:0.166243\n",
      "[2252]\ttrain-rmse:0.166207\n",
      "[2253]\ttrain-rmse:0.166206\n",
      "[2254]\ttrain-rmse:0.166203\n",
      "[2255]\ttrain-rmse:0.166155\n",
      "[2256]\ttrain-rmse:0.16608\n",
      "[2257]\ttrain-rmse:0.166068\n",
      "[2258]\ttrain-rmse:0.165997\n",
      "[2259]\ttrain-rmse:0.16593\n",
      "[2260]\ttrain-rmse:0.165901\n",
      "[2261]\ttrain-rmse:0.165865\n",
      "[2262]\ttrain-rmse:0.165857\n",
      "[2263]\ttrain-rmse:0.165806\n",
      "[2264]\ttrain-rmse:0.165707\n",
      "[2265]\ttrain-rmse:0.165674\n",
      "[2266]\ttrain-rmse:0.165653\n",
      "[2267]\ttrain-rmse:0.165606\n",
      "[2268]\ttrain-rmse:0.165502\n",
      "[2269]\ttrain-rmse:0.165462\n",
      "[2270]\ttrain-rmse:0.16546\n",
      "[2271]\ttrain-rmse:0.165457\n",
      "[2272]\ttrain-rmse:0.165431\n",
      "[2273]\ttrain-rmse:0.165361\n",
      "[2274]\ttrain-rmse:0.165311\n",
      "[2275]\ttrain-rmse:0.165305\n",
      "[2276]\ttrain-rmse:0.16524\n",
      "[2277]\ttrain-rmse:0.165139\n",
      "[2278]\ttrain-rmse:0.165062\n",
      "[2279]\ttrain-rmse:0.165009\n",
      "[2280]\ttrain-rmse:0.165007\n",
      "[2281]\ttrain-rmse:0.165001\n",
      "[2282]\ttrain-rmse:0.164956\n",
      "[2283]\ttrain-rmse:0.164932\n",
      "[2284]\ttrain-rmse:0.164882\n",
      "[2285]\ttrain-rmse:0.164878\n",
      "[2286]\ttrain-rmse:0.164874\n",
      "[2287]\ttrain-rmse:0.164872\n",
      "[2288]\ttrain-rmse:0.164853\n",
      "[2289]\ttrain-rmse:0.164847\n",
      "[2290]\ttrain-rmse:0.164799\n",
      "[2291]\ttrain-rmse:0.164768\n",
      "[2292]\ttrain-rmse:0.164733\n",
      "[2293]\ttrain-rmse:0.164709\n",
      "[2294]\ttrain-rmse:0.164707\n",
      "[2295]\ttrain-rmse:0.164703\n",
      "[2296]\ttrain-rmse:0.164676\n",
      "[2297]\ttrain-rmse:0.164657\n",
      "[2298]\ttrain-rmse:0.164656\n",
      "[2299]\ttrain-rmse:0.16463\n",
      "[2300]\ttrain-rmse:0.164604\n",
      "[2301]\ttrain-rmse:0.16458\n",
      "[2302]\ttrain-rmse:0.164554\n",
      "[2303]\ttrain-rmse:0.164517\n",
      "[2304]\ttrain-rmse:0.164473\n",
      "[2305]\ttrain-rmse:0.16447\n",
      "[2306]\ttrain-rmse:0.164377\n",
      "[2307]\ttrain-rmse:0.164374\n",
      "[2308]\ttrain-rmse:0.16434\n",
      "[2309]\ttrain-rmse:0.164322\n",
      "[2310]\ttrain-rmse:0.164319\n",
      "[2311]\ttrain-rmse:0.164243\n",
      "[2312]\ttrain-rmse:0.164211\n",
      "[2313]\ttrain-rmse:0.164162\n",
      "[2314]\ttrain-rmse:0.164147\n",
      "[2315]\ttrain-rmse:0.164122\n",
      "[2316]\ttrain-rmse:0.164076\n",
      "[2317]\ttrain-rmse:0.164074\n",
      "[2318]\ttrain-rmse:0.164011\n",
      "[2319]\ttrain-rmse:0.164008\n",
      "[2320]\ttrain-rmse:0.163943\n",
      "[2321]\ttrain-rmse:0.163936\n",
      "[2322]\ttrain-rmse:0.163809\n",
      "[2323]\ttrain-rmse:0.163805\n",
      "[2324]\ttrain-rmse:0.163749\n",
      "[2325]\ttrain-rmse:0.163737\n",
      "[2326]\ttrain-rmse:0.163713\n",
      "[2327]\ttrain-rmse:0.16371\n",
      "[2328]\ttrain-rmse:0.163708\n",
      "[2329]\ttrain-rmse:0.163707\n",
      "[2330]\ttrain-rmse:0.163703\n",
      "[2331]\ttrain-rmse:0.163698\n",
      "[2332]\ttrain-rmse:0.163667\n",
      "[2333]\ttrain-rmse:0.163661\n",
      "[2334]\ttrain-rmse:0.163659\n",
      "[2335]\ttrain-rmse:0.163657\n",
      "[2336]\ttrain-rmse:0.163652\n",
      "[2337]\ttrain-rmse:0.16365\n",
      "[2338]\ttrain-rmse:0.163647\n",
      "[2339]\ttrain-rmse:0.163601\n",
      "[2340]\ttrain-rmse:0.163548\n",
      "[2341]\ttrain-rmse:0.163539\n",
      "[2342]\ttrain-rmse:0.163533\n",
      "[2343]\ttrain-rmse:0.163529\n",
      "[2344]\ttrain-rmse:0.163485\n",
      "[2345]\ttrain-rmse:0.163483\n",
      "[2346]\ttrain-rmse:0.163474\n",
      "[2347]\ttrain-rmse:0.163468\n",
      "[2348]\ttrain-rmse:0.163412\n",
      "[2349]\ttrain-rmse:0.16341\n",
      "[2350]\ttrain-rmse:0.163406\n",
      "[2351]\ttrain-rmse:0.163381\n",
      "[2352]\ttrain-rmse:0.163334\n",
      "[2353]\ttrain-rmse:0.163332\n",
      "[2354]\ttrain-rmse:0.16333\n",
      "[2355]\ttrain-rmse:0.16332\n",
      "[2356]\ttrain-rmse:0.16332\n",
      "[2357]\ttrain-rmse:0.163277\n",
      "[2358]\ttrain-rmse:0.16323\n",
      "[2359]\ttrain-rmse:0.163201\n",
      "[2360]\ttrain-rmse:0.1632\n",
      "[2361]\ttrain-rmse:0.163157\n",
      "[2362]\ttrain-rmse:0.163153\n",
      "[2363]\ttrain-rmse:0.163063\n",
      "[2364]\ttrain-rmse:0.163061\n",
      "[2365]\ttrain-rmse:0.163058\n",
      "[2366]\ttrain-rmse:0.163025\n",
      "[2367]\ttrain-rmse:0.163022\n",
      "[2368]\ttrain-rmse:0.162936\n",
      "[2369]\ttrain-rmse:0.162933\n",
      "[2370]\ttrain-rmse:0.162908\n",
      "[2371]\ttrain-rmse:0.162878\n",
      "[2372]\ttrain-rmse:0.162876\n",
      "[2373]\ttrain-rmse:0.162816\n",
      "[2374]\ttrain-rmse:0.162772\n",
      "[2375]\ttrain-rmse:0.162769\n",
      "[2376]\ttrain-rmse:0.162762\n",
      "[2377]\ttrain-rmse:0.162738\n",
      "[2378]\ttrain-rmse:0.162736\n",
      "[2379]\ttrain-rmse:0.162734\n",
      "[2380]\ttrain-rmse:0.162731\n",
      "[2381]\ttrain-rmse:0.162637\n",
      "[2382]\ttrain-rmse:0.162601\n",
      "[2383]\ttrain-rmse:0.1626\n",
      "[2384]\ttrain-rmse:0.162543\n",
      "[2385]\ttrain-rmse:0.162532\n",
      "[2386]\ttrain-rmse:0.162523\n",
      "[2387]\ttrain-rmse:0.162522\n",
      "[2388]\ttrain-rmse:0.162519\n",
      "[2389]\ttrain-rmse:0.162496\n",
      "[2390]\ttrain-rmse:0.162449\n",
      "[2391]\ttrain-rmse:0.162443\n",
      "[2392]\ttrain-rmse:0.162389\n",
      "[2393]\ttrain-rmse:0.162383\n",
      "[2394]\ttrain-rmse:0.162333\n",
      "[2395]\ttrain-rmse:0.16233\n",
      "[2396]\ttrain-rmse:0.162323\n",
      "[2397]\ttrain-rmse:0.162319\n",
      "[2398]\ttrain-rmse:0.162275\n",
      "[2399]\ttrain-rmse:0.162255\n",
      "[2400]\ttrain-rmse:0.162254\n",
      "[2401]\ttrain-rmse:0.162181\n",
      "[2402]\ttrain-rmse:0.162114\n",
      "[2403]\ttrain-rmse:0.162112\n",
      "[2404]\ttrain-rmse:0.162108\n",
      "[2405]\ttrain-rmse:0.162104\n",
      "[2406]\ttrain-rmse:0.162076\n",
      "[2407]\ttrain-rmse:0.16207\n",
      "[2408]\ttrain-rmse:0.162068\n",
      "[2409]\ttrain-rmse:0.162063\n",
      "[2410]\ttrain-rmse:0.162062\n",
      "[2411]\ttrain-rmse:0.162059\n",
      "[2412]\ttrain-rmse:0.162054\n",
      "[2413]\ttrain-rmse:0.161995\n",
      "[2414]\ttrain-rmse:0.161993\n",
      "[2415]\ttrain-rmse:0.16199\n",
      "[2416]\ttrain-rmse:0.161979\n",
      "[2417]\ttrain-rmse:0.161911\n",
      "[2418]\ttrain-rmse:0.161908\n",
      "[2419]\ttrain-rmse:0.161907\n",
      "[2420]\ttrain-rmse:0.16185\n",
      "[2421]\ttrain-rmse:0.161827\n",
      "[2422]\ttrain-rmse:0.161823\n",
      "[2423]\ttrain-rmse:0.161786\n",
      "[2424]\ttrain-rmse:0.161758\n",
      "[2425]\ttrain-rmse:0.161749\n",
      "[2426]\ttrain-rmse:0.161738\n",
      "[2427]\ttrain-rmse:0.161735\n",
      "[2428]\ttrain-rmse:0.161692\n",
      "[2429]\ttrain-rmse:0.161687\n",
      "[2430]\ttrain-rmse:0.161624\n",
      "[2431]\ttrain-rmse:0.161619\n",
      "[2432]\ttrain-rmse:0.161572\n",
      "[2433]\ttrain-rmse:0.161566\n",
      "[2434]\ttrain-rmse:0.161561\n",
      "[2435]\ttrain-rmse:0.161557\n",
      "[2436]\ttrain-rmse:0.16152\n",
      "[2437]\ttrain-rmse:0.161464\n",
      "[2438]\ttrain-rmse:0.161461\n",
      "[2439]\ttrain-rmse:0.161401\n",
      "[2440]\ttrain-rmse:0.161399\n",
      "[2441]\ttrain-rmse:0.161344\n",
      "[2442]\ttrain-rmse:0.161248\n",
      "[2443]\ttrain-rmse:0.161202\n",
      "[2444]\ttrain-rmse:0.161153\n",
      "[2445]\ttrain-rmse:0.161068\n",
      "[2446]\ttrain-rmse:0.161062\n",
      "[2447]\ttrain-rmse:0.161034\n",
      "[2448]\ttrain-rmse:0.160993\n",
      "[2449]\ttrain-rmse:0.160987\n",
      "[2450]\ttrain-rmse:0.16098\n",
      "[2451]\ttrain-rmse:0.160976\n",
      "[2452]\ttrain-rmse:0.16097\n",
      "[2453]\ttrain-rmse:0.160938\n",
      "[2454]\ttrain-rmse:0.160936\n",
      "[2455]\ttrain-rmse:0.160895\n",
      "[2456]\ttrain-rmse:0.160866\n",
      "[2457]\ttrain-rmse:0.160803\n",
      "[2458]\ttrain-rmse:0.160801\n",
      "[2459]\ttrain-rmse:0.160796\n",
      "[2460]\ttrain-rmse:0.160756\n",
      "[2461]\ttrain-rmse:0.16065\n",
      "[2462]\ttrain-rmse:0.160608\n",
      "[2463]\ttrain-rmse:0.160604\n",
      "[2464]\ttrain-rmse:0.160544\n",
      "[2465]\ttrain-rmse:0.160521\n",
      "[2466]\ttrain-rmse:0.16052\n",
      "[2467]\ttrain-rmse:0.160493\n",
      "[2468]\ttrain-rmse:0.160485\n",
      "[2469]\ttrain-rmse:0.160444\n",
      "[2470]\ttrain-rmse:0.160443\n",
      "[2471]\ttrain-rmse:0.160349\n",
      "[2472]\ttrain-rmse:0.160316\n",
      "[2473]\ttrain-rmse:0.160313\n",
      "[2474]\ttrain-rmse:0.160283\n",
      "[2475]\ttrain-rmse:0.16028\n",
      "[2476]\ttrain-rmse:0.160276\n",
      "[2477]\ttrain-rmse:0.160253\n",
      "[2478]\ttrain-rmse:0.160244\n",
      "[2479]\ttrain-rmse:0.160225\n",
      "[2480]\ttrain-rmse:0.160168\n",
      "[2481]\ttrain-rmse:0.160113\n",
      "[2482]\ttrain-rmse:0.16005\n",
      "[2483]\ttrain-rmse:0.160039\n",
      "[2484]\ttrain-rmse:0.160037\n",
      "[2485]\ttrain-rmse:0.160027\n",
      "[2486]\ttrain-rmse:0.159982\n",
      "[2487]\ttrain-rmse:0.15998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2488]\ttrain-rmse:0.159953\n",
      "[2489]\ttrain-rmse:0.159947\n",
      "[2490]\ttrain-rmse:0.15992\n",
      "[2491]\ttrain-rmse:0.159918\n",
      "[2492]\ttrain-rmse:0.15991\n",
      "[2493]\ttrain-rmse:0.159895\n",
      "[2494]\ttrain-rmse:0.159835\n",
      "[2495]\ttrain-rmse:0.15983\n",
      "[2496]\ttrain-rmse:0.159808\n",
      "[2497]\ttrain-rmse:0.159807\n",
      "[2498]\ttrain-rmse:0.159798\n",
      "[2499]\ttrain-rmse:0.159775\n",
      "[2500]\ttrain-rmse:0.159743\n",
      "[2501]\ttrain-rmse:0.159742\n",
      "[2502]\ttrain-rmse:0.159664\n",
      "[2503]\ttrain-rmse:0.159639\n",
      "[2504]\ttrain-rmse:0.159637\n",
      "[2505]\ttrain-rmse:0.159635\n",
      "[2506]\ttrain-rmse:0.159627\n",
      "[2507]\ttrain-rmse:0.159589\n",
      "[2508]\ttrain-rmse:0.159562\n",
      "[2509]\ttrain-rmse:0.159507\n",
      "[2510]\ttrain-rmse:0.159382\n",
      "[2511]\ttrain-rmse:0.15936\n",
      "[2512]\ttrain-rmse:0.159353\n",
      "[2513]\ttrain-rmse:0.159315\n",
      "[2514]\ttrain-rmse:0.159253\n",
      "[2515]\ttrain-rmse:0.159226\n",
      "[2516]\ttrain-rmse:0.159223\n",
      "[2517]\ttrain-rmse:0.15922\n",
      "[2518]\ttrain-rmse:0.159183\n",
      "[2519]\ttrain-rmse:0.159126\n",
      "[2520]\ttrain-rmse:0.1591\n",
      "[2521]\ttrain-rmse:0.159099\n",
      "[2522]\ttrain-rmse:0.159065\n",
      "[2523]\ttrain-rmse:0.159062\n",
      "[2524]\ttrain-rmse:0.159012\n",
      "[2525]\ttrain-rmse:0.159005\n",
      "[2526]\ttrain-rmse:0.159004\n",
      "[2527]\ttrain-rmse:0.15895\n",
      "[2528]\ttrain-rmse:0.158933\n",
      "[2529]\ttrain-rmse:0.158933\n",
      "[2530]\ttrain-rmse:0.158912\n",
      "[2531]\ttrain-rmse:0.158857\n",
      "[2532]\ttrain-rmse:0.158852\n",
      "[2533]\ttrain-rmse:0.158778\n",
      "[2534]\ttrain-rmse:0.158759\n",
      "[2535]\ttrain-rmse:0.158752\n",
      "[2536]\ttrain-rmse:0.158692\n",
      "[2537]\ttrain-rmse:0.158615\n",
      "[2538]\ttrain-rmse:0.15859\n",
      "[2539]\ttrain-rmse:0.158552\n",
      "[2540]\ttrain-rmse:0.158493\n",
      "[2541]\ttrain-rmse:0.158489\n",
      "[2542]\ttrain-rmse:0.158485\n",
      "[2543]\ttrain-rmse:0.15848\n",
      "[2544]\ttrain-rmse:0.158472\n",
      "[2545]\ttrain-rmse:0.158467\n",
      "[2546]\ttrain-rmse:0.158464\n",
      "[2547]\ttrain-rmse:0.15839\n",
      "[2548]\ttrain-rmse:0.158342\n",
      "[2549]\ttrain-rmse:0.158293\n",
      "[2550]\ttrain-rmse:0.158221\n",
      "[2551]\ttrain-rmse:0.158217\n",
      "[2552]\ttrain-rmse:0.158188\n",
      "[2553]\ttrain-rmse:0.158167\n",
      "[2554]\ttrain-rmse:0.158116\n",
      "[2555]\ttrain-rmse:0.158073\n",
      "[2556]\ttrain-rmse:0.158069\n",
      "[2557]\ttrain-rmse:0.158067\n",
      "[2558]\ttrain-rmse:0.158065\n",
      "[2559]\ttrain-rmse:0.158027\n",
      "[2560]\ttrain-rmse:0.158025\n",
      "[2561]\ttrain-rmse:0.157996\n",
      "[2562]\ttrain-rmse:0.157975\n",
      "[2563]\ttrain-rmse:0.157962\n",
      "[2564]\ttrain-rmse:0.157924\n",
      "[2565]\ttrain-rmse:0.157868\n",
      "[2566]\ttrain-rmse:0.157822\n",
      "[2567]\ttrain-rmse:0.157778\n",
      "[2568]\ttrain-rmse:0.157777\n",
      "[2569]\ttrain-rmse:0.157738\n",
      "[2570]\ttrain-rmse:0.157733\n",
      "[2571]\ttrain-rmse:0.157732\n",
      "[2572]\ttrain-rmse:0.157729\n",
      "[2573]\ttrain-rmse:0.157689\n",
      "[2574]\ttrain-rmse:0.157654\n",
      "[2575]\ttrain-rmse:0.157602\n",
      "[2576]\ttrain-rmse:0.157598\n",
      "[2577]\ttrain-rmse:0.157556\n",
      "[2578]\ttrain-rmse:0.157521\n",
      "[2579]\ttrain-rmse:0.15748\n",
      "[2580]\ttrain-rmse:0.157449\n",
      "[2581]\ttrain-rmse:0.157448\n",
      "[2582]\ttrain-rmse:0.157444\n",
      "[2583]\ttrain-rmse:0.157387\n",
      "[2584]\ttrain-rmse:0.157332\n",
      "[2585]\ttrain-rmse:0.15733\n",
      "[2586]\ttrain-rmse:0.157325\n",
      "[2587]\ttrain-rmse:0.157321\n",
      "[2588]\ttrain-rmse:0.157284\n",
      "[2589]\ttrain-rmse:0.157232\n",
      "[2590]\ttrain-rmse:0.157229\n",
      "[2591]\ttrain-rmse:0.157194\n",
      "[2592]\ttrain-rmse:0.15714\n",
      "[2593]\ttrain-rmse:0.157137\n",
      "[2594]\ttrain-rmse:0.1571\n",
      "[2595]\ttrain-rmse:0.157031\n",
      "[2596]\ttrain-rmse:0.156985\n",
      "[2597]\ttrain-rmse:0.156961\n",
      "[2598]\ttrain-rmse:0.156951\n",
      "[2599]\ttrain-rmse:0.156879\n",
      "[2600]\ttrain-rmse:0.156875\n",
      "[2601]\ttrain-rmse:0.156794\n",
      "[2602]\ttrain-rmse:0.156787\n",
      "[2603]\ttrain-rmse:0.156762\n",
      "[2604]\ttrain-rmse:0.156725\n",
      "[2605]\ttrain-rmse:0.156722\n",
      "[2606]\ttrain-rmse:0.156697\n",
      "[2607]\ttrain-rmse:0.156695\n",
      "[2608]\ttrain-rmse:0.156692\n",
      "[2609]\ttrain-rmse:0.15669\n",
      "[2610]\ttrain-rmse:0.156627\n",
      "[2611]\ttrain-rmse:0.156537\n",
      "[2612]\ttrain-rmse:0.15653\n",
      "[2613]\ttrain-rmse:0.156528\n",
      "[2614]\ttrain-rmse:0.156525\n",
      "[2615]\ttrain-rmse:0.15649\n",
      "[2616]\ttrain-rmse:0.156483\n",
      "[2617]\ttrain-rmse:0.15647\n",
      "[2618]\ttrain-rmse:0.156467\n",
      "[2619]\ttrain-rmse:0.156414\n",
      "[2620]\ttrain-rmse:0.156396\n",
      "[2621]\ttrain-rmse:0.156394\n",
      "[2622]\ttrain-rmse:0.156333\n",
      "[2623]\ttrain-rmse:0.156312\n",
      "[2624]\ttrain-rmse:0.15627\n",
      "[2625]\ttrain-rmse:0.156262\n",
      "[2626]\ttrain-rmse:0.156241\n",
      "[2627]\ttrain-rmse:0.156219\n",
      "[2628]\ttrain-rmse:0.156173\n",
      "[2629]\ttrain-rmse:0.156131\n",
      "[2630]\ttrain-rmse:0.156126\n",
      "[2631]\ttrain-rmse:0.156089\n",
      "[2632]\ttrain-rmse:0.156087\n",
      "[2633]\ttrain-rmse:0.156031\n",
      "[2634]\ttrain-rmse:0.155996\n",
      "[2635]\ttrain-rmse:0.155982\n",
      "[2636]\ttrain-rmse:0.155946\n",
      "[2637]\ttrain-rmse:0.1559\n",
      "[2638]\ttrain-rmse:0.155873\n",
      "[2639]\ttrain-rmse:0.155814\n",
      "[2640]\ttrain-rmse:0.155772\n",
      "[2641]\ttrain-rmse:0.155766\n",
      "[2642]\ttrain-rmse:0.155762\n",
      "[2643]\ttrain-rmse:0.155759\n",
      "[2644]\ttrain-rmse:0.15575\n",
      "[2645]\ttrain-rmse:0.155678\n",
      "[2646]\ttrain-rmse:0.155638\n",
      "[2647]\ttrain-rmse:0.155597\n",
      "[2648]\ttrain-rmse:0.155592\n",
      "[2649]\ttrain-rmse:0.155568\n",
      "[2650]\ttrain-rmse:0.155566\n",
      "[2651]\ttrain-rmse:0.155564\n",
      "[2652]\ttrain-rmse:0.155536\n",
      "[2653]\ttrain-rmse:0.15553\n",
      "[2654]\ttrain-rmse:0.15552\n",
      "[2655]\ttrain-rmse:0.155481\n",
      "[2656]\ttrain-rmse:0.155479\n",
      "[2657]\ttrain-rmse:0.15543\n",
      "[2658]\ttrain-rmse:0.155392\n",
      "[2659]\ttrain-rmse:0.155388\n",
      "[2660]\ttrain-rmse:0.155382\n",
      "[2661]\ttrain-rmse:0.155365\n",
      "[2662]\ttrain-rmse:0.155336\n",
      "[2663]\ttrain-rmse:0.155299\n",
      "[2664]\ttrain-rmse:0.155283\n",
      "[2665]\ttrain-rmse:0.155264\n",
      "[2666]\ttrain-rmse:0.155262\n",
      "[2667]\ttrain-rmse:0.155217\n",
      "[2668]\ttrain-rmse:0.15519\n",
      "[2669]\ttrain-rmse:0.155176\n",
      "[2670]\ttrain-rmse:0.15517\n",
      "[2671]\ttrain-rmse:0.155117\n",
      "[2672]\ttrain-rmse:0.155069\n",
      "[2673]\ttrain-rmse:0.155065\n",
      "[2674]\ttrain-rmse:0.155063\n",
      "[2675]\ttrain-rmse:0.155063\n",
      "[2676]\ttrain-rmse:0.155006\n",
      "[2677]\ttrain-rmse:0.154983\n",
      "[2678]\ttrain-rmse:0.15498\n",
      "[2679]\ttrain-rmse:0.154972\n",
      "[2680]\ttrain-rmse:0.154936\n",
      "[2681]\ttrain-rmse:0.15493\n",
      "[2682]\ttrain-rmse:0.154874\n",
      "[2683]\ttrain-rmse:0.154822\n",
      "[2684]\ttrain-rmse:0.154806\n",
      "[2685]\ttrain-rmse:0.154804\n",
      "[2686]\ttrain-rmse:0.154786\n",
      "[2687]\ttrain-rmse:0.154783\n",
      "[2688]\ttrain-rmse:0.154756\n",
      "[2689]\ttrain-rmse:0.154634\n",
      "[2690]\ttrain-rmse:0.154628\n",
      "[2691]\ttrain-rmse:0.154611\n",
      "[2692]\ttrain-rmse:0.154595\n",
      "[2693]\ttrain-rmse:0.154541\n",
      "[2694]\ttrain-rmse:0.154541\n",
      "[2695]\ttrain-rmse:0.154499\n",
      "[2696]\ttrain-rmse:0.154498\n",
      "[2697]\ttrain-rmse:0.15448\n",
      "[2698]\ttrain-rmse:0.154439\n",
      "[2699]\ttrain-rmse:0.154437\n",
      "[2700]\ttrain-rmse:0.154434\n",
      "[2701]\ttrain-rmse:0.154431\n",
      "[2702]\ttrain-rmse:0.154395\n",
      "[2703]\ttrain-rmse:0.154379\n",
      "[2704]\ttrain-rmse:0.154338\n",
      "[2705]\ttrain-rmse:0.154335\n",
      "[2706]\ttrain-rmse:0.154333\n",
      "[2707]\ttrain-rmse:0.154332\n",
      "[2708]\ttrain-rmse:0.154329\n",
      "[2709]\ttrain-rmse:0.154325\n",
      "[2710]\ttrain-rmse:0.154281\n",
      "[2711]\ttrain-rmse:0.154214\n",
      "[2712]\ttrain-rmse:0.154193\n",
      "[2713]\ttrain-rmse:0.154175\n",
      "[2714]\ttrain-rmse:0.154172\n",
      "[2715]\ttrain-rmse:0.154145\n",
      "[2716]\ttrain-rmse:0.154143\n",
      "[2717]\ttrain-rmse:0.154133\n",
      "[2718]\ttrain-rmse:0.154119\n",
      "[2719]\ttrain-rmse:0.15407\n",
      "[2720]\ttrain-rmse:0.154011\n",
      "[2721]\ttrain-rmse:0.153972\n",
      "[2722]\ttrain-rmse:0.153946\n",
      "[2723]\ttrain-rmse:0.153943\n",
      "[2724]\ttrain-rmse:0.153895\n",
      "[2725]\ttrain-rmse:0.15389\n",
      "[2726]\ttrain-rmse:0.153888\n",
      "[2727]\ttrain-rmse:0.153881\n",
      "[2728]\ttrain-rmse:0.153862\n",
      "[2729]\ttrain-rmse:0.15384\n",
      "[2730]\ttrain-rmse:0.153839\n",
      "[2731]\ttrain-rmse:0.153837\n",
      "[2732]\ttrain-rmse:0.153776\n",
      "[2733]\ttrain-rmse:0.153773\n",
      "[2734]\ttrain-rmse:0.153725\n",
      "[2735]\ttrain-rmse:0.153706\n",
      "[2736]\ttrain-rmse:0.1537\n",
      "[2737]\ttrain-rmse:0.153669\n",
      "[2738]\ttrain-rmse:0.153663\n",
      "[2739]\ttrain-rmse:0.153662\n",
      "[2740]\ttrain-rmse:0.153619\n",
      "[2741]\ttrain-rmse:0.153588\n",
      "[2742]\ttrain-rmse:0.153555\n",
      "[2743]\ttrain-rmse:0.153552\n",
      "[2744]\ttrain-rmse:0.153511\n",
      "[2745]\ttrain-rmse:0.153509\n",
      "[2746]\ttrain-rmse:0.153457\n",
      "[2747]\ttrain-rmse:0.153421\n",
      "[2748]\ttrain-rmse:0.153394\n",
      "[2749]\ttrain-rmse:0.15339\n",
      "[2750]\ttrain-rmse:0.153365\n",
      "[2751]\ttrain-rmse:0.153283\n",
      "[2752]\ttrain-rmse:0.15327\n",
      "[2753]\ttrain-rmse:0.153236\n",
      "[2754]\ttrain-rmse:0.153212\n",
      "[2755]\ttrain-rmse:0.153206\n",
      "[2756]\ttrain-rmse:0.153175\n",
      "[2757]\ttrain-rmse:0.153116\n",
      "[2758]\ttrain-rmse:0.153115\n",
      "[2759]\ttrain-rmse:0.15311\n",
      "[2760]\ttrain-rmse:0.153065\n",
      "[2761]\ttrain-rmse:0.153033\n",
      "[2762]\ttrain-rmse:0.153024\n",
      "[2763]\ttrain-rmse:0.152978\n",
      "[2764]\ttrain-rmse:0.15295\n",
      "[2765]\ttrain-rmse:0.152916\n",
      "[2766]\ttrain-rmse:0.152887\n",
      "[2767]\ttrain-rmse:0.152885\n",
      "[2768]\ttrain-rmse:0.152862\n",
      "[2769]\ttrain-rmse:0.152847\n",
      "[2770]\ttrain-rmse:0.152804\n",
      "[2771]\ttrain-rmse:0.152755\n",
      "[2772]\ttrain-rmse:0.152706\n",
      "[2773]\ttrain-rmse:0.152653\n",
      "[2774]\ttrain-rmse:0.152651\n",
      "[2775]\ttrain-rmse:0.152644\n",
      "[2776]\ttrain-rmse:0.152623\n",
      "[2777]\ttrain-rmse:0.152606\n",
      "[2778]\ttrain-rmse:0.152599\n",
      "[2779]\ttrain-rmse:0.152577\n",
      "[2780]\ttrain-rmse:0.152523\n",
      "[2781]\ttrain-rmse:0.152521\n",
      "[2782]\ttrain-rmse:0.152482\n",
      "[2783]\ttrain-rmse:0.152472\n",
      "[2784]\ttrain-rmse:0.152442\n",
      "[2785]\ttrain-rmse:0.152439\n",
      "[2786]\ttrain-rmse:0.152417\n",
      "[2787]\ttrain-rmse:0.1524\n",
      "[2788]\ttrain-rmse:0.152362\n",
      "[2789]\ttrain-rmse:0.152358\n",
      "[2790]\ttrain-rmse:0.152356\n",
      "[2791]\ttrain-rmse:0.152335\n",
      "[2792]\ttrain-rmse:0.15231\n",
      "[2793]\ttrain-rmse:0.152305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2794]\ttrain-rmse:0.152274\n",
      "[2795]\ttrain-rmse:0.152254\n",
      "[2796]\ttrain-rmse:0.15225\n",
      "[2797]\ttrain-rmse:0.152222\n",
      "[2798]\ttrain-rmse:0.15216\n",
      "[2799]\ttrain-rmse:0.152154\n",
      "[2800]\ttrain-rmse:0.152114\n",
      "[2801]\ttrain-rmse:0.15206\n",
      "[2802]\ttrain-rmse:0.152058\n",
      "[2803]\ttrain-rmse:0.152057\n",
      "[2804]\ttrain-rmse:0.152\n",
      "[2805]\ttrain-rmse:0.151944\n",
      "[2806]\ttrain-rmse:0.151819\n",
      "[2807]\ttrain-rmse:0.151792\n",
      "[2808]\ttrain-rmse:0.151773\n",
      "[2809]\ttrain-rmse:0.151744\n",
      "[2810]\ttrain-rmse:0.151741\n",
      "[2811]\ttrain-rmse:0.151689\n",
      "[2812]\ttrain-rmse:0.151686\n",
      "[2813]\ttrain-rmse:0.151647\n",
      "[2814]\ttrain-rmse:0.151644\n",
      "[2815]\ttrain-rmse:0.15162\n",
      "[2816]\ttrain-rmse:0.151597\n",
      "[2817]\ttrain-rmse:0.151569\n",
      "[2818]\ttrain-rmse:0.151552\n",
      "[2819]\ttrain-rmse:0.151523\n",
      "[2820]\ttrain-rmse:0.151508\n",
      "[2821]\ttrain-rmse:0.151502\n",
      "[2822]\ttrain-rmse:0.151495\n",
      "[2823]\ttrain-rmse:0.151494\n",
      "[2824]\ttrain-rmse:0.151484\n",
      "[2825]\ttrain-rmse:0.15148\n",
      "[2826]\ttrain-rmse:0.151464\n",
      "[2827]\ttrain-rmse:0.151408\n",
      "[2828]\ttrain-rmse:0.151407\n",
      "[2829]\ttrain-rmse:0.151329\n",
      "[2830]\ttrain-rmse:0.151325\n",
      "[2831]\ttrain-rmse:0.151309\n",
      "[2832]\ttrain-rmse:0.151306\n",
      "[2833]\ttrain-rmse:0.151302\n",
      "[2834]\ttrain-rmse:0.151257\n",
      "[2835]\ttrain-rmse:0.151207\n",
      "[2836]\ttrain-rmse:0.151207\n",
      "[2837]\ttrain-rmse:0.151202\n",
      "[2838]\ttrain-rmse:0.151178\n",
      "[2839]\ttrain-rmse:0.151153\n",
      "[2840]\ttrain-rmse:0.151151\n",
      "[2841]\ttrain-rmse:0.151149\n",
      "[2842]\ttrain-rmse:0.151117\n",
      "[2843]\ttrain-rmse:0.151085\n",
      "[2844]\ttrain-rmse:0.151038\n",
      "[2845]\ttrain-rmse:0.151032\n",
      "[2846]\ttrain-rmse:0.151003\n",
      "[2847]\ttrain-rmse:0.150948\n",
      "[2848]\ttrain-rmse:0.150894\n",
      "[2849]\ttrain-rmse:0.150893\n",
      "[2850]\ttrain-rmse:0.150863\n",
      "[2851]\ttrain-rmse:0.15086\n",
      "[2852]\ttrain-rmse:0.150807\n",
      "[2853]\ttrain-rmse:0.150787\n",
      "[2854]\ttrain-rmse:0.150783\n",
      "[2855]\ttrain-rmse:0.150696\n",
      "[2856]\ttrain-rmse:0.150692\n",
      "[2857]\ttrain-rmse:0.150674\n",
      "[2858]\ttrain-rmse:0.150671\n",
      "[2859]\ttrain-rmse:0.15064\n",
      "[2860]\ttrain-rmse:0.150639\n",
      "[2861]\ttrain-rmse:0.150637\n",
      "[2862]\ttrain-rmse:0.150568\n",
      "[2863]\ttrain-rmse:0.150567\n",
      "[2864]\ttrain-rmse:0.150565\n",
      "[2865]\ttrain-rmse:0.150521\n",
      "[2866]\ttrain-rmse:0.150519\n",
      "[2867]\ttrain-rmse:0.150507\n",
      "[2868]\ttrain-rmse:0.150497\n",
      "[2869]\ttrain-rmse:0.150463\n",
      "[2870]\ttrain-rmse:0.150437\n",
      "[2871]\ttrain-rmse:0.150385\n",
      "[2872]\ttrain-rmse:0.150363\n",
      "[2873]\ttrain-rmse:0.150291\n",
      "[2874]\ttrain-rmse:0.150288\n",
      "[2875]\ttrain-rmse:0.150287\n",
      "[2876]\ttrain-rmse:0.150243\n",
      "[2877]\ttrain-rmse:0.150207\n",
      "[2878]\ttrain-rmse:0.150206\n",
      "[2879]\ttrain-rmse:0.150204\n",
      "[2880]\ttrain-rmse:0.15018\n",
      "[2881]\ttrain-rmse:0.150176\n",
      "[2882]\ttrain-rmse:0.150175\n",
      "[2883]\ttrain-rmse:0.150144\n",
      "[2884]\ttrain-rmse:0.150141\n",
      "[2885]\ttrain-rmse:0.150098\n",
      "[2886]\ttrain-rmse:0.150096\n",
      "[2887]\ttrain-rmse:0.150089\n",
      "[2888]\ttrain-rmse:0.150049\n",
      "[2889]\ttrain-rmse:0.150048\n",
      "[2890]\ttrain-rmse:0.149992\n",
      "[2891]\ttrain-rmse:0.14996\n",
      "[2892]\ttrain-rmse:0.149958\n",
      "[2893]\ttrain-rmse:0.149919\n",
      "[2894]\ttrain-rmse:0.149889\n",
      "[2895]\ttrain-rmse:0.149861\n",
      "[2896]\ttrain-rmse:0.149856\n",
      "[2897]\ttrain-rmse:0.149849\n",
      "[2898]\ttrain-rmse:0.149825\n",
      "[2899]\ttrain-rmse:0.149821\n",
      "[2900]\ttrain-rmse:0.149809\n",
      "[2901]\ttrain-rmse:0.149786\n",
      "[2902]\ttrain-rmse:0.149766\n",
      "[2903]\ttrain-rmse:0.149746\n",
      "[2904]\ttrain-rmse:0.1497\n",
      "[2905]\ttrain-rmse:0.149673\n",
      "[2906]\ttrain-rmse:0.14967\n",
      "[2907]\ttrain-rmse:0.149667\n",
      "[2908]\ttrain-rmse:0.149667\n",
      "[2909]\ttrain-rmse:0.149615\n",
      "[2910]\ttrain-rmse:0.149592\n",
      "[2911]\ttrain-rmse:0.149588\n",
      "[2912]\ttrain-rmse:0.149521\n",
      "[2913]\ttrain-rmse:0.149519\n",
      "[2914]\ttrain-rmse:0.149517\n",
      "[2915]\ttrain-rmse:0.149482\n",
      "[2916]\ttrain-rmse:0.149481\n",
      "[2917]\ttrain-rmse:0.149445\n",
      "[2918]\ttrain-rmse:0.149443\n",
      "[2919]\ttrain-rmse:0.14944\n",
      "[2920]\ttrain-rmse:0.149413\n",
      "[2921]\ttrain-rmse:0.14938\n",
      "[2922]\ttrain-rmse:0.14933\n",
      "[2923]\ttrain-rmse:0.149327\n",
      "[2924]\ttrain-rmse:0.149309\n",
      "[2925]\ttrain-rmse:0.149304\n",
      "[2926]\ttrain-rmse:0.149239\n",
      "[2927]\ttrain-rmse:0.149234\n",
      "[2928]\ttrain-rmse:0.149232\n",
      "[2929]\ttrain-rmse:0.149199\n",
      "[2930]\ttrain-rmse:0.149154\n",
      "[2931]\ttrain-rmse:0.14915\n",
      "[2932]\ttrain-rmse:0.149102\n",
      "[2933]\ttrain-rmse:0.149059\n",
      "[2934]\ttrain-rmse:0.149043\n",
      "[2935]\ttrain-rmse:0.148986\n",
      "[2936]\ttrain-rmse:0.148956\n",
      "[2937]\ttrain-rmse:0.148912\n",
      "[2938]\ttrain-rmse:0.148894\n",
      "[2939]\ttrain-rmse:0.148839\n",
      "[2940]\ttrain-rmse:0.148837\n",
      "[2941]\ttrain-rmse:0.148803\n",
      "[2942]\ttrain-rmse:0.148803\n",
      "[2943]\ttrain-rmse:0.148801\n",
      "[2944]\ttrain-rmse:0.148767\n",
      "[2945]\ttrain-rmse:0.148742\n",
      "[2946]\ttrain-rmse:0.148735\n",
      "[2947]\ttrain-rmse:0.148733\n",
      "[2948]\ttrain-rmse:0.148733\n",
      "[2949]\ttrain-rmse:0.14873\n",
      "[2950]\ttrain-rmse:0.148728\n",
      "[2951]\ttrain-rmse:0.148712\n",
      "[2952]\ttrain-rmse:0.148711\n",
      "[2953]\ttrain-rmse:0.148669\n",
      "[2954]\ttrain-rmse:0.148668\n",
      "[2955]\ttrain-rmse:0.148645\n",
      "[2956]\ttrain-rmse:0.148643\n",
      "[2957]\ttrain-rmse:0.148616\n",
      "[2958]\ttrain-rmse:0.148613\n",
      "[2959]\ttrain-rmse:0.148609\n",
      "[2960]\ttrain-rmse:0.148581\n",
      "[2961]\ttrain-rmse:0.148547\n",
      "[2962]\ttrain-rmse:0.148513\n",
      "[2963]\ttrain-rmse:0.148499\n",
      "[2964]\ttrain-rmse:0.148478\n",
      "[2965]\ttrain-rmse:0.148443\n",
      "[2966]\ttrain-rmse:0.148419\n",
      "[2967]\ttrain-rmse:0.148382\n",
      "[2968]\ttrain-rmse:0.148375\n",
      "[2969]\ttrain-rmse:0.148303\n",
      "[2970]\ttrain-rmse:0.148252\n",
      "[2971]\ttrain-rmse:0.148227\n",
      "[2972]\ttrain-rmse:0.148225\n",
      "[2973]\ttrain-rmse:0.148223\n",
      "[2974]\ttrain-rmse:0.148183\n",
      "[2975]\ttrain-rmse:0.148182\n",
      "[2976]\ttrain-rmse:0.14818\n",
      "[2977]\ttrain-rmse:0.148128\n",
      "[2978]\ttrain-rmse:0.148125\n",
      "[2979]\ttrain-rmse:0.148123\n",
      "[2980]\ttrain-rmse:0.148119\n",
      "[2981]\ttrain-rmse:0.148101\n",
      "[2982]\ttrain-rmse:0.148079\n",
      "[2983]\ttrain-rmse:0.148011\n",
      "[2984]\ttrain-rmse:0.147989\n",
      "[2985]\ttrain-rmse:0.147969\n",
      "[2986]\ttrain-rmse:0.147954\n",
      "[2987]\ttrain-rmse:0.147948\n",
      "[2988]\ttrain-rmse:0.147933\n",
      "[2989]\ttrain-rmse:0.147932\n",
      "[2990]\ttrain-rmse:0.147927\n",
      "[2991]\ttrain-rmse:0.147922\n",
      "[2992]\ttrain-rmse:0.147879\n",
      "[2993]\ttrain-rmse:0.147854\n",
      "[2994]\ttrain-rmse:0.147852\n",
      "[2995]\ttrain-rmse:0.147821\n",
      "[2996]\ttrain-rmse:0.147817\n",
      "[2997]\ttrain-rmse:0.147795\n",
      "[2998]\ttrain-rmse:0.147781\n",
      "[2999]\ttrain-rmse:0.147738\n",
      "Result on validation data:  0.11917678406883771\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "print(\"Fitting XGBoost...\")\n",
    "models.append(XGBoost(X_train, y_train, X_val, y_val))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[2997]\ttrain-rmse:0.147795\n",
    "[2998]\ttrain-rmse:0.147781\n",
    "[2999]\ttrain-rmse:0.147738\n",
    "Result on validation data:  0.11917678406883771"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate combined models...\n",
      "Training error...\n",
      "0.11259707148948464\n",
      "Validation error...\n",
      "0.11917678406883771\n"
     ]
    }
   ],
   "source": [
    "def evaluate_models(models, X, y):\n",
    "    assert(min(y) > 0)\n",
    "    guessed_sales = numpy.array([model.guess(X) for model in models])\n",
    "    mean_sales = guessed_sales.mean(axis=0)\n",
    "    relative_err = numpy.absolute((y - mean_sales) / y)\n",
    "    result = numpy.sum(relative_err) / len(y)\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Evaluate combined models...\")\n",
    "print(\"Training error...\")\n",
    "r_train = evaluate_models(models, X_train, y_train)\n",
    "print(r_train)\n",
    "\n",
    "print(\"Validation error...\")\n",
    "r_val = evaluate_models(models, X_val, y_val)\n",
    "print(r_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 训练神经网络（不带Embedding层）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 转换为one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using shuffled data\n"
     ]
    }
   ],
   "source": [
    "#打乱数据\n",
    "shuffle_data=True\n",
    "if shuffle_data:\n",
    "    print(\"Using shuffled data\")\n",
    "    sh = numpy.arange(X.shape[0])\n",
    "    numpy.random.shuffle(sh)\n",
    "    X = X[sh]\n",
    "    y = y[sh]\n",
    "\n",
    "\n",
    "#把数据集转换为oneHot编码\n",
    "def one_hot(X):\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    enc.fit(X)\n",
    "    X1= enc.transform(X)\n",
    "    return X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.array([[1,2,3],[4,5,6]])\n",
    "a1=a[...,[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [4]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\user\\python\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 数据划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1_train = X1[:train_size]\n",
    "X1_val = X1[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759904, 1183)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.5 取样数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used for training: 200000\n"
     ]
    }
   ],
   "source": [
    "X2_train, y_train = sample(X1_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.6 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting NN...\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 119s 593us/step - loss: 0.0123 - val_loss: 0.0098\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 115s 573us/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 109s 546us/step - loss: 0.0071 - val_loss: 0.0079\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 104s 520us/step - loss: 0.0065 - val_loss: 0.0076\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 105s 524us/step - loss: 0.0059 - val_loss: 0.0074\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 107s 533us/step - loss: 0.0055 - val_loss: 0.0079\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 119s 596us/step - loss: 0.0051 - val_loss: 0.0074\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 121s 606us/step - loss: 0.0048 - val_loss: 0.0075\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 122s 612us/step - loss: 0.0045 - val_loss: 0.0073\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 123s 613us/step - loss: 0.0042 - val_loss: 0.0072\n",
      "Result on validation data:  0.08003406537139751\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "print(\"Fitting NN...\")\n",
    "for i in range(1):\n",
    "     models.append(NN(X2_train, y_train, X1_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.7 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate combined models...\n",
      "Training error...\n",
      "0.042231767600769236\n",
      "Validation error...\n",
      "0.08003406537139751\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate combined models...\")\n",
    "print(\"Training error...\")\n",
    "r_train = evaluate_models(models, X2_train, y_train)\n",
    "print(r_train)\n",
    "\n",
    "print(\"Validation error...\")\n",
    "r_val = evaluate_models(models, X1_val, y_val)\n",
    "print(r_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 训练带Embedding层的神经网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using shuffled data\n"
     ]
    }
   ],
   "source": [
    "#打乱数据\n",
    "shuffle_data=True\n",
    "if shuffle_data:\n",
    "    print(\"Using shuffled data\")\n",
    "    sh = numpy.arange(X.shape[0])\n",
    "    numpy.random.shuffle(sh)\n",
    "    X = X[sh]\n",
    "    y = y[sh]\n",
    "\n",
    "\n",
    "#把数据集中每列转换为oneHot编码\n",
    "def one_hot(X):\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    enc.fit(X)\n",
    "    X1= enc.transform(X)\n",
    "    return X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分数据\n",
    "X1_train = X[:train_size]\n",
    "X1_val = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###查看数据\n",
    "#X1_train[..., [1]][:6]\n",
    "dd=X1_train[0]\n",
    "#sorted(dd, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759904, 8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used for training: 759904\n"
     ]
    }
   ],
   "source": [
    "#数据抽样\n",
    "X1_train, y1_train = sample(X1_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.array([[1,2,3,4,5,6,7,8],[9,10,11,12,13,14,15,16]])\n",
    "b=split_features(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1,2,3],[2,3,4]])\n",
    "a1=np.array([[1,2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot1(X):\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(X)\n",
    "    X1= enc.transform(X).toarray()\n",
    "    return X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\user\\python\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\user\\python\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_list=[]\n",
    "store_index = X1_train[..., [1]]\n",
    "store_index=one_hot1(store_index)\n",
    "X_list.append(store_index)\n",
    "day_of_week = X1_train[..., [2]]\n",
    "day_of_week=one_hot1(day_of_week)\n",
    "X_list.append(day_of_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 1115)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories=None,\n",
       "       dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "       n_values=[[2], [3]], sparse=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 从新定义split_features函数，添加one_hot的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_features01(X):\n",
    "    X_list = []\n",
    "\n",
    "    store_index = X[..., [1]]\n",
    "    store_index=one_hot1(store_index)\n",
    "    X_list.append(store_index)\n",
    "    \n",
    "    day_of_week = X[..., [2]]\n",
    "    day_of_week=one_hot1(day_of_week)\n",
    "    X_list.append(day_of_week)\n",
    "\n",
    "    promo = X[..., [3]]\n",
    "    promo=one_hot1(promo)\n",
    "    X_list.append(promo)\n",
    "\n",
    "    year = X[..., [4]]\n",
    "    year=one_hot1(year)\n",
    "    X_list.append(year)\n",
    "\n",
    "    month = X[..., [5]]\n",
    "    month=one_hot1(month)\n",
    "    X_list.append(month)\n",
    "\n",
    "    day = X[..., [6]]\n",
    "    day=one_hot1(day)\n",
    "    X_list.append(day)\n",
    "\n",
    "    State = X[..., [7]]\n",
    "    State=one_hot1(State)\n",
    "    X_list.append(State)\n",
    "\n",
    "    return X_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN_with_EntityEmbedding01(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.epochs = 10\n",
    "        self.checkpointer = ModelCheckpoint(filepath=\"best_model_weights.hdf5\", verbose=1, save_best_only=True)\n",
    "        self.max_log_y = max(numpy.max(numpy.log(y_train)), numpy.max(numpy.log(y_val)))\n",
    "        self.__build_keras_model()\n",
    "        self.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    def preprocessing(self, X):\n",
    "        X_list = split_features01(X)\n",
    "        return X_list\n",
    "\n",
    "    def __build_keras_model(self):\n",
    "        input_store = Input(shape=(1115,))\n",
    "        output_store = Embedding(1115, 10, name='store_embedding')(input_store)\n",
    "        output_store = Reshape(target_shape=(10,))(output_store)\n",
    "\n",
    "        input_dow = Input(shape=(7,))\n",
    "        output_dow = Embedding(7, 6, name='dow_embedding')(input_dow)\n",
    "        output_dow = Reshape(target_shape=(6,))(output_dow)\n",
    "\n",
    "        input_promo = Input(shape=(1,))\n",
    "        output_promo = Dense(1)(input_promo)\n",
    "\n",
    "        input_year = Input(shape=(3,))\n",
    "        output_year = Embedding(3, 2, name='year_embedding')(input_year)\n",
    "        output_year = Reshape(target_shape=(2,))(output_year)\n",
    "\n",
    "        input_month = Input(shape=(12,))\n",
    "        output_month = Embedding(12, 6, name='month_embedding')(input_month)\n",
    "        output_month = Reshape(target_shape=(6,))(output_month)\n",
    "\n",
    "        input_day = Input(shape=(31,))\n",
    "        output_day = Embedding(31, 10, name='day_embedding')(input_day)\n",
    "        output_day = Reshape(target_shape=(10,))(output_day)\n",
    "\n",
    "        input_germanstate = Input(shape=(12,))\n",
    "        output_germanstate = Embedding(12, 6, name='state_embedding')(input_germanstate)\n",
    "        output_germanstate = Reshape(target_shape=(6,))(output_germanstate)\n",
    "\n",
    "        input_model = [input_store, input_dow, input_promo,\n",
    "                       input_year, input_month, input_day, input_germanstate]\n",
    "\n",
    "        output_embeddings = [output_store, output_dow, output_promo,\n",
    "                             output_year, output_month, output_day, output_germanstate]\n",
    "\n",
    "        output_model = Concatenate()(output_embeddings)\n",
    "        output_model = Flatten()(output_model)\n",
    "        output_model = Dense(1000, kernel_initializer=\"uniform\")(output_model)\n",
    "        output_model = Activation('relu')(output_model)\n",
    "        output_model = Dense(500, kernel_initializer=\"uniform\")(output_model)\n",
    "        output_model = Activation('relu')(output_model)\n",
    "        output_model = Dense(1)(output_model)\n",
    "        output_model = Activation('sigmoid')(output_model)\n",
    "\n",
    "        self.model = KerasModel(inputs=input_model, outputs=output_model)\n",
    "\n",
    "        self.model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "    def _val_for_fit(self, val):\n",
    "        val = numpy.log(val) / self.max_log_y\n",
    "        return val\n",
    "\n",
    "    def _val_for_pred(self, val):\n",
    "        return numpy.exp(val * self.max_log_y)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        self.model.fit(self.preprocessing(X_train), self._val_for_fit(y_train),\n",
    "                       validation_data=(self.preprocessing(X_val), self._val_for_fit(y_val)),\n",
    "                       epochs=self.epochs, batch_size=128,\n",
    "                       # callbacks=[self.checkpointer],\n",
    "                       )\n",
    "        # self.model.load_weights('best_model_weights.hdf5')\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        features = self.preprocessing(features)\n",
    "        result = self.model.predict(features).flatten()\n",
    "        return self._val_for_pred(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting NN_with_EntityEmbedding...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-3192d7caae4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitting NN_with_EntityEmbedding...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN_with_EntityEmbedding01\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX1_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-112-0d4587114961>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpointer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"best_model_weights.hdf5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_log_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__build_keras_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-112-0d4587114961>\u001b[0m in \u001b[0;36m__build_keras_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0minput_store\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1115\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0moutput_store\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1115\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'store_embedding'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_store\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moutput_store\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_store\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0minput_dow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\python\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;31m# Inferring the output shape is only relevant for Theano.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m                 \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\python\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[1;31m# input shape known? then we can compute the output shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             return (input_shape[0],) + self._fix_unknown_dimension(\n\u001b[1;32m--> 403\u001b[1;33m                 input_shape[1:], self.target_shape)\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\user\\python\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36m_fix_unknown_dimension\u001b[1;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0moutput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munknown\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0moriginal\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "print(\"Fitting NN_with_EntityEmbedding...\")\n",
    "for i in range(2):\n",
    "    models.append(NN_with_EntityEmbedding01(X1_train, y1_train, X1_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 保存并评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if save_embeddings:\n",
    "    model = models[0].model\n",
    "    store_embedding = model.get_layer('store_embedding').get_weights()[0]\n",
    "    dow_embedding = model.get_layer('dow_embedding').get_weights()[0]\n",
    "    year_embedding = model.get_layer('year_embedding').get_weights()[0]\n",
    "    month_embedding = model.get_layer('month_embedding').get_weights()[0]\n",
    "    day_embedding = model.get_layer('day_embedding').get_weights()[0]\n",
    "    german_states_embedding = model.get_layer('state_embedding').get_weights()[0]\n",
    "    with open(saved_embeddings_fname, 'wb') as f:\n",
    "        pickle.dump([store_embedding, dow_embedding, year_embedding,\n",
    "                     month_embedding, day_embedding, german_states_embedding], f, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate combined models...\n",
      "Training error...\n",
      "0.36276543002692446\n",
      "Validation error...\n",
      "0.3290402526694366\n"
     ]
    }
   ],
   "source": [
    "def evaluate_models(models, X, y):\n",
    "    assert(min(y) > 0)\n",
    "    guessed_sales = numpy.array([model.guess(X) for model in models])\n",
    "    mean_sales = guessed_sales.mean(axis=0)\n",
    "    relative_err = numpy.absolute((y - mean_sales) / y)\n",
    "    result = numpy.sum(relative_err) / len(y)\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Evaluate combined models...\")\n",
    "print(\"Training error...\")\n",
    "r_train = evaluate_models(models, X_train, y_train)\n",
    "print(r_train)\n",
    "\n",
    "print(\"Validation error...\")\n",
    "r_val = evaluate_models(models, X_val, y_val)\n",
    "print(r_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 训练不带embedding层的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting NN...\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 39s 196us/step - loss: 0.1778 - val_loss: 0.1725\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 37s 186us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 37s 186us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 38s 189us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 44s 221us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 44s 222us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 44s 222us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 44s 222us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 45s 226us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 45s 223us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Result on validation data:  5.825855990372018\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 45s 225us/step - loss: 0.1843 - val_loss: 0.1725\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 45s 225us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 45s 224us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 45s 226us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 45s 224us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 45s 224us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 45s 225us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 45s 223us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 44s 221us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 45s 223us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Result on validation data:  5.825855990372018\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 46s 229us/step - loss: 0.1778 - val_loss: 0.1725\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 45s 223us/step - loss: 0.1772 - val_loss: 0.1725\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 45s 223us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 44s 218us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 42s 210us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 44s 218us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 42s 208us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 41s 205us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 44s 220us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 45s 223us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Result on validation data:  5.825855990372018\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 46s 232us/step - loss: 0.1774 - val_loss: 0.1725\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 46s 228us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 46s 229us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 46s 229us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 46s 230us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 45s 227us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 46s 230us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 46s 229us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 46s 230us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 44s 222us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Result on validation data:  5.825855990372018\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 47s 234us/step - loss: 0.1871 - val_loss: 0.1725\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 46s 228us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 46s 231us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 47s 235us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 43s 213us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 44s 221us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 46s 231us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 49s 243us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 46s 229us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 46s 230us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Result on validation data:  5.825855990372018\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting NN...\")\n",
    "for i in range(5):\n",
    "     models.append(NN(X_train, y_train, X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate combined models...\n",
      "Training error...\n",
      "3.112884150840002\n",
      "Validation error...\n",
      "2.9053194526974613\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate combined models...\")\n",
    "print(\"Training error...\")\n",
    "r_train = evaluate_models(models, X_train, y_train)\n",
    "print(r_train)\n",
    "\n",
    "print(\"Validation error...\")\n",
    "r_val = evaluate_models(models, X_val, y_val)\n",
    "print(r_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN1(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.epochs = 10\n",
    "        self.checkpointer = ModelCheckpoint(filepath=\"best_model_weights.hdf5\", verbose=1, save_best_only=True)\n",
    "        self.max_log_y = max(numpy.max(numpy.log(y_train)), numpy.max(numpy.log(y_val)))\n",
    "        self.__build_keras_model()\n",
    "        self.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    def __build_keras_model(self):\n",
    "        self.model = Sequential()\n",
    "        #self.model.add(Dense(1000, kernel_initializer=\"uniform\", input_dim=1183))\n",
    "        self.model.add(Dense(100, input_dim=8))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(50))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.add(Activation('sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "    def _val_for_fit(self, val):\n",
    "        val = numpy.log(val) / self.max_log_y\n",
    "        return val\n",
    "\n",
    "    def _val_for_pred(self, val):\n",
    "        return numpy.exp(val * self.max_log_y)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        self.model.fit(X_train, self._val_for_fit(y_train),\n",
    "                       validation_data=(X_val, self._val_for_fit(y_val)),\n",
    "                       epochs=self.epochs, batch_size=128,\n",
    "                       # callbacks=[self.checkpointer],\n",
    "                       )\n",
    "        # self.model.load_weights('best_model_weights.hdf5')\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        result = self.model.predict(features).flatten()\n",
    "        return self._val_for_pred(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting NN...\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 5s 23us/step - loss: 0.1873 - val_loss: 0.1725\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 4s 18us/step - loss: 0.1769 - val_loss: 0.1714\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.1773 - val_loss: 0.1725\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.1782 - val_loss: 0.1725\n",
      "Result on validation data:  5.825855549448704\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 5s 23us/step - loss: 0.1773 - val_loss: 0.1706\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.1778 - val_loss: 0.1725\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.1773 - val_loss: 0.1725\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 5s 23us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 5s 23us/step - loss: 0.1760 - val_loss: 0.1725\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 5s 23us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 5s 23us/step - loss: 0.1765 - val_loss: 0.1665\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 5s 23us/step - loss: 0.1769 - val_loss: 0.1724\n",
      "Result on validation data:  5.819964447145853\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 6s 29us/step - loss: 0.1769 - val_loss: 0.1699\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 5s 24us/step - loss: 0.1767 - val_loss: 0.1725\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 5s 23us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 5s 24us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 5s 24us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 5s 24us/step - loss: 0.1770 - val_loss: 0.1725\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 5s 25us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 5s 24us/step - loss: 0.1893 - val_loss: 0.1725\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 5s 24us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 5s 25us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Result on validation data:  5.825855015565381\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 6s 30us/step - loss: 0.1809 - val_loss: 0.1725\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 5s 25us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 5s 24us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 5s 26us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 6s 30us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 6s 30us/step - loss: 0.1769 - val_loss: 0.1725\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 6s 30us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 6s 28us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 6s 28us/step - loss: 0.1772 - val_loss: 0.1725\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 6s 28us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Result on validation data:  5.825802734868867\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 8s 38us/step - loss: 0.1764 - val_loss: 0.1690\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 5s 27us/step - loss: 0.1774 - val_loss: 0.1725\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 5s 27us/step - loss: 0.1773 - val_loss: 0.1725\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 6s 28us/step - loss: 0.1768 - val_loss: 0.1723\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 6s 28us/step - loss: 0.1766 - val_loss: 0.1725\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 5s 27us/step - loss: 0.1770 - val_loss: 0.1710\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 5s 27us/step - loss: 0.1767 - val_loss: 0.1725\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 5s 27us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 5s 27us/step - loss: 0.1771 - val_loss: 0.1725\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 5s 25us/step - loss: 0.1752 - val_loss: 0.1691\n",
      "Result on validation data:  5.627001439110409\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting NN...\")\n",
    "for i in range(5):\n",
    "     models.append(NN1(X_train, y_train, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate combined models...\n",
      "Training error...\n",
      "4.133171781147755\n",
      "Validation error...\n",
      "3.865164903940068\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate combined models...\")\n",
    "print(\"Training error...\")\n",
    "r_train = evaluate_models(models, X_train, y_train)\n",
    "print(r_train)\n",
    "\n",
    "print(\"Validation error...\")\n",
    "r_val = evaluate_models(models, X_val, y_val)\n",
    "print(r_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('feature_train_data.pickle', 'rb')\n",
    "(X, y) = pickle.load(f)\n",
    "\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\user\\python\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X2=X\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit(X2)\n",
    "X2 = enc.transform(X2)\n",
    "\n",
    "\n",
    "X1_train = X2[:train_size]\n",
    "X1_val = X2[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759904,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used for training: 759904\n"
     ]
    }
   ],
   "source": [
    "X1_train, y1_train = sample(X1_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1183"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X1_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\user\\python\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "V1=X_val\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit(V1)\n",
    "V1 = enc.transform(V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting NN...\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 112s 559us/step - loss: 0.0122 - val_loss: 0.0110\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 105s 527us/step - loss: 0.0081 - val_loss: 0.0102\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 106s 529us/step - loss: 0.0071 - val_loss: 0.0096\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 114s 568us/step - loss: 0.0065 - val_loss: 0.0104\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 117s 583us/step - loss: 0.0060 - val_loss: 0.0099\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 124s 622us/step - loss: 0.0055 - val_loss: 0.0095\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 125s 627us/step - loss: 0.0051 - val_loss: 0.0096\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 125s 626us/step - loss: 0.0048 - val_loss: 0.0099\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 125s 624us/step - loss: 0.0045 - val_loss: 0.0095\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 124s 621us/step - loss: 0.0042 - val_loss: 0.0099\n",
      "Result on validation data:  0.10832588508976115\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 119s 595us/step - loss: 0.0121 - val_loss: 0.0111\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 116s 580us/step - loss: 0.0081 - val_loss: 0.0105TA: 0s - l\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 117s 584us/step - loss: 0.0071 - val_loss: 0.0101\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 116s 581us/step - loss: 0.0064 - val_loss: 0.0100\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 668s 3ms/step - loss: 0.0060 - val_loss: 0.0096\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 106s 528us/step - loss: 0.0055 - val_loss: 0.0096\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 104s 521us/step - loss: 0.0051 - val_loss: 0.0095\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 111s 554us/step - loss: 0.0048 - val_loss: 0.0096\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 122s 608us/step - loss: 0.0045 - val_loss: 0.0096\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 119s 593us/step - loss: 0.0043 - val_loss: 0.0098\n",
      "Result on validation data:  0.10354307748809172\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 122s 610us/step - loss: 0.0121 - val_loss: 0.0126\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 121s 605us/step - loss: 0.0081 - val_loss: 0.0101\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 119s 597us/step - loss: 0.0071 - val_loss: 0.0107\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 119s 597us/step - loss: 0.0065 - val_loss: 0.0105\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 118s 591us/step - loss: 0.0059 - val_loss: 0.0097\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 118s 589us/step - loss: 0.0054 - val_loss: 0.0100\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 119s 595us/step - loss: 0.0051 - val_loss: 0.0100\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 116s 580us/step - loss: 0.0047 - val_loss: 0.0098\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 111s 556us/step - loss: 0.0044 - val_loss: 0.0100\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 111s 555us/step - loss: 0.0042 - val_loss: 0.0099\n",
      "Result on validation data:  0.10932309884352688\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 139s 695us/step - loss: 0.0123 - val_loss: 0.0104\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 128s 639us/step - loss: 0.0080 - val_loss: 0.0103\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 125s 627us/step - loss: 0.0071 - val_loss: 0.0105\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 121s 606us/step - loss: 0.0065 - val_loss: 0.0099\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 122s 608us/step - loss: 0.0060 - val_loss: 0.0097\n",
      "Epoch 6/10\n",
      "116992/200000 [================>.............] - ETA: 43s - loss: 0.0055"
     ]
    }
   ],
   "source": [
    "print(\"Fitting NN...\")\n",
    "for i in range(5):\n",
    "     models.append(NN(X1_train, y1_train, X1_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1183"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1172"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(V1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
